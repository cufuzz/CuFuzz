{"pdf_page": [26, 29], "model_response": [{"head": "VecAdd", "head_type": "__global__", "description": "A kernel function to add elements from two vectors A and B into vector C.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "MatAdd", "head_type": "__global__", "description": "A kernel function to add elements from two matrices A and B into matrix C.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads", "head_type": "CUDA_API", "description": "An intrinsic function for synchronizing threads within a block.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [29, 32], "model_response": [{"head": "__cluster_dims__(2,1,1)cluster_kernel", "head_type": "__global__", "description": "A kernel defined with a compile-time cluster size of 2 in the X-dimension and 1 in the Y and Z dimensions.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cluster_kernel", "head_type": "__global__", "description": "A kernel defined without compile-time cluster size attributes.", "relation": "calls", "tail": "cudaLaunchKernelEx", "tail_type": "CUDA_API"}]}
{"pdf_page": [32, 35], "model_response": [{"head": "cuda::memcpy_async", "head_type": "CUDA_API", "description": "Performs an asynchronous data copy from global memory to the device", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_API"}, {"head": "cuda::memcpy_async", "head_type": "CUDA_API", "description": "Performs an asynchronous data copy from global memory to the device", "relation": "calls", "tail": "cuda::pipeline", "tail_type": "CUDA_API"}]}
{"pdf_page": [41, 45], "model_response": [{"head": "VecAdd", "head_type": "__global__", "description": "Device function to add vectors A and B.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMalloc", "head_type": "", "description": "Allocates device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpy", "head_type": "", "description": "Copies data between host and device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "", "description": "Frees device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "MyKernel", "head_type": "__global__", "description": "Device function to process a 2D array with pitch.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "MyKernel", "head_type": "__global__", "description": "Device function to process a 3D array with pitched memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyToSymbol", "head_type": "", "description": "Copies data to a symbol in device code.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyFromSymbol", "head_type": "", "description": "Copies data from a symbol in device code.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "", "description": "Gets the properties of a selected device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetLimit", "head_type": "", "description": "Sets a memory limit for CUDA device management.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [45, 49], "model_response": [{"head": "cudaStreamSetAttribute", "head_type": "CUDA_API", "description": "Set attributes to a CUDA Stream to set L2 persisting access window.", "relation": "calls", "tail": "cudaStreamAttrValue", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphKernelNodeSetAttribute", "head_type": "CUDA_API", "description": "Set attributes to a CUDA Graph Kernel node to set L2 persisting access window.", "relation": "calls", "tail": "cudaKernelNodeAttrValue", "tail_type": "CUDA_CLASS"}, {"head": "cudaStreamCreate", "head_type": "CUDA_API", "description": "Create a CUDA stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "CUDA_API", "description": "Query GPU properties.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetLimit", "head_type": "CUDA_API", "description": "Set-aside L2 cache for persisting accesses or the max allowed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamSetAttribute", "head_type": "CUDA_API", "description": "Set attributes to a CUDA Stream to set access policy.", "relation": "calls", "tail": "stream_attribute.accessPolicyWindow", "tail_type": "CUDA_CLASS"}, {"head": "cudaCtxResetPersistingL2Cache", "head_type": "CUDA_API", "description": "Remove any persistent lines in L2 cache.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaKernel_launchA", "head_type": "__global__", "description": "Kernel A using the persisting L2 cache set-aside.", "relation": "calls", "tail": "cudaStreamSetAttribute", "tail_type": "CUDA_API"}, {"head": "cudaKernel_launchB", "head_type": "__global__", "description": "Kernel B benefits from the persisting of data1.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaKernel_launchC", "head_type": "__global__", "description": "Kernel C uses L2 cache in normal mode.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [49, 54], "model_response": [{"head": "MatMulKernel", "head_type": "__global__", "description": "A CUDA kernel that performs matrix multiplication by computing each element of the output matrix C.", "relation": "calls", "tail": "GetSubMatrix", "tail_type": "CUDA_API"}, {"head": "MatMulKernel", "head_type": "__global__", "description": "A CUDA kernel that performs matrix multiplication by computing each element of the output matrix C.", "relation": "calls", "tail": "GetElement", "tail_type": "CUDA_API"}, {"head": "MatMulKernel", "head_type": "__global__", "description": "A CUDA kernel that performs matrix multiplication by computing each element of the output matrix C.", "relation": "calls", "tail": "SetElement", "tail_type": "CUDA_API"}]}
{"pdf_page": [55, 58], "model_response": [{"head": "clusterHist_kernel", "head_type": "__global__", "description": "This kernel computes histograms in shared memory or distributed shared memory, depending on the number of histogram bins.", "relation": "calls", "tail": "cg::this_cluster", "tail_type": "CUDA_API"}, {"head": "clusterHist_kernel", "head_type": "__global__", "description": "This kernel computes histograms in shared memory or distributed shared memory, depending on the number of histogram bins.", "relation": "calls", "tail": "cg::this_grid", "tail_type": "CUDA_API"}, {"head": "clusterHist_kernel", "head_type": "__global__", "description": "This kernel computes histograms in shared memory or distributed shared memory, depending on the number of histogram bins.", "relation": "calls", "tail": "cluster.sync", "tail_type": "CUDA_API"}, {"head": "clusterHist_kernel", "head_type": "__global__", "description": "This kernel computes histograms in shared memory or distributed shared memory, depending on the number of histogram bins.", "relation": "calls", "tail": "cluster.map_shared_rank", "tail_type": "CUDA_API"}, {"head": "clusterHist_kernel", "head_type": "__global__", "description": "This kernel computes histograms in shared memory or distributed shared memory, depending on the number of histogram bins.", "relation": "calls", "tail": "atomicAdd", "tail_type": "CUDA_API"}, {"head": "cudaFuncSetAttribute", "head_type": "CUDA_API", "description": "Sets specific attributes for a kernel.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchKernelEx", "head_type": "CUDA_API", "description": "Launches a CUDA kernel with extended attributes.", "relation": "calls", "tail": "cudaLaunchAttributeClusterDimension", "tail_type": "CUDA_CLASS"}, {"head": "cudaHostAlloc", "head_type": "CUDA_API", "description": "Allocates page-locked host memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeHost", "head_type": "CUDA_API", "description": "Frees page-locked host memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaHostRegister", "head_type": "CUDA_API", "description": "Page-locks a range of memory allocated by malloc.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [60, 62], "model_response": [{"head": "cudaLaunchKernelEx", "head_type": "CUDA_API", "description": "A CUDA API for launching a kernel with additional configuration.", "relation": "calls", "tail": "myKernel", "tail_type": "CUDA_API"}, {"head": "cudaStreamSetAttribute", "head_type": "CUDA_API", "description": "A CUDA API to set attributes for a CUDA stream.", "relation": "calls", "tail": "cudaLaunchAttributeMemSyncDomainMap", "tail_type": "CUDA_API"}]}
{"pdf_page": [63, 67], "model_response": [{"head": "MyKernel", "head_type": "__global__", "description": "Executes MyKernel on the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamCreate", "head_type": "CUDA_API", "description": "Creates a new stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocHost", "head_type": "CUDA_API", "description": "Allocates page-locked memory on the host.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyAsync", "head_type": "CUDA_API", "description": "Performs asynchronous memory copy between host and device.", "relation": "calls", "tail": "cudaStream_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaStreamDestroy", "head_type": "CUDA_API", "description": "Destroys the stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetStreamPriorityRange", "head_type": "CUDA_API", "description": "Gets the allowable range of priorities for streams on this device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamCreateWithPriority", "head_type": "CUDA_API", "description": "Creates a stream with a specified priority.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchHostFunc", "head_type": "CUDA_API", "description": "Launches a host function into a stream.", "relation": "calls", "tail": "MyCallback", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [67, 70], "model_response": [{"head": "primary_kernel", "head_type": "__global__", "description": "Primary kernel involved in programmatically dependent launch.", "relation": "calls", "tail": "cudaTriggerProgrammaticLaunchCompletion", "tail_type": "CUDA_API"}, {"head": "primary_kernel", "head_type": "__global__", "description": "Primary kernel involved in programmatically dependent launch.", "relation": "calls", "tail": "secondary_kernel", "tail_type": "CUDA_API"}, {"head": "secondary_kernel", "head_type": "__global__", "description": "Secondary kernel in programmatic dependent execution.", "relation": "calls", "tail": "cudaGridDependencySynchronize", "tail_type": "CUDA_API"}, {"head": "secondary_kernel", "head_type": "__global__", "description": "Secondary kernel in programmatic dependent execution.", "relation": "calls", "tail": "primary_kernel", "tail_type": "CUDA_API"}]}
{"pdf_page": [72, 75], "model_response": [{"head": "cudaGraphCreate", "head_type": "CUDA_API", "description": "Creates an empty CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddKernelNode", "head_type": "CUDA_API", "description": "Adds a kernel node to a CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddDependencies", "head_type": "CUDA_API", "description": "Adds dependencies between nodes in a CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamBeginCapture", "head_type": "CUDA_API", "description": "Begins stream capture mode for creating a CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamEndCapture", "head_type": "CUDA_API", "description": "Ends stream capture mode and returns the created CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventRecord", "head_type": "CUDA_API", "description": "Records an event at a specific point in a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "CUDA_API", "description": "Makes a stream wait for an event to occur.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [75, 77], "model_response": [{"head": "cudaUserObjectCreate", "head_type": "CUDA_API", "description": "Creates a CUDA user object with a user-specified destructor callback and an internal reference count.", "relation": "calls", "tail": "cudaGraphRetainUserObject", "tail_type": "CUDA_API"}, {"head": "cudaGraphRetainUserObject", "head_type": "CUDA_API", "description": "Associates a CUDA user object reference with a CUDA graph.", "relation": "calls", "tail": "cudaGraphInstantiate", "tail_type": "CUDA_API"}, {"head": "cudaGraphInstantiate", "head_type": "CUDA_API", "description": "Instantiates a CUDA graph to create a graph execution object, retaining references to user objects.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "cudaGraphLaunch", "head_type": "CUDA_API", "description": "Launches a CUDA graph execution instance asynchronously.", "relation": "calls", "tail": "cudaGraphExecDestroy", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecDestroy", "head_type": "CUDA_API", "description": "Destroys a CUDA graph execution object, deferring release of user object references until execution is complete.", "relation": "calls", "tail": "cudaStreamSynchronize", "tail_type": "CUDA_API"}, {"head": "cudaStreamSynchronize", "head_type": "CUDA_API", "description": "Synchronizes the stream, after which any deferred user object destructors will execute.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [77, 81], "model_response": [{"head": "cudaGraphExecUpdate", "head_type": "CUDA_API", "description": "Allows an instantiated graph to be updated with parameters from a topologically identical graph.", "relation": "calls", "tail": "cudaGraphExecDestroy", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecUpdate", "head_type": "CUDA_API", "description": "Allows an instantiated graph to be updated with parameters from a topologically identical graph.", "relation": "calls", "tail": "cudaGraphInstantiate", "tail_type": "CUDA_API"}, {"head": "cudaGraphNodeSetEnabled", "head_type": "CUDA_API", "description": "Enables or disables kernel, memset, and memcpy nodes in an instantiated graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphNodeGetEnabled", "head_type": "CUDA_API", "description": "Queries the enable state of a node.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [81, 87], "model_response": [{"head": "launchFireAndForgetGraph", "head_type": "__global__", "description": "This function launches a CUDA graph in the Fire and Forget mode.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "launchTailGraph", "head_type": "__global__", "description": "This function launches a CUDA graph using Tail Launch mode.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}]}
{"pdf_page": [87, 91], "model_response": [{"head": "relaunchSelf", "head_type": "__global__", "description": "A kernel that relaunches itself for up to a maximum relaunch count.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "relaunchSelf", "head_type": "__global__", "description": "A kernel that relaunches itself for up to a maximum relaunch count.", "relation": "calls", "tail": "cudaGetCurrentGraphExec", "tail_type": "CUDA_API"}, {"head": "launchSiblingGraph", "head_type": "__global__", "description": "A kernel that launches a sibling graph using the CUDA runtime API.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}]}
{"pdf_page": [91, 95], "model_response": [{"head": "setHandle", "head_type": "__global__", "description": "Sets the condition for a CUDA graph conditional node.", "relation": "calls", "tail": "cudaGraphSetConditional", "tail_type": "CUDA_API"}, {"head": "loopKernel", "head_type": "__global__", "description": "Executes a loop by decrementing a counter and setting the condition for a CUDA graph conditional node.", "relation": "calls", "tail": "cudaGraphSetConditional", "tail_type": "CUDA_API"}]}
{"pdf_page": [95, 98], "model_response": [{"head": "cudaGetDeviceCount", "head_type": "__host__", "description": "A host function that retrieves the number of CUDA-enabled devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "A host function that retrieves properties of a specific device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaSetDevice", "head_type": "__host__", "description": "A host function that sets the current device for subsequent operations.", "relation": "calls", "tail": "cudaMalloc", "tail_type": "CUDA_API"}, {"head": "cudaStreamCreate", "head_type": "__host__", "description": "A host function that creates a new stream object associated with the current device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceEnablePeerAccess", "head_type": "__host__", "description": "A host function that enables peer-to-peer memory access between devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyPeer", "head_type": "__host__", "description": "A host function for copying memory between different devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaPointerGetAttributes", "head_type": "__host__", "description": "A host function that retrieves attributes of a pointer to determine its location.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaHostAlloc", "head_type": "__host__", "description": "A host function that allocates portable, mapped pinned host memory for use by devices.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [98, 101], "model_response": [{"head": "cudaIpcGetMemHandle", "head_type": "CUDA_API", "description": "Gets the IPC handle for a given device memory pointer.", "relation": "calls", "tail": "Inter Process Communication API", "tail_type": "CUDA_API"}, {"head": "cudaIpcOpenMemHandle", "head_type": "CUDA_API", "description": "Retrieves a device pointer from the IPC handle within another process.", "relation": "calls", "tail": "Inter Process Communication API", "tail_type": "CUDA_API"}, {"head": "cudaDeviceSynchronize", "head_type": "CUDA_API", "description": "Synchronizes the device to ensure previous operations are complete.", "relation": "calls", "tail": "", "tail_type": ""}, {"head": "cudaPeekAtLastError", "head_type": "CUDA_API", "description": "Returns the error code of the last CUDA runtime function called.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetLastError", "head_type": "CUDA_API", "description": "Returns the last error code and resets the error variable to cudaSuccess.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetLimit", "head_type": "CUDA_API", "description": "Queries the specified limit on the shared memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetLimit", "head_type": "CUDA_API", "description": "Sets the specified limit on the shared memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [101, 105], "model_response": [{"head": "transformKernel", "head_type": "__global__", "description": "Simple transformation kernel that applies texture fetch and writes to global memory.", "relation": "calls", "tail": "tex2D", "tail_type": "CUDA_API"}]}
{"pdf_page": [106, 108], "model_response": [{"head": "surf2Dread", "head_type": "__device__", "description": "Reads data from a 2D surface object at specified coordinates.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "surf2Dwrite", "head_type": "__device__", "description": "Writes data to a 2D surface object at specified coordinates.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [108, 111], "model_response": [{"head": "createVertices", "head_type": "__global__", "description": "Kernel function to dynamically modify the vertices stored in a vertex buffer object.", "relation": "calls", "tail": "make_float4", "tail_type": "CUDA_API"}]}
{"pdf_page": [111, 114], "model_response": [{"head": "createVertices", "head_type": "__global__", "description": "A kernel to create a simple sine wave pattern over a grid of vertices.", "relation": "calls", "tail": "make_float4", "tail_type": "CUDA_API"}, {"head": "createVertices", "head_type": "__global__", "description": "A kernel to create a simple sine wave pattern over a grid of vertices.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [113, 116], "model_response": [{"head": "cudaD3D10GetDevice", "head_type": "CUDA_API", "description": "A function to get a CUDA-enabled adapter for Direct3D 10.", "relation": "calls", "tail": "IDXGIAdapter", "tail_type": "CUDA_CLASS"}, {"head": "cudaSetDevice", "head_type": "CUDA_API", "description": "A function to set the active device for the current thread.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsD3D10RegisterResource", "head_type": "CUDA_API", "description": "Registers a Direct3D 10 resource for access by CUDA.", "relation": "calls", "tail": "ID3D10Buffer", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphicsResourceSetMapFlags", "head_type": "CUDA_API", "description": "Sets the map flags for a registered graphics resource.", "relation": "calls", "tail": "cudaGraphicsResource", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphicsMapResources", "head_type": "CUDA_API", "description": "Maps graphics resources for access by CUDA.", "relation": "calls", "tail": "cudaGraphicsResource", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphicsResourceGetMappedPointer", "head_type": "CUDA_API", "description": "Get a pointer to a mapped graphics resource.", "relation": "calls", "tail": "cudaGraphicsResource", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphicsUnmapResources", "head_type": "CUDA_API", "description": "Unmaps graphics resources after access by CUDA.", "relation": "calls", "tail": "cudaGraphicsResource", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphicsUnregisterResource", "head_type": "CUDA_API", "description": "Unregisters a graphics resource.", "relation": "calls", "tail": "cudaGraphicsResource", "tail_type": "CUDA_CLASS"}, {"head": "createVertices", "head_type": "__global__", "description": "Generates vertex positions using a sine wave pattern.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [115, 118], "model_response": [{"head": "createVertices", "head_type": "__global__", "description": "Generates vertex positions using sine wave patterns.", "relation": "calls", "tail": "make_float4", "tail_type": "CUDA_API"}, {"head": "createVertices", "head_type": "__global__", "description": "Generates vertex positions using sine wave patterns.", "relation": "calls", "tail": "__int_as_float", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsResourceSetMapFlags", "head_type": "CUDA_API", "description": "Sets map flags for a registered graphics resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsD3D11RegisterResource", "head_type": "CUDA_API", "description": "Registers a Direct3D graphics resource with CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsMapResources", "head_type": "CUDA_API", "description": "Maps graphics resources for use with CUDA operations.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsResourceGetMappedPointer", "head_type": "CUDA_API", "description": "Gets a pointer to a mapped graphics resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsUnmapResources", "head_type": "CUDA_API", "description": "Unmaps graphics resources from CUDA operations.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsUnregisterResource", "head_type": "CUDA_API", "description": "Unregisters a graphics resource from CUDA.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [118, 122], "model_response": [{"head": "getCudaDeviceForVulkanPhysicalDevice", "head_type": "__host__", "description": "Determines the corresponding CUDA device for a given Vulkan physical device", "relation": "calls", "tail": "cudaGetDeviceCount", "tail_type": "CUDA_API"}, {"head": "getCudaDeviceForVulkanPhysicalDevice", "head_type": "__host__", "description": "Determines the corresponding CUDA device for a given Vulkan physical device", "relation": "calls", "tail": "cudaGetDeviceProperties", "tail_type": "CUDA_API"}, {"head": "importVulkanMemoryObjectFromFileDescriptor", "head_type": "__host__", "description": "Imports a Vulkan memory object into CUDA using a file descriptor", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "importVulkanMemoryObjectFromNTHandle", "head_type": "__host__", "description": "Imports a Vulkan memory object into CUDA using an NT handle", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "importVulkanMemoryObjectFromNamedNTHandle", "head_type": "__host__", "description": "Imports a Vulkan memory object into CUDA using a named NT handle", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "importVulkanMemoryObjectFromKMTHandle", "head_type": "__host__", "description": "Imports a Vulkan memory object into CUDA using a globally shared D3DKMT handle", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "mapBufferOntoExternalMemory", "head_type": "__host__", "description": "Maps a buffer onto an imported external memory object", "relation": "calls", "tail": "cudaExternalMemoryGetMappedBuffer", "tail_type": "CUDA_API"}]}
{"pdf_page": [122, 125], "model_response": [{"head": "mapMipmappedArrayOntoExternalMemory", "head_type": "__host__", "description": "Maps a CUDA mipmapped array onto an imported Vulkan memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedMipmappedArray", "tail_type": "CUDA_API"}, {"head": "getCudaChannelFormatDescForVulkanFormat", "head_type": "__host__", "description": "Converts Vulkan format to CUDA channel format descriptor.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "getCudaExtentForVulkanExtent", "head_type": "__host__", "description": "Converts Vulkan extent to CUDA extent.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "getCudaMipmappedArrayFlagsForVulkanImage", "head_type": "__host__", "description": "Determines the CUDA mipmapped array flags for a given Vulkan image.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "importVulkanSemaphoreObjectFromFileDescriptor", "head_type": "__host__", "description": "Imports a Vulkan semaphore object into CUDA using a file descriptor.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}]}
{"pdf_page": [125, 127], "model_response": [{"head": "importVulkanSemaphoreObjectFromNTHandle", "head_type": "__host__", "description": "Imports a Vulkan semaphore object using a NT handle into CUDA.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importVulkanSemaphoreObjectFromNamedNTHandle", "head_type": "__host__", "description": "Imports a Vulkan semaphore object using a named NT handle into CUDA.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importVulkanSemaphoreObjectFromKMTHandle", "head_type": "__host__", "description": "Imports a Vulkan semaphore object using a globally shared D3DKMT handle into CUDA.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "signalExternalSemaphore", "head_type": "__host__", "description": "Signals an imported Vulkan semaphore object using CUDA.", "relation": "calls", "tail": "cudaSignalExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "waitExternalSemaphore", "head_type": "__host__", "description": "Waits on an imported Vulkan semaphore object using CUDA until it reaches the signaled state.", "relation": "calls", "tail": "cudaWaitExternalSemaphoresAsync", "tail_type": "CUDA_API"}]}
{"pdf_page": [127, 130], "model_response": [{"head": "cudaGetDeviceCount", "head_type": "CUDA_API", "description": "This API retrieves the number of available CUDA devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "CUDA_API", "description": "This API retrieves properties of a specific CUDA device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaImportExternalMemory", "head_type": "CUDA_API", "description": "This API imports an external memory object.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "CUDA_API", "description": "This API maps a buffer onto an imported memory object.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [129, 133], "model_response": [{"head": "mapBufferOntoExternalMemory", "head_type": "__host__", "description": "A device pointer can be mapped onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedBuffer", "tail_type": "CUDA_API"}, {"head": "mapMipmappedArrayOntoExternalMemory", "head_type": "__host__", "description": "A CUDA mipmapped array can be mapped onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedMipmappedArray", "tail_type": "CUDA_API"}, {"head": "importD3D12FenceFromNTHandle", "head_type": "__host__", "description": "Imports a shareable Direct3D 12 fence object using the NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D12FenceFromNamedNTHandle", "head_type": "__host__", "description": "Imports a shareable Direct3D 12 fence object using a named handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}]}
{"pdf_page": [133, 136], "model_response": [{"head": "signalExternalSemaphore", "head_type": "__host__", "description": "Signals an imported Direct3D 12 fence object, setting its value to the specified one.", "relation": "calls", "tail": "cudaSignalExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "waitExternalSemaphore", "head_type": "__host__", "description": "Waits on an imported Direct3D 12 fence object until its value becomes greater than or equal to the specified value.", "relation": "calls", "tail": "cudaWaitExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "getCudaDeviceForD3D11Device", "head_type": "__host__", "description": "Determines the CUDA device corresponding to a Direct3D 11 device by comparing LUIDs.", "relation": "calls", "tail": "cudaGetDeviceCount", "tail_type": "CUDA_API"}, {"head": "getCudaDeviceForD3D11Device", "head_type": "__host__", "description": "Determines the CUDA device corresponding to a Direct3D 11 device by comparing LUIDs.", "relation": "calls", "tail": "cudaGetDeviceProperties", "tail_type": "CUDA_API"}, {"head": "importD3D11ResourceFromNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 resource into CUDA using an NT handle.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "importD3D11ResourceFromNamedNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 resource into CUDA using a named NT handle.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "importD3D11ResourceFromKMTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 resource into CUDA using a globally shared D3DKMT handle.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}]}
{"pdf_page": [135, 139], "model_response": [{"head": "importD3D11ResourceFromNamedNTHandle", "head_type": "__host__", "description": "Function to import a shareable Direct3D 11 resource using a named handle.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "importD3D11ResourceFromKMTHandle", "head_type": "__host__", "description": "Function to import a shareable Direct3D 11 resource using a globally shared D3DKMT handle.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "mapBufferOntoExternalMemory", "head_type": "__host__", "description": "Function to map a device pointer onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedBuffer", "tail_type": "CUDA_API"}, {"head": "mapMipmappedArrayOntoExternalMemory", "head_type": "__host__", "description": "Function to map a CUDA mipmapped array onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedMipmappedArray", "tail_type": "CUDA_API"}, {"head": "importD3D11FenceFromNTHandle", "head_type": "__host__", "description": "Function to import a shareable Direct3D 11 fence object using an NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}]}
{"pdf_page": [138, 142], "model_response": [{"head": "importD3D11FenceFromNTHandle", "head_type": "__device__", "description": "Imports a Direct3D 11 fence object into CUDA using an NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11FenceFromNamedNTHandle", "head_type": "__device__", "description": "Imports a Direct3D 11 fence object into CUDA using a named NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11KeyedMutexFromNTHandle", "head_type": "__device__", "description": "Imports a Direct3D 11 keyed mutex object into CUDA using an NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11KeyedMutexFromNamedNTHandle", "head_type": "__device__", "description": "Imports a Direct3D 11 keyed mutex object into CUDA using a named NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11FenceFromKMTHandle", "head_type": "__device__", "description": "Imports a Direct3D 11 fence object into CUDA using a KMT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "signalExternalSemaphore", "head_type": "__host__", "description": "Signals an imported Direct3D 11 fence or keyed mutex object.", "relation": "calls", "tail": "cudaSignalExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "waitExternalSemaphore", "head_type": "__host__", "description": "Waits on an imported Direct3D 11 fence or keyed mutex object.", "relation": "calls", "tail": "cudaWaitExternalSemaphoresAsync", "tail_type": "CUDA_API"}]}
{"pdf_page": [142, 145], "model_response": [{"head": "cuDeviceGetUuid", "head_type": "CUDA_API", "description": "Retrieves the UUID of a CUDA device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaImportExternalMemory", "head_type": "CUDA_API", "description": "Imports an external memory object into CUDA.", "relation": "calls", "tail": "cudaExternalMemoryHandleDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaImportExternalMemory", "head_type": "CUDA_API", "description": "Imports an external memory object into CUDA.", "relation": "calls", "tail": "NvSciBufObj", "tail_type": "CUDA_CLASS"}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "CUDA_API", "description": "Maps a buffer onto an external memory object and obtains a device pointer.", "relation": "calls", "tail": "cudaExternalMemoryBufferDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaFree", "head_type": "CUDA_API", "description": "Frees device memory allocated with CUDA APIs.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [145, 148], "model_response": [{"head": "mapMipmappedArrayOntoExternalMemory", "head_type": "__device__", "description": "Maps a CUDA mipmapped array onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedMipmappedArray", "tail_type": "CUDA_API"}, {"head": "createNvSciSyncObject", "head_type": "__device__", "description": "Creates an NvSciSyncObj compatible with CUDA.", "relation": "calls", "tail": "cudaDeviceGetNvSciSyncAttributes", "tail_type": "CUDA_API"}, {"head": "importNvSciSyncObject", "head_type": "__device__", "description": "Imports a NvSciSync object into CUDA.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "signalExternalSemaphore", "head_type": "__device__", "description": "Signals an imported NvSciSyncObj object.", "relation": "calls", "tail": "cudaSignalExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "waitExternalSemaphore", "head_type": "__device__", "description": "Waits on an imported NvSciSyncObj object.", "relation": "calls", "tail": "cudaWaitExternalSemaphoresAsync", "tail_type": "CUDA_API"}]}
{"pdf_page": [157, 160], "model_response": [{"head": "MyKernel", "head_type": "__global__", "description": "A kernel that computes element-wise multiplication of inputs a and b and stores in d.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaOccupancyMaxActiveBlocksPerMultiprocessor", "head_type": "CUDA_API", "description": "Provides an occupancy prediction in terms of active blocks per multiprocessor.", "relation": "calls", "tail": "MyKernel", "tail_type": "CUDA_API"}, {"head": "MyKernel", "head_type": "__global__", "description": "A kernel that computes square of each element in the array if within bounds.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaOccupancyMaxPotentialBlockSize", "head_type": "CUDA_API", "description": "Computes execution configuration for maximum multiprocessor-level occupancy.", "relation": "calls", "tail": "MyKernel", "tail_type": "CUDA_API"}, {"head": "cudaOccupancyMaxPotentialClusterSize", "head_type": "CUDA_API", "description": "Finds the maximum cluster size for a given kernel.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}, {"head": "cudaOccupancyMaxActiveClusters", "head_type": "CUDA_API", "description": "Calculates the maximum number of active clusters for a given configuration.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}]}
{"pdf_page": [185, 188], "model_response": [{"head": "writeXY", "head_type": "__device__", "description": "A function that writes values to shared memory variables X and Y.", "relation": "calls", "tail": "__threadfence", "tail_type": "CUDA_API"}, {"head": "readXY", "head_type": "__device__", "description": "A function that reads values from shared memory variables Y and X.", "relation": "calls", "tail": "__threadfence", "tail_type": "CUDA_API"}, {"head": "sum", "head_type": "__global__", "description": "A kernel that computes the sum of an array of N numbers.", "relation": "calls", "tail": "__threadfence", "tail_type": "CUDA_API"}]}
{"pdf_page": [188, 189], "model_response": [{"head": "__syncthreads", "head_type": "__device__", "description": "__syncthreads waits for all threads in a block and ensures memory visibility.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads_count", "head_type": "__device__", "description": "__syncthreads_count syncs threads and counts those meeting a condition.", "relation": "calls", "tail": "__syncthreads", "tail_type": "CUDA_API"}, {"head": "__syncthreads_and", "head_type": "__device__", "description": "__syncthreads_and syncs threads meeting a collective condition.", "relation": "calls", "tail": "__syncthreads", "tail_type": "CUDA_API"}, {"head": "__syncthreads_or", "head_type": "__device__", "description": "__syncthreads_or syncs threads if any meet a condition.", "relation": "calls", "tail": "__syncthreads", "tail_type": "CUDA_API"}, {"head": "__syncwarp", "head_type": "__device__", "description": "__syncwarp syncs threads within a warp using a mask.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [201, 206], "model_response": [{"head": "mykernel", "head_type": "__global__", "description": "GPU kernel function updating an integer value atomically using system-wide scope.", "relation": "calls", "tail": "atomicAdd_system", "tail_type": "CUDA_API"}, {"head": "atomicAdd", "head_type": "__device__", "description": "Device function that implements atomic addition for double-precision floating-point numbers based on atomicCAS for architectures lower than 6.0.", "relation": "calls", "tail": "atomicCAS", "tail_type": "CUDA_API"}]}
{"pdf_page": [207, 210], "model_response": [{"head": "__isShared", "head_type": "__device__", "description": "Returns 1 if ptr contains the generic address of an object in shared memory space, otherwise returns 0.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__isConstant", "head_type": "__device__", "description": "Returns 1 if ptr contains the generic address of an object in constant memory space, otherwise returns 0.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__isGridConstant", "head_type": "__device__", "description": "Returns 1 if ptr contains the generic address of a kernel parameter annotated with __grid_constant__, otherwise returns 0.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__isLocal", "head_type": "__device__", "description": "Returns 1 if ptr contains the generic address of an object in local memory space, otherwise returns 0.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_global", "head_type": "__device__", "description": "Returns the result of executing the PTX cvta.to.global instruction on the generic address denoted by ptr.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_shared", "head_type": "__device__", "description": "Returns the result of executing the PTX cvta.to.shared instruction on the generic address denoted by ptr.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_constant", "head_type": "__device__", "description": "Returns the result of executing the PTX cvta.to.const instruction on the generic address denoted by ptr.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_local", "head_type": "__device__", "description": "Returns the result of executing the PTX cvta.to.local instruction on the generic address denoted by ptr.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_global_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTX cvta.global instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_shared_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTX cvta.shared instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_constant_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTX cvta.const instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_local_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTX cvta.local instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "alloca", "head_type": "__host__ __device__", "description": "Allocates size bytes of memory in the stack frame of the caller.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [209, 213], "model_response": [{"head": "__cvta_constant_to_generic", "head_type": "__device__", "description": "Returns a generic pointer using the PTXcvta.const instruction with a value.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_local_to_generic", "head_type": "__device__", "description": "Returns a generic pointer using the PTXcvta.local instruction with a value.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "alloca", "head_type": "__host__ __device__", "description": "Allocates memory in the stack frame of the caller.", "relation": "calls", "tail": "<malloc.h>", "tail_type": "CUDA_API"}, {"head": "__builtin_assume_aligned", "head_type": "", "description": "Allows compiler to assume argument pointer alignment.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_assume", "head_type": "", "description": "Allows compiler to assume the Boolean argument is true.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__assume", "head_type": "", "description": "Allows compiler to assume the Boolean argument is true.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_expect", "head_type": "", "description": "Indicates expected equality to a compiler for branch prediction.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_unreachable", "head_type": "", "description": "Indicates that control flow can never reach this point.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__all_sync", "head_type": "", "description": "Evaluates predicate for all non-exited threads in mask.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__any_sync", "head_type": "", "description": "Evaluates predicate for any non-exited thread in mask.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__ballot_sync", "head_type": "", "description": "Evaluates predicate for all non-exited threads and returns an integer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__activemask", "head_type": "", "description": "Returns a 32-bit integer mask of all currently active threads in the warp.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [212, 215], "model_response": [{"head": "__all_sync", "head_type": "__device__", "description": "Evaluate predicate for all non-exited threads in mask and return non-zero if predicate evaluates to non-zero for all of them.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__any_sync", "head_type": "__device__", "description": "Evaluate predicate for all non-exited threads in mask and return non-zero if predicate evaluates to non-zero for any of them.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__ballot_sync", "head_type": "__device__", "description": "Evaluate predicate for all non-exited threads in mask and return an integer whose Nth bit is set if predicate evaluates to non-zero for the Nth thread of the warp.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__activemask", "head_type": "__device__", "description": "Returns a 32-bit integer mask of all currently active threads in the calling warp.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__match_any_sync", "head_type": "__device__", "description": "Returns mask of threads that have same value in mask.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__match_all_sync", "head_type": "__device__", "description": "Returns mask if all threads in mask have the same value; otherwise 0 is returned.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_add_sync", "head_type": "__device__", "description": "Perform reduction operation adding values provided by threads named in mask.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_min_sync", "head_type": "__device__", "description": "Perform reduction operation computing minimum of values provided by threads named in mask.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_max_sync", "head_type": "__device__", "description": "Perform reduction operation computing maximum of values provided by threads named in mask.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_and_sync", "head_type": "__device__", "description": "Perform logical AND reduction operation on the values provided by threads named in mask.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_or_sync", "head_type": "__device__", "description": "Perform logical OR reduction operation on the values provided by threads named in mask.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_xor_sync", "head_type": "__device__", "description": "Perform logical XOR reduction operation on the values provided by threads named in mask.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [216, 218], "model_response": [{"head": "bcast", "head_type": "__global__", "description": "Broadcast a single value across a warp.", "relation": "calls", "tail": "__shfl_sync", "tail_type": "CUDA_API"}, {"head": "scan4", "head_type": "__global__", "description": "Perform inclusive plus-scan across sub-partitions of 8 threads.", "relation": "calls", "tail": "__shfl_up_sync", "tail_type": "CUDA_API"}, {"head": "warpReduce", "head_type": "__global__", "description": "Reduction across a warp using XOR mode.", "relation": "calls", "tail": "__shfl_xor_sync", "tail_type": "CUDA_API"}]}
{"pdf_page": [227, 230], "model_response": [{"head": "simple_sync", "head_type": "__global__", "description": "Synchronizes all threads in a block using cooperative_groups::this_thread_block().", "relation": "calls", "tail": "cooperative_groups::this_thread_block().sync", "tail_type": "CUDA_API"}, {"head": "split_arrive_wait", "head_type": "__global__", "description": "Implements a synchronization pattern with cuda::barrier.", "relation": "calls", "tail": "cuda/barrier<cuda::thread_scope_block>.arrive", "tail_type": "CUDA_API"}, {"head": "split_arrive_wait", "head_type": "__global__", "description": "Implements a synchronization pattern with cuda::barrier.", "relation": "calls", "tail": "cuda/barrier<cuda::thread_scope_block>.wait", "tail_type": "CUDA_API"}, {"head": "init_barrier", "head_type": "__global__", "description": "Initializes a cuda::barrier for thread synchronization.", "relation": "calls", "tail": "cuda/barrier<cuda::thread_scope_block>.init", "tail_type": "CUDA_API"}]}
{"pdf_page": [230, 232], "model_response": [{"head": "producer", "head_type": "__device__", "description": "producer threads wait for consumer threads to signal that the buffer can be filled.", "relation": "calls", "tail": "arrive_and_wait", "tail_type": "CUDA_API"}, {"head": "producer", "head_type": "__device__", "description": "producer threads wait for consumer threads to signal that the buffer can be filled.", "relation": "calls", "tail": "arrive", "tail_type": "CUDA_API"}, {"head": "consumer", "head_type": "__device__", "description": "consumer threads wait for producer threads to signal that the buffer is filled.", "relation": "calls", "tail": "arrive", "tail_type": "CUDA_API"}, {"head": "consumer", "head_type": "__device__", "description": "consumer threads wait for producer threads to signal that the buffer is filled.", "relation": "calls", "tail": "arrive_and_wait", "tail_type": "CUDA_API"}, {"head": "producer_consumer_pattern", "head_type": "__global__", "description": "specialized as producer and consumer threads participate in cuda::barrier synchronization.", "relation": "calls", "tail": "producer", "tail_type": "CUDA_API"}, {"head": "producer_consumer_pattern", "head_type": "__global__", "description": "specialized as producer and consumer threads participate in cuda::barrier synchronization.", "relation": "calls", "tail": "consumer", "tail_type": "CUDA_API"}]}
{"pdf_page": [233, 235], "model_response": [{"head": "psum", "head_type": "__global__", "description": "psum function performs partial sum calculation with barrier synchronization.", "relation": "calls", "tail": "cooperative_groups::this_thread_block", "tail_type": "CUDA_CLASS"}, {"head": "psum", "head_type": "__global__", "description": "psum function performs partial sum calculation with barrier synchronization.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "psum", "head_type": "__global__", "description": "psum function performs partial sum calculation with barrier synchronization.", "relation": "calls", "tail": "arrive", "tail_type": "CUDA_API"}, {"head": "divergent_compute", "head_type": "__device__", "description": "Function marked to perform divergent computation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "independent_computation", "head_type": "__device__", "description": "Function marked to perform independent computation on data.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [236, 239], "model_response": [{"head": "without_memcpy_async", "head_type": "__global__", "description": "Function implementing copy and compute pattern without memcpy_async.", "relation": "calls", "tail": "block.sync", "tail_type": "CUDA_API"}, {"head": "without_memcpy_async", "head_type": "__global__", "description": "Function implementing copy and compute pattern without memcpy_async.", "relation": "calls", "tail": "compute", "tail_type": "__device__"}, {"head": "with_memcpy_async", "head_type": "__global__", "description": "Function implementing asynchronous data copies with memcpy_async.", "relation": "calls", "tail": "cooperative_groups::memcpy_async", "tail_type": "CUDA_API"}, {"head": "with_memcpy_async", "head_type": "__global__", "description": "Function implementing asynchronous data copies with memcpy_async.", "relation": "calls", "tail": "cooperative_groups::wait", "tail_type": "CUDA_API"}, {"head": "with_memcpy_async", "head_type": "__global__", "description": "Function implementing asynchronous data copies with memcpy_async.", "relation": "calls", "tail": "compute", "tail_type": "__device__"}, {"head": "with_barrier", "head_type": "__global__", "description": "Function implementing asynchronous data copies using cuda::barrier.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "with_barrier", "head_type": "__global__", "description": "Function implementing asynchronous data copies using cuda::barrier.", "relation": "calls", "tail": "barrier.arrive_and_wait", "tail_type": "CUDA_API"}, {"head": "with_barrier", "head_type": "__global__", "description": "Function implementing asynchronous data copies using cuda::barrier.", "relation": "calls", "tail": "compute", "tail_type": "__device__"}]}
{"pdf_page": [241, 243], "model_response": [{"head": "with_single_stage", "head_type": "__global__", "description": "Global kernel function implementing single-stage asynchronous data copy using cuda::pipeline.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "with_single_stage", "head_type": "__global__", "description": "Global kernel function implementing single-stage asynchronous data copy using cuda::pipeline.", "relation": "calls", "tail": "cuda::make_pipeline", "tail_type": "CUDA_API"}, {"head": "with_single_stage", "head_type": "__global__", "description": "Global kernel function implementing single-stage asynchronous data copy using cuda::pipeline.", "relation": "calls", "tail": "pipeline.producer_acquire", "tail_type": "CUDA_API"}, {"head": "with_single_stage", "head_type": "__global__", "description": "Global kernel function implementing single-stage asynchronous data copy using cuda::pipeline.", "relation": "calls", "tail": "pipeline.producer_commit", "tail_type": "CUDA_API"}, {"head": "with_single_stage", "head_type": "__global__", "description": "Global kernel function implementing single-stage asynchronous data copy using cuda::pipeline.", "relation": "calls", "tail": "pipeline.consumer_wait", "tail_type": "CUDA_API"}, {"head": "with_single_stage", "head_type": "__global__", "description": "Global kernel function implementing single-stage asynchronous data copy using cuda::pipeline.", "relation": "calls", "tail": "compute", "tail_type": "__device__"}, {"head": "with_single_stage", "head_type": "__global__", "description": "Global kernel function implementing single-stage asynchronous data copy using cuda::pipeline.", "relation": "calls", "tail": "pipeline.consumer_release", "tail_type": "CUDA_API"}]}
{"pdf_page": [243, 246], "model_response": [{"head": "with_staging", "head_type": "__global__", "description": "A CUDA kernel that implements a two-stage pipeline to overlap data-transfer with computation.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "with_staging_unified", "head_type": "__global__", "description": "A CUDA kernel that implements a unified pipeline for data-transfer and computation.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "compute", "head_type": "__device__", "description": "Compute function used within the data pipeline.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [246, 249], "model_response": [{"head": "with_specialized_staging_unified", "head_type": "__global__", "description": "In this kernel function, threads are divided into producers and consumers to handle asynchronous data copy operations efficiently.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "with_specialized_staging_unified", "head_type": "__global__", "description": "In this kernel function, threads are divided into producers and consumers to handle asynchronous data copy operations efficiently.", "relation": "calls", "tail": "compute", "tail_type": "CUDA_API"}, {"head": "with_staging_scope_thread", "head_type": "__global__", "description": "This kernel function uses a pipeline with thread scope combined with synchronization techniques to efficiently process batches.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "with_staging_scope_thread", "head_type": "__global__", "description": "This kernel function uses a pipeline with thread scope combined with synchronization techniques to efficiently process batches.", "relation": "calls", "tail": "compute", "tail_type": "CUDA_API"}]}
{"pdf_page": [254, 259], "model_response": [{"head": "cuda::device::experimental::cp_async_bulk_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk-asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::fence_proxy_async_shared_cta", "head_type": "__device__", "description": "Waits for shared memory writes to become visible to the async proxy.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_shared_to_global", "head_type": "__device__", "description": "Initiates a bulk-asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_commit_group", "head_type": "__device__", "description": "Combines all preceding bulk-asynchronous operations into a bulk async-group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_wait_group_read", "head_type": "__device__", "description": "Waits for bulk async-group operations to complete.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [259, 261], "model_response": [{"head": "cuda::device::experimental::cp_async_bulk_tensor_1d_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from global memory to shared memory in 1D.", "relation": "calls", "tail": "CUtensorMap", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_1d_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from global memory to shared memory in 1D.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_2d_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from global memory to shared memory in 2D.", "relation": "calls", "tail": "CUtensorMap", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_2d_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from global memory to shared memory in 2D.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_3d_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from global memory to shared memory in 3D.", "relation": "calls", "tail": "CUtensorMap", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_3d_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from global memory to shared memory in 3D.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_4d_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from global memory to shared memory in 4D.", "relation": "calls", "tail": "CUtensorMap", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_4d_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from global memory to shared memory in 4D.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_5d_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from global memory to shared memory in 5D.", "relation": "calls", "tail": "CUtensorMap", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_5d_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from global memory to shared memory in 5D.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_1d_shared_to_global", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from shared memory to global memory in 1D.", "relation": "calls", "tail": "CUtensorMap", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_2d_shared_to_global", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from shared memory to global memory in 2D.", "relation": "calls", "tail": "CUtensorMap", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_3d_shared_to_global", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from shared memory to global memory in 3D.", "relation": "calls", "tail": "CUtensorMap", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_4d_shared_to_global", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from shared memory to global memory in 4D.", "relation": "calls", "tail": "CUtensorMap", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_5d_shared_to_global", "head_type": "__device__", "description": "Initiates a bulk tensor asynchronous copy from shared memory to global memory in 5D.", "relation": "calls", "tail": "CUtensorMap", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [264, 267], "model_response": [{"head": "helloCUDA", "head_type": "__global__", "description": "A kernel function that utilizes printf to output thread information.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "cudaDeviceSynchronize", "head_type": "", "description": "Synchronizes all threads in the CUDA device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "malloc", "head_type": "__host__ __device__", "description": "Allocates memory from the device heap.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__nv_aligned_device_malloc", "head_type": "__device__", "description": "Allocates aligned memory from the device heap.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "free", "head_type": "__host__ __device__", "description": "Deallocates memory previously allocated.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "memcpy", "head_type": "__host__ __device__", "description": "Copies memory from source to destination.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "memset", "head_type": "__host__ __device__", "description": "Sets memory block to a specified value.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [267, 270], "model_response": [{"head": "mallocTest", "head_type": "__global__", "description": "Per thread memory allocation and initialization using malloc and memset in device code.", "relation": "calls", "tail": "malloc", "tail_type": "CUDA_API"}, {"head": "mallocTest", "head_type": "__global__", "description": "Per thread memory allocation and initialization using malloc and memset in device code.", "relation": "calls", "tail": "memset", "tail_type": "CUDA_API"}, {"head": "mallocTest", "head_type": "__global__", "description": "Per thread memory allocation and initialization using malloc and memset in device code.", "relation": "calls", "tail": "free", "tail_type": "CUDA_API"}, {"head": "mallocTest", "head_type": "__global__", "description": "Per thread block memory allocation with shared memory using malloc.", "relation": "calls", "tail": "malloc", "tail_type": "CUDA_API"}, {"head": "mallocTest", "head_type": "__global__", "description": "Per thread block memory allocation with shared memory using malloc.", "relation": "calls", "tail": "free", "tail_type": "CUDA_API"}, {"head": "allocmem", "head_type": "__global__", "description": "Allocation of memory per block and initialize it by allocation function between launches.", "relation": "calls", "tail": "malloc", "tail_type": "CUDA_API"}, {"head": "freemem", "head_type": "__global__", "description": "Free allocated block memory and print buffer content.", "relation": "calls", "tail": "free", "tail_type": "CUDA_API"}]}
{"pdf_page": [270, 274], "model_response": [{"head": "cudaLaunchKernelEx", "head_type": "CUDA_API", "description": "Launches a kernel with a specified runtime cluster size.", "relation": "calls", "tail": "cudaLaunchConfig_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaLaunchKernelEx", "head_type": "CUDA_API", "description": "Launches a kernel with a specified runtime cluster size.", "relation": "calls", "tail": "Func", "tail_type": "CUDA_API"}]}
{"pdf_page": [281, 284], "model_response": [{"head": "kernel", "head_type": "__global__", "description": "Loading an integer from global into shared memory", "relation": "calls", "tail": "this_thread_block", "tail_type": "CUDA_API"}, {"head": "kernel", "head_type": "__global__", "description": "Loading an integer from global into shared memory", "relation": "calls", "tail": "sync", "tail_type": "CUDA_API"}, {"head": "kernel", "head_type": "__global__", "description": "Loading an integer from global into shared memory", "relation": "calls", "tail": "thread_rank", "tail_type": "CUDA_API"}, {"head": "kernel", "head_type": "__global__", "description": "Loading an integer from global into shared memory", "relation": "calls", "tail": "thread_block", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [284, 287], "model_response": [{"head": "this_multi_grid", "head_type": "__device__", "description": "Constructs a multi_grid_group utilizing the cooperative multi-device API.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "is_valid", "head_type": "__device__", "description": "Returns whether the multi_grid_group can be used.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "sync", "head_type": "__device__", "description": "Synchronizes the threads named in the multi_grid_group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "num_threads", "head_type": "__device__", "description": "Gets the total number of threads in the multi_grid_group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "tiled_partition", "head_type": "__device__", "description": "Partitions the parent thread group into tiled subgroups based on size.", "relation": "calls", "tail": "this_thread_block", "tail_type": "CUDA_API"}, {"head": "this_thread_block", "head_type": "__device__", "description": "Obtains the default 'current thread block' group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "sync", "head_type": "__device__", "description": "Synchronizes the threads in a thread_block_tile subgroup.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "meta_group_rank", "head_type": "__device__", "description": "Determines the linear rank of the group within the set of tiles partitioned from a parent group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "block_tile_memory", "head_type": "__device__", "description": "Reserves shared memory for thread_block_tile usage when tiles are larger than 32 on hardware with Compute Capability <= 7.5.", "relation": "calls", "tail": "this_thread_block", "tail_type": "CUDA_API"}]}
{"pdf_page": [287, 290], "model_response": [{"head": "memcpy_async", "head_type": "CUDA_API", "description": "Perform asynchronous memory copy using thread_group to copy an int element.", "relation": "calls", "tail": "this_thread", "tail_type": "CUDA_API"}, {"head": "__global__ kernel", "head_type": "__global__", "description": "Kernel function for processing data by coalesced threads.", "relation": "calls", "tail": "coalesced_threads", "tail_type": "CUDA_API"}, {"head": "coalesced_group::sync", "head_type": "CUDA_API", "description": "Synchronize the coalesced threads within the group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "tiled_partition", "head_type": "CUDA_API", "description": "Partition a parent group into subgroups.", "relation": "calls", "tail": "this_thread_block", "tail_type": "CUDA_API"}]}
{"pdf_page": [290, 293], "model_response": [{"head": "oddEven", "head_type": "__global__", "description": "Divides a 32-sized tile into groups based on odd and even numbers.", "relation": "calls", "tail": "cg::binary_partition", "tail_type": "CUDA_API"}, {"head": "cluster_kernel", "head_type": "__global__", "description": "Kernel function to synchronize initialization of shared memory across the cluster.", "relation": "calls", "tail": "cluster.barrier_arrive", "tail_type": "CUDA_API"}, {"head": "cluster_kernel", "head_type": "__global__", "description": "Kernel function to synchronize initialization of shared memory across the cluster.", "relation": "calls", "tail": "cluster.barrier_wait", "tail_type": "CUDA_API"}, {"head": "cluster_kernel", "head_type": "__global__", "description": "Kernel function to synchronize initialization of shared memory across the cluster.", "relation": "calls", "tail": "cluster.sync", "tail_type": "CUDA_API"}]}
{"pdf_page": [293, 296], "model_response": [{"head": "kernel", "head_type": "__global__", "description": "Kernel function that streams data from global memory into shared memory blocks.", "relation": "calls", "tail": "memcpy_async", "tail_type": "CUDA_API"}, {"head": "kernel", "head_type": "__global__", "description": "Kernel function that streams data from global memory into shared memory blocks.", "relation": "calls", "tail": "wait", "tail_type": "CUDA_API"}, {"head": "kernel", "head_type": "__global__", "description": "Kernel function that streams data from global memory into shared memory blocks.", "relation": "calls", "tail": "wait_prior", "tail_type": "CUDA_API"}]}
{"pdf_page": [296, 299], "model_response": [{"head": "std_dev", "head_type": "__device__", "description": "Calculate approximate standard deviation of integers in vec", "relation": "calls", "tail": "cg::reduce", "tail_type": "CUDA_API"}, {"head": "block_reduce", "head_type": "__device__", "description": "Accepts input in *A and outputs a result into *sum, spreading the data equally within the block", "relation": "calls", "tail": "cg::reduce_update_async", "tail_type": "CUDA_API"}]}
{"pdf_page": [299, 303], "model_response": [{"head": "inclusive_scan", "head_type": "__device__", "description": "Performs an inclusive scan operation on data provided by threads in a group", "relation": "calls", "tail": "cg::tiled_partition", "tail_type": "CUDA_API"}, {"head": "exclusive_scan", "head_type": "__device__", "description": "Performs an exclusive scan operation on data provided by threads in a group", "relation": "calls", "tail": "cg::shfl", "tail_type": "CUDA_API"}, {"head": "exclusive_scan_update", "head_type": "__device__", "description": "Performs an exclusive scan operation and updates an atomic variable", "relation": "calls", "tail": "cuda::atomic", "tail_type": "CUDA_CLASS"}, {"head": "exclusive_scan_update", "head_type": "__device__", "description": "Performs an exclusive scan operation and updates an atomic variable", "relation": "calls", "tail": "cg::tiled_partition", "tail_type": "CUDA_API"}]}
{"pdf_page": [303, 307], "model_response": [{"head": "atomicAddOneRelaxed", "head_type": "__device__", "description": "atomicAddOneRelaxed performs an atomic addition operation in a relaxed memory order", "relation": "calls", "tail": "cg::invoke_one_broadcast", "tail_type": "CUDA_API"}, {"head": "cudaLaunchCooperativeKernel", "head_type": "CUDA_API", "description": "cudaLaunchCooperativeKernel launches a kernel with cooperative groups", "relation": "calls", "tail": "cudaGetDeviceProperties", "tail_type": "CUDA_API"}, {"head": "cudaOccupancyMaxActiveBlocksPerMultiprocessor", "head_type": "CUDA_API", "description": "cudaOccupancyMaxActiveBlocksPerMultiprocessor computes the maximum number of active blocks per SM", "relation": "calls", "tail": "cudaGetDeviceProperties", "tail_type": "CUDA_API"}, {"head": "cudaDeviceGetAttribute", "head_type": "CUDA_API", "description": "cudaDeviceGetAttribute gets attributes of the device", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchCooperativeKernelMultiDevice", "head_type": "CUDA_API", "description": "cudaLaunchCooperativeKernelMultiDevice launches kernels across multiple devices cooperatively", "relation": "calls", "tail": "cudaGetDeviceCount", "tail_type": "CUDA_API"}, {"head": "multi_grid.sync", "head_type": "CUDA_API", "description": "multi_grid.sync is used for multi-device grid synchronization", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [319, 322], "model_response": [{"head": "permute", "head_type": "__global__", "description": "This function performs permutation of data and recursively launches itself for further permutations.", "relation": "calls", "tail": "__syncthreads", "tail_type": "CUDA_API"}, {"head": "permute", "head_type": "__global__", "description": "This function performs permutation of data and recursively launches itself for further permutations.", "relation": "calls", "tail": "permute_data", "tail_type": "CUDA_API"}, {"head": "permute", "head_type": "__global__", "description": "This function performs permutation of data and recursively launches itself for further permutations.", "relation": "calls", "tail": "permute", "tail_type": "CUDA_API"}, {"head": "cudaGetParameterBuffer", "head_type": "__device__", "description": "Retrieves parameter buffer for kernel launch in device runtime.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchDevice", "head_type": "__device__", "description": "Launches a kernel on the device using device runtime.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [323, 327], "model_response": [{"head": "parentKernel", "head_type": "__global__", "description": "A kernel function to launch other child kernels.", "relation": "calls", "tail": "childKernel", "tail_type": "__global__"}, {"head": "parentKernel", "head_type": "__global__", "description": "A kernel function to launch other child kernels.", "relation": "calls", "tail": "cudaGetLastError", "tail_type": "CUDA_API"}, {"head": "parentKernel", "head_type": "__global__", "description": "A kernel function to launch other child kernels.", "relation": "calls", "tail": "tailKernel", "tail_type": "__global__"}, {"head": "parentKernel", "head_type": "__global__", "description": "A kernel function to launch other child kernels.", "relation": "calls", "tail": "cudaStreamTailLaunch", "tail_type": "CUDA_CLASS"}, {"head": "childKernel", "head_type": "__global__", "description": "A simple kernel function that prints 'Hello'.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "tailKernel", "head_type": "__global__", "description": "A simple kernel function that prints 'World!'.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [334, 337], "model_response": [{"head": "child_launch", "head_type": "__global__", "description": "Updates each element of the data array by incrementing it.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "parent_launch", "head_type": "__global__", "description": "Initializes data array and invokes child kernel.", "relation": "calls", "tail": "child_launch", "tail_type": "CUDA_API"}, {"head": "parent_launch", "head_type": "__global__", "description": "Synchronizes parent thread with the completion of child kernel.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}]}
{"pdf_page": [341, 344], "model_response": [{"head": "permute", "head_type": "__global__", "description": "This CUDA kernel function performs a permutation and further device-side kernel launches.", "relation": "calls", "tail": "__syncthreads", "tail_type": "CUDA_API"}, {"head": "permute", "head_type": "__global__", "description": "This CUDA kernel function performs a permutation and further device-side kernel launches.", "relation": "calls", "tail": "permute", "tail_type": "CUDA_API"}, {"head": "permute", "head_type": "__global__", "description": "This CUDA kernel function performs a permutation and further device-side kernel launches.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetLastError", "head_type": "CUDA_API", "description": "Retrieves the last error that has been produced by a CUDA function call.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetParameterBuffer", "head_type": "__device__", "description": "Generates the parameter buffer needed for launching a device kernel.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchDevice", "head_type": "__device__", "description": "Launches a device kernel execution using the provided parameters.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [345, 348], "model_response": [{"head": "cudaLaunchDevice", "head_type": "__device__", "description": "A function for device-side kernel launches using parameter buffers.", "relation": "calls", "tail": "cudaGetParameterBuffer", "tail_type": "CUDA_API"}, {"head": "cudaGetParameterBuffer", "head_type": "__device__", "description": "A function to obtain and fill parameter buffers for device-side kernel launches.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [348, 352], "model_response": [{"head": "childKernel", "head_type": "__global__", "description": "A CUDA kernel function that prints 'Hello'.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "parentKernel", "head_type": "__global__", "description": "A CUDA kernel function that invokes the childKernel and performs synchronization.", "relation": "calls", "tail": "childKernel", "tail_type": "CUDA_API"}, {"head": "parentKernel", "head_type": "__global__", "description": "A CUDA kernel function that invokes the childKernel and performs synchronization.", "relation": "calls", "tail": "cudaGetLastError", "tail_type": "CUDA_API"}, {"head": "parentKernel", "head_type": "__global__", "description": "A CUDA kernel function that invokes the childKernel and performs synchronization.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}]}
{"pdf_page": [355, 357], "model_response": [{"head": "cuDeviceGetAttribute", "head_type": "__host__", "description": "Queries the attribute of a device for Virtual Memory Management support.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemCreate", "head_type": "__host__", "description": "Allocates physical memory without device or host mappings.", "relation": "calls", "tail": "CUmemGenericAllocationHandle", "tail_type": "CUDA_CLASS"}, {"head": "cuMemGetAllocationGranularity", "head_type": "__host__", "description": "Queries the allocation granularity requirements.", "relation": "calls", "tail": "CUmemAllocationProp", "tail_type": "CUDA_CLASS"}, {"head": "cuMemCreate", "head_type": "__host__", "description": "Allocates physical memory without device or host mappings.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemExportToShareableHandle", "head_type": "__host__", "description": "Exports memory allocation as OS-specific handles for interprocess communication.", "relation": "calls", "tail": "CUmemCreate", "tail_type": "CUDA_API"}, {"head": "cuDeviceGetAttribute", "head_type": "__host__", "description": "Queries handle type support for exporting memory using OS-specific handles.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [357, 360], "model_response": [{"head": "cuDeviceGetAttribute", "head_type": "CUDA_API", "description": "Queries device attributes.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemGetAllocationPropertiesFromHandle", "head_type": "CUDA_API", "description": "Checks properties of allocated memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemAddressReserve", "head_type": "CUDA_API", "description": "Reserves a virtual address range.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "foo1 / foo2", "head_type": "__global__", "description": "Demonstrates virtual aliasing support with potential undefined behavior in concurrent execution.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyAsync", "head_type": "CUDA_API", "description": "Performs asynchronous memory copy with aliasing allowed at operation boundaries.", "relation": "calls", "tail": "cudaStreamWaitEvent", "tail_type": "CUDA_API"}, {"head": "cuMemMap", "head_type": "CUDA_API", "description": "Maps physical memory to a virtual address space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemSetAccess", "head_type": "CUDA_API", "description": "Sets access rights for mapped address ranges.", "relation": "calls", "tail": "CUmemAccessDesc", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [360, 363], "model_response": [{"head": "cudaDeviceGetAttribute", "head_type": "__host__", "description": "Queries attributes of a CUDA device.", "relation": "calls", "tail": "cudaDevAttrMemoryPoolsSupported", "tail_type": "CUDA_API"}, {"head": "cudaDeviceGetAttribute", "head_type": "__host__", "description": "Queries attributes of a CUDA device.", "relation": "calls", "tail": "cudaDevAttrMemoryPoolSupportedHandleTypes", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Performs asynchronous memory allocation.", "relation": "calls", "tail": "cudaStreamPerThread", "tail_type": "CUDA_API"}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "Performs asynchronous memory deallocation.", "relation": "calls", "tail": "cudaStreamPerThread", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Performs asynchronous memory allocation.", "relation": "calls", "tail": "cudaEventRecord", "tail_type": "CUDA_API"}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Waits for an event in a CUDA stream.", "relation": "calls", "tail": "cudaEventRecord", "tail_type": "CUDA_API"}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "Performs asynchronous memory deallocation.", "relation": "calls", "tail": "cudaStreamWaitEvent", "tail_type": "CUDA_API"}, {"head": "cudaMalloc", "head_type": "__host__", "description": "Performs synchronous memory allocation.", "relation": "calls", "tail": "cudaFreeAsync", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Performs asynchronous memory allocation.", "relation": "calls", "tail": "cudaStreamSynchronize", "tail_type": "CUDA_API"}, {"head": "cudaFree", "head_type": "__host__", "description": "Performs synchronous memory deallocation.", "relation": "calls", "tail": "cudaStreamSynchronize", "tail_type": "CUDA_API"}]}
{"pdf_page": [363, 365], "model_response": [{"head": "cudaDeviceGetDefaultMempool", "head_type": "CUDA_API", "description": "Retrieves the default memory pool of a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAccess", "head_type": "CUDA_API", "description": "Modifies the accessibility of the default memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolGetAccess", "head_type": "CUDA_API", "description": "Queries the accessibility of the default memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolCreate", "head_type": "CUDA_API", "description": "Creates an explicit memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAttribute", "head_type": "CUDA_API", "description": "Sets an attribute for a memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocAsync", "head_type": "CUDA_API", "description": "Asynchronously allocates device memory.", "relation": "calls", "tail": "cudaFreeAsync", "tail_type": "CUDA_API"}, {"head": "kernel", "head_type": "__global__", "description": "Performs operations on allocated memory asynchronously.", "relation": "calls", "tail": "cudaMallocAsync", "tail_type": "CUDA_API"}, {"head": "cudaFreeAsync", "head_type": "CUDA_API", "description": "Asynchronously frees device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamSynchronize", "head_type": "CUDA_API", "description": "Synchronizes operations on a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolTrimTo", "head_type": "CUDA_API", "description": "Trims a memory pool's memory footprint.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [365, 369], "model_response": [{"head": "getUsageStatistics", "head_type": "__device__", "description": "A function to get usage statistics of a given memory pool.", "relation": "calls", "tail": "cudaMemPoolGetAttribute", "tail_type": "CUDA_API"}, {"head": "resetStatistics", "head_type": "__device__", "description": "A function to reset high watermarks of memory pool attributes.", "relation": "calls", "tail": "cudaMemPoolSetAttribute", "tail_type": "CUDA_API"}, {"head": "setAccessOnDevice", "head_type": "__device__", "description": "A function to set access permissions for a memory pool from a specific device.", "relation": "calls", "tail": "cudaDeviceCanAccessPeer", "tail_type": "CUDA_API"}, {"head": "setAccessOnDevice", "head_type": "__device__", "description": "A function to set access permissions for a memory pool from a specific device.", "relation": "calls", "tail": "cudaMemPoolSetAccess", "tail_type": "CUDA_API"}]}
{"pdf_page": [368, 372], "model_response": [{"head": "cudaMemPoolSetAccess", "head_type": "CUDA_API", "description": "Enables access from any GPU for an imported memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolExportToShareableHandle", "head_type": "CUDA_API", "description": "Retrieves an OS native handle for sharing IPC capable pools.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolImportFromShareableHandle", "head_type": "CUDA_API", "description": "Creates an imported memory pool from a shareable handle.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolExportPointer", "head_type": "CUDA_API", "description": "Exports allocation pointer data for sharing with importing process.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolImportPointer", "head_type": "CUDA_API", "description": "Imports allocation pointer data in the importing process.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocAsync", "head_type": "CUDA_API", "description": "Allocates memory asynchronously using a specified memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeAsync", "head_type": "CUDA_API", "description": "Frees asynchronous memory allocations.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaIpcGetEventHandle", "head_type": "CUDA_API", "description": "Gets an interprocess event handle.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventRecord", "head_type": "CUDA_API", "description": "Records an event in a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventCreate", "head_type": "CUDA_API", "description": "Creates a CUDA event with specified flags.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "CUDA_API", "description": "Waits for an event to be completed before proceeding with stream operations.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaIpcOpenEventHandle", "head_type": "CUDA_API", "description": "Opens an IPC event handle.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaIpcEventRecord", "head_type": "CUDA_API", "description": "Records events for synchronization across processes.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [375, 379], "model_response": [{"head": "cudaGraphAddMemAllocNode", "head_type": "CUDA_API", "description": "Explicitly creates a memory allocation node in a CUDA graph.", "relation": "calls", "tail": "CUDA_MEM_ALLOC_NODE_PARAMS", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphAddMemFreeNode", "head_type": "CUDA_API", "description": "Creates a memory freeing node in a CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddKernelNode", "head_type": "CUDA_API", "description": "Adds a kernel execution node within a CUDA graph.", "relation": "calls", "tail": "nodeParams", "tail_type": "CUDA_CLASS"}, {"head": "cudaMallocAsync", "head_type": "CUDA_API", "description": "Performs asynchronous memory allocation with stream ordering in CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeAsync", "head_type": "CUDA_API", "description": "Performs asynchronous memory freeing with stream ordering in CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamEndCapture", "head_type": "CUDA_API", "description": "Ends the capture of CUDA operations into a graph.", "relation": "calls", "tail": "graph", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphInstantiate", "head_type": "CUDA_API", "description": "Instantiates a graph for execution.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphLaunch", "head_type": "CUDA_API", "description": "Launches a previously instantiated graph onto a stream.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [379, 382], "model_response": [{"head": "cudaGraphAddMemAllocNode", "head_type": "CUDA_API", "description": "Add a memory allocation node to the graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddKernelNode", "head_type": "CUDA_API", "description": "Add a kernel node to the graph for executing kernels.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddEventRecordNode", "head_type": "CUDA_API", "description": "Add an event record node to graph for recording events.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddEventWaitNode", "head_type": "CUDA_API", "description": "Add an event wait node to the graph for waiting on events.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddMemFreeNode", "head_type": "CUDA_API", "description": "Add a memory free node to the graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphInstantiate", "head_type": "CUDA_API", "description": "Instantiate an executable version of the graph.", "relation": "calls", "tail": "cudaGraphAddMemAllocNode", "tail_type": "CUDA_API"}, {"head": "cudaGraphInstantiateWithFlags", "head_type": "CUDA_API", "description": "Instantiate an executable graph with the specified flags.", "relation": "calls", "tail": "cudaGraphAddKernelNode", "tail_type": "CUDA_API"}, {"head": "cudaGraphLaunch", "head_type": "CUDA_API", "description": "Launch an instantiated graph on a stream.", "relation": "calls", "tail": "cudaGraphInstantiate", "tail_type": "CUDA_API"}, {"head": "cudaStreamBeginCapture", "head_type": "CUDA_API", "description": "Begin capturing a stream to create a CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamEndCapture", "head_type": "CUDA_API", "description": "End capture of a stream to finish creating a CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocAsync", "head_type": "CUDA_API", "description": "Allocate device memory asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeAsync", "head_type": "CUDA_API", "description": "Free device memory asynchronously.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [386, 388], "model_response": [{"head": "cudaDeviceGetGraphMemAttribute", "head_type": "__host__", "description": "API to query graph memory footprint.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddMemAllocNode", "head_type": "__host__", "description": "API to add a memory allocation node to a CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamBeginCapture", "head_type": "__host__", "description": "API to begin capturing CUDA operations in a stream.", "relation": "calls", "tail": "cudaMallocAsync", "tail_type": "CUDA_API"}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "API to end capture of operations in a CUDA stream and create a graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "API to set access permissions for a memory pool.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [483, 485], "model_response": [{"head": "cuInit", "head_type": "__host__", "description": "Initialize the CUDA driver API.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGetCount", "head_type": "__host__", "description": "Get the number of CUDA-capable devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGet", "head_type": "__host__", "description": "Get a handle for a specific CUDA device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuCtxCreate", "head_type": "__host__", "description": "Create a CUDA context.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuModuleLoad", "head_type": "__host__", "description": "Load a CUDA module from binary file.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemAlloc", "head_type": "__host__", "description": "Allocate memory on the CUDA device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemcpyHtoD", "head_type": "__host__", "description": "Copy data from host memory to device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuModuleGetFunction", "head_type": "__host__", "description": "Retrieve a kernel function handle from a module.", "relation": "calls", "tail": "cuModuleLoad", "tail_type": "CUDA_API"}, {"head": "cuLaunchKernel", "head_type": "__host__", "description": "Launch a CUDA kernel.", "relation": "calls", "tail": "cuMemcpyHtoD", "tail_type": "CUDA_API"}]}
{"pdf_page": [485, 487], "model_response": [{"head": "cuModuleLoad", "head_type": "__host__", "description": "This function loads a compiled module into the current context.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuModuleGetFunction", "head_type": "__host__", "description": "This function retrieves a handle to a function in a loaded module.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuModuleLoadDataEx", "head_type": "__host__", "description": "This function loads a module from memory with options.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuLinkCreate", "head_type": "__host__", "description": "This function creates a linker invocation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuLinkAddData", "head_type": "__host__", "description": "This function adds a data item to a linker invocation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuLinkComplete", "head_type": "__host__", "description": "This function completes a linker invocation and produces the output.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuModuleLoadData", "head_type": "__host__", "description": "This function loads a module from data.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuLinkDestroy", "head_type": "__host__", "description": "This function destroys a linker invocation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuLaunchKernel", "head_type": "__host__", "description": "This function launches a CUDA kernel with specific execution parameters.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [487, 490], "model_response": [{"head": "cuLaunchKernel", "head_type": "CUDA_API", "description": "Launches CUDA kernel with specified execution configuration.", "relation": "calls", "tail": "CU_LAUNCH_PARAM_BUFFER_POINTER", "tail_type": "CUDA_API"}, {"head": "cuLaunchKernel", "head_type": "CUDA_API", "description": "Launches CUDA kernel with specified execution configuration.", "relation": "calls", "tail": "CU_LAUNCH_PARAM_BUFFER_SIZE", "tail_type": "CUDA_API"}, {"head": "cuLaunchKernel", "head_type": "CUDA_API", "description": "Launches CUDA kernel with specified execution configuration.", "relation": "calls", "tail": "CU_LAUNCH_PARAM_END", "tail_type": "CUDA_API"}, {"head": "cuMemAlloc", "head_type": "CUDA_API", "description": "Allocates device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMalloc", "head_type": "CUDA_API", "description": "Allocates device memory with runtime API.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [490, 493], "model_response": [{"head": "cuStreamBeginCapture", "head_type": "CUDA_API", "description": "Initiates capture of a sequence of CUDA operations.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuStreamBeginCapture_v2", "head_type": "CUDA_API", "description": "Initiates capture of a sequence of CUDA operations with a specific stream capture mode.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuGetProcAddress", "head_type": "CUDA_API", "description": "Retrieves the address of the specified driver API function.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDriverEntryPoint", "head_type": "CUDA_API", "description": "Retrieves the ABI compatible version of the requested driver symbol using the CUDA runtime version.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDriverGetVersion", "head_type": "CUDA_API", "description": "Retrieves the CUDA driver version.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [493, 496], "model_response": [{"head": "cuGetProcAddress", "head_type": "CUDA_API", "description": "Retrieves a function pointer to a specified symbol.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "CUDA_API", "description": "Retrieves a function pointer to a specified symbol using runtime version.", "relation": "calls", "tail": "cuDriverGetVersion", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "CUDA_API", "description": "Retrieves a function pointer for API version maintenance.", "relation": "calls", "tail": "cuFoo", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "CUDA_API", "description": "Retrieves a function pointer for API version maintenance based on conditions.", "relation": "calls", "tail": "cuFoo", "tail_type": "CUDA_API"}]}
{"pdf_page": [496, 499], "model_response": [{"head": "cudaGetDriverEntryPoint", "head_type": "CUDA_API", "description": "Obtains a function pointer to a CUDA Driver API using the Runtime API.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "CUDA_API", "description": "Obtains a function pointer to a CUDA Driver API using dynamic linking.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}, {"head": "cudaDriverGetVersion", "head_type": "CUDA_API", "description": "Gets the CUDA Driver version.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuGetProcAddress", "head_type": "CUDA_API", "description": "Obtains the function pointer based on the driver version obtained via runtime.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "CUDA_API", "description": "Obtains a function pointer for cuCtxCreate using updated CUDA version.", "relation": "calls", "tail": "cuCtxCreate", "tail_type": "CUDA_API"}]}
{"pdf_page": [511, 514], "model_response": [{"head": "cudaMalloc", "head_type": "single", "description": "Allocate memory on the GPU device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpy", "head_type": "single", "description": "Copy data between host and device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "single", "description": "Free memory on the GPU device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "single", "description": "Synchronize host and device execution.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocManaged", "head_type": "single", "description": "Allocate managed memory accessible from both host and device.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [514, 518], "model_response": [{"head": "printme", "head_type": "__global__", "description": "Prints a string passed to it.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "printme", "head_type": "__global__", "description": "Prints a string passed to it.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "write_value", "head_type": "__global__", "description": "Writes a given value to a pointer.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [518, 521], "model_response": [{"head": "cudaMemPrefetchAsync", "head_type": "__host__", "description": "Asynchronously prefetches memory to a destination device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemAdvise", "head_type": "__host__", "description": "Sets advice for a given memory region to optimize memory access.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemRangeGetAttribute", "head_type": "__host__", "description": "Queries an attribute of the specified memory range.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [521, 525], "model_response": [{"head": "kernel", "head_type": "__global__", "description": "A kernel that prints the first 8 characters of an input character array to the standard output stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "CUDA_API", "description": "Synchronizes the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocManaged", "head_type": "CUDA_API", "description": "Allocates managed memory on the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "CUDA_API", "description": "Frees the memory allocated by CUDA.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [527, 529], "model_response": [{"head": "write", "head_type": "__global__", "description": "Executes a kernel to compute a+b+threadIdx.x and stores it in ret.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "append", "head_type": "__global__", "description": "Executes a kernel to add a+b+threadIdx.x to ret.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "test_malloc", "head_type": "__host__", "description": "Performs memory allocation and kernel executions with direct access hints.", "relation": "calls", "tail": "cudaMemAdvise", "tail_type": "CUDA_API"}, {"head": "test_malloc", "head_type": "__host__", "description": "Performs memory allocation and kernel executions with direct access hints.", "relation": "calls", "tail": "write", "tail_type": "__global__"}, {"head": "test_malloc", "head_type": "__host__", "description": "Performs memory allocation and kernel executions with direct access hints.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "test_malloc", "head_type": "__host__", "description": "Performs memory allocation and kernel executions with direct access hints.", "relation": "calls", "tail": "append", "tail_type": "__global__"}, {"head": "write", "head_type": "__global__", "description": "Executes a kernel to compute a+b+threadIdx.x and stores it in ret.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "append", "head_type": "__global__", "description": "Executes a kernel to add a+b+threadIdx.x to ret.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "test_managed", "head_type": "__host__", "description": "Uses CUDA Managed Memory to demonstrate kernel execution and memory synchronization.", "relation": "calls", "tail": "cudaMallocManaged", "tail_type": "CUDA_API"}, {"head": "test_managed", "head_type": "__host__", "description": "Uses CUDA Managed Memory to demonstrate kernel execution and memory synchronization.", "relation": "calls", "tail": "cudaMemAdvise", "tail_type": "CUDA_API"}, {"head": "test_managed", "head_type": "__host__", "description": "Uses CUDA Managed Memory to demonstrate kernel execution and memory synchronization.", "relation": "calls", "tail": "write", "tail_type": "__global__"}, {"head": "test_managed", "head_type": "__host__", "description": "Uses CUDA Managed Memory to demonstrate kernel execution and memory synchronization.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "test_managed", "head_type": "__host__", "description": "Uses CUDA Managed Memory to demonstrate kernel execution and memory synchronization.", "relation": "calls", "tail": "append", "tail_type": "__global__"}, {"head": "test_managed", "head_type": "__host__", "description": "Uses CUDA Managed Memory to demonstrate kernel execution and memory synchronization.", "relation": "calls", "tail": "cudaFree", "tail_type": "CUDA_API"}]}
{"pdf_page": [532, 535], "model_response": [{"head": "kernel", "head_type": "__global__", "description": "Executes with concurrent GPU kernel access.", "relation": "calls", "tail": "cudaSetDevice", "tail_type": "CUDA_API"}, {"head": "cudaStreamAttachMemAsync", "head_type": "CUDA_API", "description": "Associates a memory region with a specified stream for Unified Memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "kernel", "head_type": "__global__", "description": "Modifies a managed variable.", "relation": "calls", "tail": "cudaStreamAttachMemAsync", "tail_type": "CUDA_API"}, {"head": "run_task", "head_type": "__host__", "description": "Performs tasks in its own private stream.", "relation": "calls", "tail": "cudaStreamAttachMemAsync", "tail_type": "CUDA_API"}]}
{"pdf_page": [547, 550], "model_response": [{"head": "cuMemCreate", "head_type": "CUDA_API", "description": "Allocates physical memory in pinned type with location type CU_MEM_LOCATION_TYPE_HOST_NUMA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemGetAllocationGranularity", "head_type": "CUDA_API", "description": "Gets the allocation granularity for a specified memory type and location.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemAddressReserve", "head_type": "CUDA_API", "description": "Reserves an address space for future mapping.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemMap", "head_type": "CUDA_API", "description": "Maps the physical memory allocation to the reserved address space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemSetAccess", "head_type": "CUDA_API", "description": "Sets access flags for the mapped memory ranges to provide read and write access.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolCreate", "head_type": "CUDA_API", "description": "Creates a memory pool on a specified node with defined properties.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAccess", "head_type": "CUDA_API", "description": "Sets peer access for the memory pool using access descriptors.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetMemPool", "head_type": "CUDA_API", "description": "Sets a created memory pool to the resident device and allocates memory using cudaMallocAsync.", "relation": "calls", "tail": "cudaMallocAsync", "tail_type": "CUDA_API"}, {"head": "cuMemExportToShareableHandle", "head_type": "CUDA_API", "description": "Exports a memory allocation handle to a shareable handle for inter-node communication.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemImportFromShareableHandle", "head_type": "CUDA_API", "description": "Imports a shareable handle from another node and treats as a local fabric handle.", "relation": "single", "tail": "", "tail_type": ""}]}
