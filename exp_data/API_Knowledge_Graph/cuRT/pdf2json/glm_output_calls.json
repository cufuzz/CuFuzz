{"pdf_page": [26, 29], "model_response": [{"head": "__global__ void VecAdd", "head_type": "__global__", "description": "Kernel for vector addition", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__global__ void MatAdd", "head_type": "__global__", "description": "Kernel for matrix addition", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads", "head_type": "__device__", "description": "Intrinsic function to synchronize threads within a block", "relation": "calls", "tail": "__syncthreads", "tail_type": "CUDA_API"}, {"head": "Cooperative Groups API", "head_type": "__host__", "description": "Set of thread-synchronization primitives", "relation": "calls", "tail": "__syncthreads", "tail_type": "CUDA_API"}]}
{"pdf_page": [29, 32], "model_response": [{"head": "__cluster_dims__", "head_type": "__global__", "description": "A compiler time kernel attribute to define the cluster size of a kernel at compile time.", "relation": "calls", "tail": "cluster_kernel", "tail_type": "CUDA_API"}, {"head": "cudaLaunchKernelEx", "head_type": "__host__", "description": "A CUDA API to launch a kernel with runtime cluster size configuration.", "relation": "calls", "tail": "cluster_kernel", "tail_type": "CUDA_API"}, {"head": "cudaOccupancyMaxPotentialClusterSize", "head_type": "__host__", "description": "A CUDA API to query the maximum potential cluster size based on the GPU architecture.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cluster.sync", "head_type": "__device__", "description": "A Cluster Group API to perform hardware-supported synchronization within a cluster group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "num_threads", "head_type": "__device__", "description": "A Cluster Group API to query the number of threads in a cluster group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "num_blocks", "head_type": "__device__", "description": "A Cluster Group API to query the number of blocks in a cluster group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "dim_threads", "head_type": "__device__", "description": "A Cluster Group API to query the dimension of threads in a cluster group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "dim_blocks", "head_type": "__device__", "description": "A Cluster Group API to query the dimension of blocks in a cluster group.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [32, 35], "model_response": [{"head": "cuda::memcpy_async", "head_type": "__device__", "description": "Used to move data asynchronously from global memory while computing in the GPU.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_API"}, {"head": "cooperaive_groups::memcpy_async", "head_type": "__device__", "description": "A library that provides asynchronous memory copy operations.", "relation": "calls", "tail": "cuda::pipeline", "tail_type": "CUDA_API"}, {"head": "cuda::barrier", "head_type": "__device__", "description": "A synchronization object used to synchronize the completion of asynchronous operations.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::pipeline", "head_type": "__device__", "description": "A synchronization object that can be used to manage asynchronous data copies and operations.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [41, 45], "model_response": [{"head": "cudaMalloc", "head_type": "__host__", "description": "Allocates device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpy", "head_type": "__host__", "description": "Copies memory between host and device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "VecAdd", "head_type": "__global__", "description": "A vector addition kernel that operates on device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocPitch", "head_type": "__host__", "description": "Allocates pitched device memory for 2D arrays.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMalloc3D", "head_type": "__host__", "description": "Allocates 3D device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyToSymbol", "head_type": "__host__", "description": "Copies data to a variable in the device's global memory space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyFromSymbol", "head_type": "__host__", "description": "Copies data from a variable in the device's global memory space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetSymbolAddress", "head_type": "__host__", "description": "Retrieves the address of a variable in the device's global memory space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetSymbolSize", "head_type": "__host__", "description": "Retrieves the size of a variable in the device's global memory space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetLimit", "head_type": "__host__", "description": "Sets the L2 cache size for persisting accesses.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [45, 49], "model_response": [{"head": "cudaStreamCreate", "head_type": "__host__", "description": "Creates a CUDA stream", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Queries the properties of the CUDA device", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetLimit", "head_type": "__host__", "description": "Sets the L2 cache set-aside size for persisting memory accesses", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamAttrValue", "head_type": "__host__", "description": "Stream level attributes data structure", "relation": "calls", "tail": "cudaStreamAttributeAccessPolicyWindow", "tail_type": "CUDA_API"}, {"head": "cudaStreamSetAttribute", "head_type": "__host__", "description": "Sets attributes to a CUDA stream", "relation": "calls", "tail": "cudaStreamAttributeAccessPolicyWindow", "tail_type": "CUDA_API"}, {"head": "cuda_kernelA", "head_type": "__global__", "description": "CUDA kernel that benefits from L2 persistence", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda_kernelB", "head_type": "__global__", "description": "Another CUDA kernel in the same stream benefiting from persistence", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaCtxResetPersistingL2Cache", "head_type": "__host__", "description": "Removes any persistent lines in L2 cache", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda_kernelC", "head_type": "__global__", "description": "CUDA kernel that benefits from full L2 cache in normal mode", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [49, 54], "model_response": [{"head": "cudaMalloc", "head_type": "__host__", "description": "Allocates memory on the device.", "relation": "calls", "tail": "CUDA_API", "tail_type": "CUDA_API"}, {"head": "cudaMemcpy", "head_type": "__host__", "description": "Copies memory for host to device or vice versa.", "relation": "calls", "tail": "CUDA_API", "tail_type": "CUDA_API"}, {"head": "MatMulKernel", "head_type": "__global__", "description": "Performs matrix multiplication using CUDA kernels.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads", "head_type": "__device__", "description": "Synchronizes all threads in a block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "GetElement", "head_type": "__device__", "description": "Gets an element from a matrix.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "SetElement", "head_type": "__device__", "description": "Sets an element in a matrix.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "GetSubMatrix", "head_type": "__device__", "description": "Gets a sub-matrix from a larger matrix.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees allocated memory on the device.", "relation": "calls", "tail": "CUDA_API", "tail_type": "CUDA_API"}]}
{"pdf_page": [55, 58], "model_response": [{"head": "clusterHist_kernel", "head_type": "__global__", "description": "CUDA kernel to compute histograms in shared or distributed shared memory.", "relation": "calls", "tail": "atomicAdd", "tail_type": "CUDA_API"}, {"head": "clusterHist_kernel", "head_type": "__global__", "description": "CUDA kernel to compute histograms in shared or distributed shared memory.", "relation": "calls", "tail": "cluster.sync", "tail_type": "CUDA_API"}, {"head": "clusterHist_kernel", "head_type": "__global__", "description": "CUDA kernel to compute histograms in shared or distributed shared memory.", "relation": "calls", "tail": "cudaLaunchKernelEx", "tail_type": "CUDA_API"}, {"head": "cudaLaunchKernelEx", "head_type": "__host__", "description": "Launches a CUDA kernel with extended launch attributes.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaHostAlloc", "head_type": "__host__", "description": "Allocates page-locked host memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeHost", "head_type": "__host__", "description": "Frees page-locked host memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaHostRegister", "head_type": "__host__", "description": "Page-locks a range of memory allocated by malloc().", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [60, 62], "model_response": [{"head": "cudaLaunchKernelEx", "head_type": "__host__", "description": "Launches a CUDA kernel with specified launch attributes.", "relation": "calls", "tail": "myKernel", "tail_type": "CUDA_API"}, {"head": "cudaStreamSetAttribute", "head_type": "__host__", "description": "Sets an attribute on a CUDA stream.", "relation": "calls", "tail": "cudaLaunchAttributeMemSyncDomainMap", "tail_type": "CUDA_CLASS"}, {"head": "cudaLaunchAttributeValue", "head_type": "__host__", "description": "Specifies a value for a CUDA launch attribute.", "relation": "used_by", "tail": "cudaStreamSetAttribute", "tail_type": "CUDA_API"}, {"head": "cudaLaunchAttribute", "head_type": "__host__", "description": "Specifies an attribute for a CUDA kernel launch.", "relation": "used_by", "tail": "cudaLaunchKernelEx", "tail_type": "CUDA_API"}]}
{"pdf_page": [63, 67], "model_response": [{"head": "cudaStreamCreate", "head_type": "__host__", "description": "Creates a stream object.", "relation": "calls", "tail": "CUDA API", "tail_type": "CUDA_API"}, {"head": "cudaMallocHost", "head_type": "__host__", "description": "Allocates page-locked host memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyAsync", "head_type": "__host__", "description": "Performs an asynchronous memory copy.", "relation": "calls", "tail": "CUDA API", "tail_type": "CUDA_API"}, {"head": "MyKernel", "head_type": "__global__", "description": "A user-defined kernel function.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamDestroy", "head_type": "__host__", "description": "Destroys a stream object.", "relation": "calls", "tail": "CUDA API", "tail_type": "CUDA_API"}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits for all preceding commands in all streams of all host threads to complete.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamSynchronize", "head_type": "__host__", "description": "Waits for all preceding commands in the given stream to complete.", "relation": "calls", "tail": "CUDA API", "tail_type": "CUDA_API"}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Makes all commands added to the given stream after the call delay their execution until the given event has completed.", "relation": "calls", "tail": "CUDA API", "tail_type": "CUDA_API"}, {"head": "cudaStreamQuery", "head_type": "__host__", "description": "Provides a way to know if all preceding commands in a stream have completed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchHostFunc", "head_type": "__host__", "description": "Inserts a CPU function call into a stream.", "relation": "calls", "tail": "CUDA API", "tail_type": "CUDA_API"}, {"head": "cudaStreamCreateWithPriority", "head_type": "__host__", "description": "Creates a stream object with a specified priority.", "relation": "calls", "tail": "CUDA API", "tail_type": "CUDA_API"}, {"head": "cudaDeviceGetStreamPriorityRange", "head_type": "__host__", "description": "Gets the range of stream priorities for the current device.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [67, 70], "model_response": [{"head": "cudaTriggerProgrammaticLaunchCompletion", "head_type": "__global__", "description": "Called within a primary kernel to signal that a secondary kernel can launch.", "relation": "calls", "tail": "", "tail_type": ""}, {"head": "cudaGridDependencySynchronize", "head_type": "__global__", "description": "Called within a secondary kernel to block until all primary kernels upon which it depends have completed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchKernelEx", "head_type": "__host__", "description": "Used to launch a secondary kernel with attributes including programmatic stream serialization.", "relation": "calls", "tail": "secondary_kernel", "tail_type": "CUDA_API"}, {"head": "cudaGraphEdgeData", "head_type": "__host__", "description": "Specifies edge data for a CUDA graph edge, including programmatic dependency type.", "relation": "calls", "tail": "cudaGraphDependencyTypeProgrammatic", "tail_type": "CUDA_API"}]}
{"pdf_page": [72, 75], "model_response": [{"head": "cudaGraphCreate", "head_type": "__host__", "description": "Creates an empty CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddKernelNode", "head_type": "__host__", "description": "Adds a kernel node to the CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddDependencies", "head_type": "__host__", "description": "Sets up dependencies between nodes in the CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamBeginCapture", "head_type": "__host__", "description": "Begins capturing work launched into a CUDA stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends capturing work launched into a CUDA stream and returns the created graph.", "relation": "calls", "tail": "cudaStream", "tail_type": "CUDA_API"}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event in a CUDA stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Makes a CUDA stream wait on an event.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [75, 77], "model_response": [{"head": "cudaStreamAttachMemAsync", "head_type": "__host__", "description": "Enqueues an asynchronous operation to attach memory to a stream.", "relation": "single"}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends stream capture and takes associated streams out of capture mode.", "relation": "single"}, {"head": "cudaStreamAddCallback", "head_type": "__host__", "description": "Adds a callback to a stream.", "tail": "launchWork", "tail_type": "CUDA_API", "relation": "calls"}, {"head": "cudaUserObjectCreate", "head_type": "__host__", "description": "Creates a CUDA user object with a specified destructor callback.", "tail": "Object", "tail_type": "CUDA_CLASS", "relation": "calls"}, {"head": "cudaGraphRetainUserObject", "head_type": "__host__", "description": "Increases the reference count of a user object in a graph.", "tail": "cudaUserObjectCreate", "tail_type": "CUDA_API", "relation": "calls"}, {"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a graph.", "tail": "cudaGraphRetainUserObject", "tail_type": "CUDA_API", "relation": "calls"}, {"head": "cudaGraphDestroy", "head_type": "__host__", "description": "Destroys a graph.", "relation": "single"}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches an instantiated graph.", "tail": "cudaGraphInstantiate", "tail_type": "CUDA_API", "relation": "calls"}, {"head": "cudaGraphExecDestroy", "head_type": "__host__", "description": "Destroys an executable graph.", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API", "relation": "calls"}, {"head": "cudaStreamSynchronize", "head_type": "__host__", "description": "Waits for a stream to complete all work.", "tail": "cudaGraphExecDestroy", "tail_type": "CUDA_API", "relation": "calls"}]}
{"pdf_page": [77, 81], "model_response": [{"head": "cudaGraphExecUpdate", "head_type": "__host__", "description": "Allows an instantiated graph to be updated with parameters from a topologically identical graph.", "relation": "calls", "tail": "cudaGraphExecDestroy", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecUpdate", "head_type": "__host__", "description": "Allows an instantiated graph to be updated with parameters from a topologically identical graph.", "relation": "calls", "tail": "cudaGraphInstantiate", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecKernelNodeSetParams", "head_type": "__host__", "description": "Directly updates the parameters of a kernel node in an instantiated graph.", "relation": "calls", "tail": "cudaGraphExecMemcpyNodeSetParams", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecKernelNodeSetParams", "head_type": "__host__", "description": "Directly updates the parameters of a kernel node in an instantiated graph.", "relation": "calls", "tail": "cudaGraphExecMemsetNodeSetParams", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecKernelNodeSetParams", "head_type": "__host__", "description": "Directly updates the parameters of a kernel node in an instantiated graph.", "relation": "calls", "tail": "cudaGraphExecHostNodeSetParams", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecChildGraphNodeSetParams", "head_type": "__host__", "description": "Updates the parameters of a child graph node in an instantiated graph.", "relation": "calls", "tail": "cudaGraphExecEventRecordNodeSetEvent", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecChildGraphNodeSetParams", "head_type": "__host__", "description": "Updates the parameters of a child graph node in an instantiated graph.", "relation": "calls", "tail": "cudaGraphExecEventWaitNodeSetEvent", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecChildGraphNodeSetParams", "head_type": "__host__", "description": "Updates the parameters of a child graph node in an instantiated graph.", "relation": "calls", "tail": "cudaGraphExecExternalSemaphoresSignalNodeSetParams", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecChildGraphNodeSetParams", "head_type": "__host__", "description": "Updates the parameters of a child graph node in an instantiated graph.", "relation": "calls", "tail": "cudaGraphExecExternalSemaphoresWaitNodeSetParams", "tail_type": "CUDA_API"}, {"head": "cudaGraphNodeSetEnabled", "head_type": "__host__", "description": "Enables or disables a node in an instantiated graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphNodeGetEnabled", "head_type": "__host__", "description": "Queries the enabled state of a node in an instantiated graph.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [81, 87], "model_response": [{"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a graph for device launch.", "relation": "calls", "tail": "cudaGraphUpload", "tail_type": "CUDA_API"}, {"head": "cudaGraphInstantiateWithParams", "head_type": "__host__", "description": "Instantiates a graph for device launch with additional parameters, including upload.", "relation": "calls", "tail": "cudaGraphUpload", "tail_type": "CUDA_API"}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches a graph from the host, implicitly uploading it if necessary.", "relation": "calls", "tail": "cudaGraphUpload", "tail_type": "CUDA_API"}, {"head": "launchFireAndForgetGraph", "head_type": "__global__", "description": "Launches a graph in fire-and-forget mode from a kernel.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "launchTailGraph", "head_type": "__global__", "description": "Launches a graph in tail mode from a kernel.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "create_graph", "head_type": "__host__", "description": "Creates a graph (not a CUDA API function, but assumed to be a user-defined function that creates a graph).", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [87, 91], "model_response": [{"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a graph.", "relation": "calls", "tail": "cudaGraphUpload", "tail_type": "CUDA_API"}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches a graph.", "relation": "calls", "tail": "cudaStreamGraphTailLaunch", "tail_type": "CUDA_API"}, {"head": "cudaStreamBeginCapture", "head_type": "__host__", "description": "Begins capturing CUDA work on a stream.", "relation": "calls", "tail": "cudaStreamEndCapture", "tail_type": "CUDA_API"}, {"head": "cudaGraphConditionalHandleCreate", "head_type": "__host__", "description": "Creates a conditional handle for a graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphSetConditional", "head_type": "__device__", "description": "Sets the conditional value for a graph node.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "launchTailGraph", "head_type": "__global__", "description": "Launches a tail graph.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "launchSiblingGraph", "head_type": "__global__", "description": "Launches a sibling graph.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}]}
{"pdf_page": [91, 95], "model_response": [{"head": "cudaGraphSetConditional", "head_type": "__global__", "description": "Sets the conditional value of a conditional node in a CUDA graph.", "relation": "calls", "tail": "cudaGraphConditionalHandle", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphCreate", "head_type": "__host__", "description": "Creates an empty CUDA graph.", "relation": "single"}, {"head": "cudaGraphConditionalHandleCreate", "head_type": "__host__", "description": "Creates a conditional handle for a CUDA graph.", "relation": "single"}, {"head": "cudaGraphAddNode", "head_type": "__host__", "description": "Adds a node to a CUDA graph.", "relation": "calls", "tail": "cudaGraphNodeParams", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a CUDA graph executable.", "relation": "single"}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches a CUDA graph executable.", "relation": "single"}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits for the device to finish all preceding tasks.", "relation": "single"}, {"head": "cudaGraphExecDestroy", "head_type": "__host__", "description": "Destroys a CUDA graph executable.", "relation": "single"}, {"head": "cudaGraphDestroy", "head_type": "__host__", "description": "Destroys a CUDA graph.", "relation": "single"}, {"head": "cudaEventCreate", "head_type": "__host__", "description": "Creates a CUDA event.", "relation": "single"}, {"head": "cudaEventDestroy", "head_type": "__host__", "description": "Destroys a CUDA event.", "relation": "single"}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records a CUDA event.", "relation": "single"}, {"head": "cudaEventSynchronize", "head_type": "__host__", "description": "Waits for a CUDA event to complete.", "relation": "single"}, {"head": "cudaEventElapsedTime", "head_type": "__host__", "description": "Computes the elapsed time between two CUDA events.", "relation": "calls", "tail": "cudaEvent_t", "tail_type": "CUDA_API"}, {"head": "cudaMemcpyAsync", "head_type": "__host__", "description": "Performs an asynchronous memory copy.", "relation": "single"}, {"head": "MyKernel", "head_type": "__global__", "description": "A user-defined kernel.", "relation": "single"}, {"head": "cudaSetDeviceFlags", "head_type": "__host__", "description": "Sets device flags.", "relation": "single"}]}
{"pdf_page": [95, 98], "model_response": [{"head": "cudaGetDeviceCount", "head_type": "__host__", "description": "Gets the number of CUDA-enabled devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Gets the properties of a specified device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaSetDevice", "head_type": "__host__", "description": "Sets the current device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMalloc", "head_type": "__host__", "description": "Allocates device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "MyKernel", "head_type": "__global__", "description": "A placeholder for a user-defined kernel launch.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamCreate", "head_type": "__host__", "description": "Creates a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventElapsedTime", "head_type": "__host__", "description": "Computes the elapsed time between two events.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventSynchronize", "head_type": "__host__", "description": "Waits for an event to complete.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventQuery", "head_type": "__host__", "description": "Queries an event's status.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Makes a stream wait on an event.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceEnablePeerAccess", "head_type": "__host__", "description": "Enables peer-to-peer memory access between devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyPeer", "head_type": "__host__", "description": "Copies memory between two devices.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [98, 101], "model_response": [{"head": "cudaDeviceGetLimit", "head_type": "__host__", "description": "Queries the size of the call stack for a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetLimit", "head_type": "__host__", "description": "Sets the size of the call stack for a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaIpcGetMemHandle", "head_type": "__host__", "description": "Gets an IPC memory handle for a device memory pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaIpcOpenMemHandle", "head_type": "__host__", "description": "Opens an IPC memory handle and retrieves a device pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaPeekAtLastError", "head_type": "__host__", "description": "Returns the last CUDA error without resetting the error variable.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetLastError", "head_type": "__host__", "description": "Returns and resets the last CUDA error.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits for the device to finish all outstanding work.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [101, 105], "model_response": [{"head": "cudaCreateTextureObject", "head_type": "__host__", "description": "Creates a texture object from a resource description and a texture description.", "relation": "calls", "tail": "cudaResourceDesc", "tail_type": "CUDA_CLASS"}, {"head": "transformKernel", "head_type": "__global__", "description": "Applies a simple transformation to a texture.", "relation": "calls", "tail": "tex2D", "tail_type": "CUDA_API"}, {"head": "cudaMallocArray", "head_type": "__host__", "description": "Allocates a CUDA array in device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpy2DToArray", "head_type": "__host__", "description": "Copies data from host memory to device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaCreateChannelDesc", "head_type": "__host__", "description": "Creates a channel description for a CUDA array.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDestroyTextureObject", "head_type": "__host__", "description": "Destroys a texture object.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeArray", "head_type": "__host__", "description": "Frees a CUDA array in device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees device memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [106, 108], "model_response": [{"head": "surf2Dread", "head_type": "__device__", "description": "Reads data from a 2D surface memory region.", "relation": "calls", "tail": "cudaSurfaceObject_t", "tail_type": "CUDA_CLASS"}, {"head": "surf2Dwrite", "head_type": "__device__", "description": "Writes data to a 2D surface memory region.", "relation": "calls", "tail": "cudaSurfaceObject_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaMallocArray", "head_type": "__host__", "description": "Allocates a CUDA array.", "relation": "calls", "tail": "cudaArray_t", "tail_type": "CUDA_API"}, {"head": "cudaCreateChannelDesc", "head_type": "__host__", "description": "Creates a channel description for a CUDA array.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpy2DToArray", "head_type": "__host__", "description": "Copies data from host memory to a CUDA array.", "relation": "calls", "tail": "cudaArray_t", "tail_type": "CUDA_API"}, {"head": "cudaCreateSurfaceObject", "head_type": "__host__", "description": "Creates a surface object from a CUDA array.", "relation": "calls", "tail": "cudaResourceDesc", "tail_type": "CUDA_CLASS"}, {"head": "copyKernel", "head_type": "__global__", "description": "Kernel that copies data from one surface to another.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpy2DFromArray", "head_type": "__host__", "description": "Copies data from a CUDA array to host memory.", "relation": "calls", "tail": "cudaArray_t", "tail_type": "CUDA_API"}, {"head": "cudaDestroySurfaceObject", "head_type": "__host__", "description": "Destroys a surface object.", "relation": "calls", "tail": "cudaSurfaceObject_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaFreeArray", "head_type": "__host__", "description": "Frees a CUDA array.", "relation": "calls", "tail": "cudaArray_t", "tail_type": "CUDA_API"}]}
{"pdf_page": [108, 111], "model_response": [{"head": "cudaGraphicsGLRegisterBuffer", "head_type": "__host__", "description": "Registers an OpenGL buffer object with CUDA.", "relation": "calls", "tail": "cudaGraphicsResource", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphicsMapResources", "head_type": "__host__", "description": "Maps graphics resources for access by CUDA.", "relation": "calls", "tail": "cudaGraphicsResourceGetMappedPointer", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsResourceGetMappedPointer", "head_type": "__host__", "description": "Gets a device pointer through which to access a mapped resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsUnmapResources", "head_type": "__host__", "description": "Unmaps graphics resources.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsUnregisterResource", "head_type": "__host__", "description": "Unregisters a CUDA graphics resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "createVertices", "head_type": "__global__", "description": "A CUDA kernel that writes positions of vertices in a buffer object.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [111, 114], "model_response": [{"head": "cudaGraphicsD3D9RegisterResource", "head_type": "__host__", "description": "Registers a Direct3D 9 resource for use with CUDA.", "relation": "calls", "tail": "cudaGraphicsResourceSetMapFlags", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsResourceSetMapFlags", "head_type": "__host__", "description": "Sets the map flags for a CUDA graphics resource.", "relation": "single"}, {"head": "cudaGraphicsMapResources", "head_type": "__host__", "description": "Maps graphics resources for access by CUDA.", "relation": "calls", "tail": "cudaGraphicsResourceGetMappedPointer", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsResourceGetMappedPointer", "head_type": "__host__", "description": "Gets a pointer through which to access a mapped graphics resource.", "relation": "single"}, {"head": "cudaGraphicsUnmapResources", "head_type": "__host__", "description": "Unmaps graphics resources.", "relation": "single"}, {"head": "cudaGraphicsUnregisterResource", "head_type": "__host__", "description": "Unregisters a CUDA graphics resource.", "relation": "single"}, {"head": "cudaD3D9GetDevice", "head_type": "__host__", "description": "Gets the CUDA device associated with a Direct3D 9 device.", "relation": "single"}, {"head": "cudaD3D10GetDevice", "head_type": "__host__", "description": "Gets the CUDA device associated with a Direct3D 10 device.", "relation": "single"}]}
{"pdf_page": [113, 116], "model_response": [{"head": "cudaSetDevice", "head_type": "__host__", "description": "Sets the device to be used for CUDA API calls", "relation": "calls", "tail": "cudaSuccess", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsD3D10RegisterResource", "head_type": "__host__", "description": "Registers a Direct3D 10 resource for access by CUDA", "relation": "calls", "tail": "cudaGraphicsRegisterFlagsNone", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsResourceSetMapFlags", "head_type": "__host__", "description": "Sets the map flags for a CUDA graphics resource", "relation": "calls", "tail": "cudaGraphicsMapFlagsWriteDiscard", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsMapResources", "head_type": "__host__", "description": "Maps graphics resources for access by CUDA", "relation": "calls", "tail": "cudaGraphicsResourceGetMappedPointer", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsResourceGetMappedPointer", "head_type": "__host__", "description": "Gets a pointer through which to access a mapped graphics resource", "relation": "calls", "tail": "cudaSuccess", "tail_type": "CUDA_API"}, {"head": "createVertices", "head_type": "__global__", "description": "Kernel that calculates and writes vertex positions", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsUnmapResources", "head_type": "__host__", "description": "Unmaps graphics resources", "relation": "calls", "tail": "cudaSuccess", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsUnregisterResource", "head_type": "__host__", "description": "Unregisters a CUDA graphics resource", "relation": "calls", "tail": "", "tail_type": ""}, {"head": "cudaD3D10GetDevice", "head_type": "__host__", "description": "Gets a CUDA-enabled adapter for Direct3D 10", "relation": "calls", "tail": "cudaSuccess", "tail_type": "CUDA_API"}, {"head": "cudaD3D11GetDevice", "head_type": "__host__", "description": "Gets a CUDA-enabled adapter for Direct3D 11", "relation": "calls", "tail": "cudaSuccess", "tail_type": "CUDA_API"}]}
{"pdf_page": [115, 118], "model_response": [{"head": "cudaD3D11GetDevice", "head_type": "__host__", "description": "Gets a CUDA-enabled adapter", "relation": "calls", "tail": "cudaSuccess", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsD3D11RegisterResource", "head_type": "__host__", "description": "Registers a Direct3D 11 resource for access by CUDA", "relation": "calls", "tail": "cudaGraphicsRegisterFlagsNone", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsResourceSetMapFlags", "head_type": "__host__", "description": "Sets the map flags for a CUDA graphics resource", "relation": "calls", "tail": "cudaGraphicsMapFlagsWriteDiscard", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsMapResources", "head_type": "__host__", "description": "Maps graphics resources for access by CUDA", "relation": "calls", "tail": "cudaGraphicsMapFlagsNone", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsResourceGetMappedPointer", "head_type": "__host__", "description": "Gets a pointer through which to access a mapped CUDA graphics resource", "relation": "calls", "tail": "cudaGraphicsMapFlagsNone", "tail_type": "CUDA_API"}, {"head": "createVertices", "head_type": "__global__", "description": "A kernel that writes vertex positions", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsUnmapResources", "head_type": "__host__", "description": "Unmaps graphics resources after access by CUDA", "relation": "calls", "tail": "cudaGraphicsMapFlagsNone", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsUnregisterResource", "head_type": "__host__", "description": "Unregisters a CUDA graphics resource", "relation": "calls", "tail": "CUDA_API", "tail_type": "CUDA_API"}]}
{"pdf_page": [118, 122], "model_response": [{"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports an external memory object into CUDA.", "relation": "calls", "tail": "cudaExternalMemoryHandleDesc", "tail_type": "CUDA_STRUCT"}, {"head": "cudaDestroyExternalMemory", "head_type": "__host__", "description": "Frees an imported external memory object.", "relation": "single"}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "__host__", "description": "Maps a device pointer onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryBufferDesc", "tail_type": "CUDA_STRUCT"}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees a device pointer mapped onto an imported memory object.", "relation": "single"}, {"head": "cudaImportExternalSemaphore", "head_type": "__host__", "description": "Imports an external synchronization object into CUDA.", "relation": "single"}, {"head": "cudaSignalExternalSemaphoresAsync", "head_type": "__host__", "description": "Signals an imported synchronization object.", "relation": "single"}, {"head": "cudaWaitExternalSemaphoresAsync", "head_type": "__host__", "description": "Waits on an imported synchronization object.", "relation": "single"}, {"head": "cudaDestroyExternalSemaphore", "head_type": "__host__", "description": "Frees an imported synchronization object.", "relation": "single"}, {"head": "cudaGetDeviceCount", "head_type": "__host__", "description": "Gets the number of CUDA devices.", "relation": "single"}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Gets the properties of a CUDA device.", "relation": "single"}, {"head": "importVulkanMemoryObjectFromFileDescriptor", "head_type": "__host__", "description": "Imports a Vulkan memory object from a file descriptor.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "importVulkanMemoryObjectFromNTHandle", "head_type": "__host__", "description": "Imports a Vulkan memory object from an NT handle.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "importVulkanMemoryObjectFromNamedNTHandle", "head_type": "__host__", "description": "Imports a Vulkan memory object from a named NT handle.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "importVulkanMemoryObjectFromKMTHandle", "head_type": "__host__", "description": "Imports a Vulkan memory object from a KMT handle.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "mapBufferOntoExternalMemory", "head_type": "__host__", "description": "Maps a buffer onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedBuffer", "tail_type": "CUDA_API"}]}
{"pdf_page": [122, 125], "model_response": [{"head": "cudaExternalMemoryGetMappedMipmappedArray", "head_type": "__host__", "description": "Maps a mipmapped array onto an imported memory object.", "relation": "calls", "tail": "cudaMipmappedArray_t", "tail_type": "CUDA_API"}, {"head": "cudaImportExternalSemaphore", "head_type": "__host__", "description": "Imports a Vulkan semaphore object into CUDA.", "relation": "calls", "tail": "cudaExternalSemaphore_t", "tail_type": "CUDA_API"}, {"head": "mapMipmappedArrayOntoExternalMemory", "head_type": "__host__", "description": "Maps a CUDA mipmapped array onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedMipmappedArray", "tail_type": "CUDA_API"}, {"head": "importVulkanSemaphoreObjectFromFileDescriptor", "head_type": "__host__", "description": "Imports a Vulkan semaphore object from a file descriptor.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}]}
{"pdf_page": [125, 127], "model_response": [{"head": "importVulkanSemaphoreObjectFromNTHandle", "head_type": "__host__", "description": "Imports a Vulkan semaphore object from a NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importVulkanSemaphoreObjectFromNamedNTHandle", "head_type": "__host__", "description": "Imports a Vulkan semaphore object from a named NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importVulkanSemaphoreObjectFromKMTHandle", "head_type": "__host__", "description": "Imports a Vulkan semaphore object from a KMT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "signalExternalSemaphore", "head_type": "__host__", "description": "Signals an imported Vulkan semaphore object.", "relation": "calls", "tail": "cudaSignalExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "waitExternalSemaphore", "head_type": "__host__", "description": "Waits on an imported Vulkan semaphore object.", "relation": "calls", "tail": "cudaWaitExternalSemaphoresAsync", "tail_type": "CUDA_API"}]}
{"pdf_page": [127, 130], "model_response": [{"head": "cudaGetDeviceCount", "head_type": "__host__", "description": "Gets the number of installed CUDA capable devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Gets the properties of the specified CUDA device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports an external memory object into the CUDA address space.", "relation": "calls", "tail": "cudaExternalMemoryHandleDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "__host__", "description": "Gets a device pointer through which the memory object can be accessed.", "relation": "calls", "tail": "cudaExternalMemoryBufferDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeMipmappedArray", "head_type": "__host__", "description": "Frees a mipmapped array.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaExternalMemoryGetMappedMipmappedArray", "head_type": "__host__", "description": "Maps a mipmapped array onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryMipmappedArrayDesc", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [129, 133], "model_response": [{"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports an external memory object into the CUDA address space.", "relation": "calls", "tail": "memset", "tail_type": "CUDA_API"}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "__host__", "description": "Gets a device pointer through which the memory object can be accessed.", "relation": "calls", "tail": "memset", "tail_type": "CUDA_API"}, {"head": "cudaExternalMemoryGetMappedMipmappedArray", "head_type": "__host__", "description": "Gets a mipmapped array through which the memory object can be accessed.", "relation": "calls", "tail": "memset", "tail_type": "CUDA_API"}, {"head": "cudaImportExternalSemaphore", "head_type": "__host__", "description": "Imports an external semaphore into the CUDA address space.", "relation": "calls", "tail": "memset", "tail_type": "CUDA_API"}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeMipmappedArray", "head_type": "__host__", "description": "Frees a mipmapped array.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "CloseHandle", "head_type": "__host__", "description": "Closes an open object handle.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [133, 136], "model_response": [{"head": "cudaSignalExternalSemaphoresAsync", "head_type": "__host__", "description": "Signals an external semaphore asynchronously.", "relation": "calls", "tail": "cudaExternalSemaphoreSignalParams", "tail_type": "CUDA_CLASS"}, {"head": "cudaWaitExternalSemaphoresAsync", "head_type": "__host__", "description": "Waits on external semaphores asynchronously.", "relation": "calls", "tail": "cudaExternalSemaphoreWaitParams", "tail_type": "CUDA_CLASS"}, {"head": "cudaGetDeviceCount", "head_type": "__host__", "description": "Gets the number of devices available.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Gets the properties of the specified device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports an external memory object into CUDA.", "relation": "calls", "tail": "cudaExternalMemoryHandleDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "mapBufferOntoExternalMemory", "head_type": "__host__", "description": "Maps a buffer onto an imported external memory object.", "relation": "calls", "tail": "cudaExternalMemoryBufferDesc", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [135, 139], "model_response": [{"head": "importD3D11ResourceFromNamedNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 resource from a named NT handle.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "importD3D11ResourceFromKMTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 resource from a KMT handle.", "relation": "calls", "tail": "cudaImportExternalMemory", "tail_type": "CUDA_API"}, {"head": "mapBufferOntoExternalMemory", "head_type": "__host__", "description": "Maps a buffer onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedBuffer", "tail_type": "CUDA_API"}, {"head": "mapMipmappedArrayOntoExternalMemory", "head_type": "__host__", "description": "Maps a mipmapped array onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedMipmappedArray", "tail_type": "CUDA_API"}, {"head": "importD3D11FenceFromNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 fence from an NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}]}
{"pdf_page": [138, 142], "model_response": [{"head": "cudaImportExternalSemaphore", "head_type": "__host__", "description": "Imports an external semaphore into the CUDA runtime.", "relation": "calls", "tail": "memset", "tail_type": "CUDA_API"}, {"head": "cudaSignalExternalSemaphoresAsync", "head_type": "__host__", "description": "Signals an external semaphore or a batch of external semaphores asynchronously.", "relation": "calls", "tail": "memset", "tail_type": "CUDA_API"}, {"head": "cudaWaitExternalSemaphoresAsync", "head_type": "__host__", "description": "Waits on an external semaphore or a batch of external semaphores asynchronously.", "relation": "calls", "tail": "memset", "tail_type": "CUDA_API"}, {"head": "importD3D11FenceFromNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 fence object from an NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11KeyedMutexFromNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 keyed mutex object from an NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11FenceFromNamedNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 fence object from a named NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11KeyedMutexFromNamedNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 keyed mutex object from a named NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11FenceFromKMTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 fence object from a globally shared D3DKMT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "signalExternalSemaphore", "head_type": "__host__", "description": "Signals an external semaphore with a specified value.", "relation": "calls", "tail": "cudaSignalExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "waitExternalSemaphore", "head_type": "__host__", "description": "Waits on an external semaphore with a specified value or a keyed mutex with a specified key.", "relation": "calls", "tail": "cudaWaitExternalSemaphoresAsync", "tail_type": "CUDA_API"}]}
{"pdf_page": [142, 145], "model_response": [{"head": "cuDeviceGetUuid", "head_type": "__host__", "description": "Retrieves the UUID of the CUDA device.", "relation": "calls", "tail": "CUuuid", "tail_type": "CUDA_API"}, {"head": "NvSciBufObjAlloc", "head_type": "__host__", "description": "Allocates an NvSciBuf object based on the provided attribute list.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "NvSciBufObjDupWithReducePerm", "head_type": "__host__", "description": "Duplicates an NvSciBuf object with reduced permissions.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports an NvSciBuf object into CUDA as an external memory object.", "relation": "calls", "tail": "cudaExternalMemoryHandleDesc", "tail_type": "CUDA_API"}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "__host__", "description": "Maps a device pointer onto an imported external memory object.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [145, 148], "model_response": [{"head": "cudaMipmappedArray_t mapMipmappedArrayOntoExternalMemory", "head_type": "__host__", "description": "Maps a CUDA mipmapped array onto an imported memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedMipmappedArray", "tail_type": "CUDA_API"}, {"head": "NvSciSyncObj createNvSciSyncObject", "head_type": "__host__", "description": "Creates a NvSciSyncObj that is compatible with a given CUDA device.", "relation": "calls", "tail": "cudaDeviceGetNvSciSyncAttributes", "tail_type": "CUDA_API"}, {"head": "cudaExternalSemaphore_t importNvSciSyncObject", "head_type": "__host__", "description": "Imports an NvSciSync object into CUDA.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "void signalExternalSemaphore", "head_type": "__host__", "description": "Signals an imported NvSciSyncObj object.", "relation": "calls", "tail": "cudaSignalExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "void waitExternalSemaphore", "head_type": "__host__", "description": "Waits on an imported NvSciSyncObj object.", "relation": "calls", "tail": "cudaWaitExternalSemaphoresAsync", "tail_type": "CUDA_API"}]}
{"pdf_page": [157, 160], "model_response": [{"head": "cudaOccupancyMaxActiveBlocksPerMultiprocessor", "head_type": "__host__", "description": "Predicts the occupancy in terms of the number of concurrent thread blocks per multiprocessor.", "relation": "calls", "tail": "cudaGetDeviceProperties", "tail_type": "CUDA_API"}, {"head": "cudaOccupancyMaxPotentialBlockSize", "head_type": "__host__", "description": "Heuristically calculates an execution configuration that achieves the maximum multiprocessor-level occupancy.", "relation": "calls", "tail": "MyKernel", "tail_type": "CUDA_CLASS"}, {"head": "cudaOccupancyMaxActiveClusters", "head_type": "__host__", "description": "Predicts the occupancy in terms of the number of max active clusters of a given size on the GPU.", "relation": "calls", "tail": "cudaOccupancyMaxPotentialClusterSize", "tail_type": "CUDA_API"}, {"head": "MyKernel", "head_type": "__global__", "description": "A sample kernel that performs a simple multiplication operation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchConfig_t", "head_type": "__host__", "description": "Structure used to specify the launch configuration for a cluster kernel.", "relation": "calls", "tail": "cudaOccupancyMaxActiveClusters", "tail_type": "CUDA_API"}, {"head": "cudaOccupancyMaxPotentialClusterSize", "head_type": "__host__", "description": "Queries the maximum cluster size for a given kernel and launch configuration.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [185, 188], "model_response": [{"head": "__threadfence_block", "head_type": "__device__", "description": "Enforces a sequentially-consistent ordering on memory accesses within a thread block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__threadfence", "head_type": "__device__", "description": "Enforces a sequentially-consistent ordering on memory accesses within the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__threadfence_system", "head_type": "__device__", "description": "Enforces a sequentially-consistent ordering on memory accesses observable by the host and peer devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "atomicInc", "head_type": "__device__", "description": "Atomically increments a variable.", "relation": "calls", "tail": "&count", "tail_type": "CUDA_API"}, {"head": "__syncthreads", "head_type": "__device__", "description": "Synchronizes all threads in the block.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [188, 189], "model_response": [{"head": "__syncthreads", "head_type": "__device__", "description": "__syncthreads() waits until all threads in the thread block have reached this point and all global and shared memory accesses are visible to all threads in the block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads_count", "head_type": "__device__", "description": "__syncthreads_count(int predicate) is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads and returns the count.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads_and", "head_type": "__device__", "description": "__syncthreads_and(int predicate) is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads and returns non-zero if all threads satisfy the predicate.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads_or", "head_type": "__device__", "description": "__syncthreads_or(int predicate) is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads and returns non-zero if any thread satisfies the predicate.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncwarp", "head_type": "__device__", "description": "__syncwarp(unsigned mask) causes the executing thread to wait until all warp lanes have executed a __syncwarp() before resuming execution, ensuring memory ordering among threads.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [207, 210], "model_response": [{"head": "__isShared", "head_type": "__device__", "description": "Returns 1 if the pointer contains the generic address of an object in shared memory space, otherwise returns 0.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__isConstant", "head_type": "__device__", "description": "Returns 1 if the pointer contains the generic address of an object in constant memory space, otherwise returns 0.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__isGridConstant", "head_type": "__device__", "description": "Returns 1 if the pointer contains the generic address of a kernel parameter annotated with __grid_constant__, otherwise returns 0.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__isLocal", "head_type": "__device__", "description": "Returns 1 if the pointer contains the generic address of an object in local memory space, otherwise returns 0.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_global", "head_type": "__device__", "description": "Returns the result of executing the PTX cvta.to.global instruction on the generic address denoted by the pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_shared", "head_type": "__device__", "description": "Returns the result of executing the PTX cvta.to.shared instruction on the generic address denoted by the pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_constant", "head_type": "__device__", "description": "Returns the result of executing the PTX cvta.to.const instruction on the generic address denoted by the pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_local", "head_type": "__device__", "description": "Returns the result of executing the PTX cvta.to.local instruction on the generic address denoted by the pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_global_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTX cvta.global instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_shared_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTX cvta.shared instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_constant_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTX cvta.const instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_local_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTX cvta.local instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "alloca", "head_type": "__host__ __device__", "description": "Allocates size bytes of memory in the stack frame of the caller. The returned value is a pointer to the allocated memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "foo", "head_type": "__device__", "description": "An example function that uses alloca to allocate memory on the stack.", "relation": "calls", "tail": "alloca", "tail_type": "CUDA_API"}]}
{"pdf_page": [209, 213], "model_response": [{"head": "__cvta_constant_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTXcvta.const instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_local_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTXcvta.local instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "alloca", "head_type": "__host__ __device__", "description": "Allocates size bytes of memory in the stack frame of the caller. The returned value is a pointer to allocated memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_assume_aligned", "head_type": "__host__ __device__", "description": "Allows the compiler to assume that the argument pointer is aligned to at least align bytes, and returns the argument pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_assume", "head_type": "__host__ __device__", "description": "Allows the compiler to assume that the Boolean argument is true.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__assume", "head_type": "__host__", "description": "Allows the compiler to assume that the Boolean argument is true, and is only supported when using cl.exe host compiler.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_expect", "head_type": "__host__ __device__", "description": "Indicates to the compiler that it is expected that exp == c, and returns the value of exp.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_unreachable", "head_type": "__host__ __device__", "description": "Indicates to the compiler that control flow never reaches the point where this function is called from.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__all_sync", "head_type": "__device__", "description": "Evaluate predicate for all non-exited threads in mask and return non-zero if and only if predicate evaluates to non-zero for all of them.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__any_sync", "head_type": "__device__", "description": "Evaluate predicate for all non-exited threads in mask and return non-zero if and only if predicate evaluates to non-zero for any of them.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__ballot_sync", "head_type": "__device__", "description": "Evaluate predicate for all non-exited threads in mask and return an integer whose Nth bit is set if and only if predicate evaluates to non-zero for the Nth thread of the warp.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__activemask", "head_type": "__device__", "description": "Returns a 32-bit integer mask of all currently active threads in the calling warp.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [212, 215], "model_response": [{"head": "__all_sync", "head_type": "__device__", "description": "__all_sync evaluates a predicate for all non-exited threads in a mask and returns non-zero if the predicate is non-zero for all of them.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__any_sync", "head_type": "__device__", "description": "__any_sync evaluates a predicate for all non-exited threads in a mask and returns non-zero if the predicate is non-zero for any of them.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__ballot_sync", "head_type": "__device__", "description": "__ballot_sync evaluates a predicate for all non-exited threads in a mask and returns an integer with bits set based on the predicate evaluations.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__match_any_sync", "head_type": "__device__", "description": "__match_any_sync performs a broadcast-and-compare operation across threads in a warp and returns a mask of threads with the same value.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__match_all_sync", "head_type": "__device__", "description": "__match_all_sync performs a broadcast-and-compare operation across threads in a warp and returns a mask if all threads have the same value.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__reduce_add_sync", "head_type": "__device__", "description": "__reduce_add_sync performs an arithmetic add reduction operation on values provided by each thread in a mask.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__reduce_min_sync", "head_type": "__device__", "description": "__reduce_min_sync performs an arithmetic min reduction operation on values provided by each thread in a mask.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__reduce_max_sync", "head_type": "__device__", "description": "__reduce_max_sync performs an arithmetic max reduction operation on values provided by each thread in a mask.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__reduce_and_sync", "head_type": "__device__", "description": "__reduce_and_sync performs a logical AND reduction operation on values provided by each thread in a mask.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__reduce_or_sync", "head_type": "__device__", "description": "__reduce_or_sync performs a logical OR reduction operation on values provided by each thread in a mask.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}, {"head": "__reduce_xor_sync", "head_type": "__device__", "description": "__reduce_xor_sync performs a logical XOR reduction operation on values provided by each thread in a mask.", "relation": "calls", "tail": "__activemask", "tail_type": "CUDA_API"}]}
{"pdf_page": [216, 218], "model_response": [{"head": "__shfl_sync", "head_type": "__device__", "description": "__shfl_sync is used for warp shuffle operations, allowing threads to exchange data with other threads in the same warp.", "relation": "calls", "tail": "__shfl_up_sync", "tail_type": "CUDA_API"}, {"head": "__shfl_up_sync", "head_type": "__device__", "description": "__shfl_up_sync shifts a value up the warp by a specified number of lanes.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__shfl_down_sync", "head_type": "__device__", "description": "__shfl_down_sync shifts a value down the warp by a specified number of lanes.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__shfl_xor_sync", "head_type": "__device__", "description": "__shfl_xor_sync performs a bitwise XOR on the caller\u2019s lane ID to determine the source lane ID for a warp shuffle operation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "bcast", "head_type": "__global__", "description": "A kernel function that demonstrates broadcasting a value across a warp.", "relation": "calls", "tail": "__shfl_sync", "tail_type": "CUDA_API"}, {"head": "scan4", "head_type": "__global__", "description": "A kernel function that performs an inclusive scan across sub-partitions of 8 threads.", "relation": "calls", "tail": "__shfl_up_sync", "tail_type": "CUDA_API"}, {"head": "warpReduce", "head_type": "__global__", "description": "A kernel function that demonstrates reduction across a warp using __shfl_xor_sync.", "relation": "calls", "tail": "__shfl_xor_sync", "tail_type": "CUDA_API"}]}
{"pdf_page": [227, 230], "model_response": [{"head": "__syncthreads", "head_type": "__device__", "description": "Synchronizes all threads in the block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cooperative_groups::this_thread_block", "head_type": "__device__", "description": "Obtains the cooperative group of the current thread block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "block.sync", "head_type": "__device__", "description": "Waits for all threads in the cooperative group to arrive at this point.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::barrier", "head_type": "__device__", "description": "CUDA barrier for thread synchronization.", "relation": "instantiated", "tail": "<cuda::thread_scope_block>", "tail_type": "CUDA_CLASS"}, {"head": "bar.arrive", "head_type": "__device__", "description": "Marks the arrival of a thread at the barrier, without blocking.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_API"}, {"head": "bar.wait", "head_type": "__device__", "description": "Waits for all participating threads to arrive at the barrier.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_API"}, {"head": "init", "head_type": "__device__", "description": "Initializes the barrier with the expected arrival count.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_API"}, {"head": "cuda::barrier::arrival_token", "head_type": "__device__", "description": "Token representing a thread's arrival at the barrier.", "relation": "instantiated", "tail": "bar.arrive", "tail_type": "CUDA_API"}]}
{"pdf_page": [230, 232], "model_response": [{"head": "bar.arrive_and_wait", "head_type": "__device__", "description": "Waits for the barrier to be ready and then arrives at it.", "relation": "calls", "tail": "bar.arrive", "tail_type": "CUDA_API"}, {"head": "bar.arrive_and_wait", "head_type": "__device__", "description": "Waits for the barrier to be ready and then arrives at it.", "relation": "calls", "tail": "bar.wait", "tail_type": "CUDA_API"}, {"head": "bar.arrive", "head_type": "__device__", "description": "Arrives at the barrier and obtain a token.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "bar.wait", "head_type": "__device__", "description": "Waits at the barrier using a token.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "producer", "head_type": "__device__", "description": "Device function for the producer part of the producer-consumer pattern.", "relation": "calls", "tail": "bar.arrive_and_wait", "tail_type": "CUDA_API"}, {"head": "producer", "head_type": "__device__", "description": "Device function for the producer part of the producer-consumer pattern.", "relation": "calls", "tail": "bar.arrive", "tail_type": "CUDA_API"}, {"head": "consumer", "head_type": "__device__", "description": "Device function for the consumer part of the producer-consumer pattern.", "relation": "calls", "tail": "bar.arrive_and_wait", "tail_type": "CUDA_API"}, {"head": "consumer", "head_type": "__device__", "description": "Device function for the consumer part of the producer-consumer pattern.", "relation": "calls", "tail": "bar.arrive", "tail_type": "CUDA_API"}, {"head": "producer_consumer_pattern", "head_type": "__global__", "description": "Global kernel for the producer-consumer pattern.", "relation": "calls", "tail": "producer", "tail_type": "CUDA_API"}, {"head": "producer_consumer_pattern", "head_type": "__global__", "description": "Global kernel for the producer-consumer pattern.", "relation": "calls", "tail": "consumer", "tail_type": "CUDA_API"}]}
{"pdf_page": [233, 235], "model_response": [{"head": "psum", "head_type": "__global__", "description": "A kernel function that performs parallel prefix sum using a cooperative group barrier.", "relation": "calls", "tail": "this_thread_block", "tail_type": "CUDA_CLASS"}, {"head": "psum", "head_type": "__global__", "description": "A kernel function that performs parallel prefix sum.", "relation": "calls", "tail": "sync", "tail_type": "CUDA_API"}, {"head": "psum", "head_type": "__global__", "description": "A kernel function that initializes a cooperative group barrier.", "relation": "calls", "tail": "barrier_t", "tail_type": "CUDA_CLASS"}, {"head": "psum", "head_type": "__global__", "description": "A kernel function that uses the arrive and wait methods of the barrier_t class.", "relation": "calls", "tail": "arrive", "tail_type": "CUDA_API"}, {"head": "__mbarrier_arrive", "head_type": "__device__", "description": "A device function that decrements the pending count for the current phase of the barrier.", "relation": "calls", "tail": "__mbarrier_t", "tail_type": "CUDA_CLASS"}, {"head": "__mbarrier_arrive_and_drop", "head_type": "__device__", "description": "A device function that decrements the pending count for the current and next phase of the barrier.", "relation": "calls", "tail": "__mbarrier_t", "tail_type": "CUDA_CLASS"}, {"head": "__mbarrier_test_wait", "head_type": "__device__", "description": "A device function that tests if the barrier can be waited on using a token.", "relation": "calls", "tail": "__mbarrier_t", "tail_type": "CUDA_CLASS"}, {"head": "__mbarrier_pending_count", "head_type": "__device__", "description": "A device function that gets the pending count from a token.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [236, 239], "model_response": [{"head": "without_memcpy_async", "head_type": "__global__", "description": "Performs data copy and compute without using memcpy_async.", "relation": "calls", "tail": "compute", "tail_type": "__device__"}, {"head": "cooperative_groups::memcpy_async", "head_type": "__device__", "description": "Asynchronously copies data from global memory to shared memory.", "relation": "calls", "tail": "cooperative_groups::wait", "tail_type": "__device__"}, {"head": "with_memcpy_async", "head_type": "__global__", "description": "Performs data copy and compute using memcpy_async for asynchronous data transfer.", "relation": "calls", "tail": "cooperative_groups::memcpy_async", "tail_type": "__device__"}, {"head": "cuda::barrier", "head_type": "__device__", "description": "Synchronization object for cooperative groups to wait for asynchronous operations.", "relation": "used_by", "tail": "cuda::memcpy_async", "tail_type": "__device__"}, {"head": "with_barrier", "head_type": "__global__", "description": "Performs data copy and compute using cuda::barrier for synchronization.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "__device__"}, {"head": "barrier.arrive_and_wait", "head_type": "__device__", "description": "Waits for all asynchronous operations to complete using the barrier.", "relation": "calls", "tail": "", "tail_type": ""}]}
{"pdf_page": [241, 243], "model_response": [{"head": "__syncwarp", "head_type": "__device__", "description": "Re-converges the warp before invoking commit or arrive-on operations.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::pipeline::producer_acquire", "head_type": "__device__", "description": "Acquires an available stage in the pipeline internal queue.", "relation": "calls", "tail": "cuda::pipeline_shared_state", "tail_type": "CUDA_CLASS"}, {"head": "cuda::pipeline::producer_commit", "head_type": "__device__", "description": "Commits asynchronous operations issued after producer_acquire call.", "relation": "calls", "tail": "cuda::pipeline_shared_state", "tail_type": "CUDA_CLASS"}, {"head": "cuda::pipeline::consumer_wait", "head_type": "__device__", "description": "Waits for completion of all asynchronous operations on the oldest stage.", "relation": "calls", "tail": "cuda::pipeline_shared_state", "tail_type": "CUDA_CLASS"}, {"head": "cuda::pipeline::consumer_release", "head_type": "__device__", "description": "Releases the oldest stage of the pipeline for reuse.", "relation": "calls", "tail": "cuda::pipeline_shared_state", "tail_type": "CUDA_CLASS"}, {"head": "cuda::memcpy_async", "head_type": "__device__", "description": "Submits asynchronous memory copy to the pipeline's head stage.", "relation": "calls", "tail": "cuda::pipeline", "tail_type": "CUDA_CLASS"}, {"head": "compute", "head_type": "__device__", "description": "Performs computation on the data copied asynchronously.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [243, 246], "model_response": [{"head": "with_staging", "head_type": "__global__", "description": "A kernel function that demonstrates the use of cuda::pipeline to overlap memory transfers with computation.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "with_staging", "head_type": "__global__", "description": "A kernel function that demonstrates the use of cuda::pipeline to overlap memory transfers with computation.", "relation": "calls", "tail": "pipeline.producer_acquire", "tail_type": "CUDA_API"}, {"head": "with_staging", "head_type": "__global__", "description": "A kernel function that demonstrates the use of cuda::pipeline to overlap memory transfers with computation.", "relation": "calls", "tail": "pipeline.producer_commit", "tail_type": "CUDA_API"}, {"head": "with_staging", "head_type": "__global__", "description": "A kernel function that demonstrates the use of cuda::pipeline to overlap memory transfers with computation.", "relation": "calls", "tail": "pipeline.consumer_wait", "tail_type": "CUDA_API"}, {"head": "with_staging", "head_type": "__global__", "description": "A kernel function that demonstrates the use of cuda::pipeline to overlap memory transfers with computation.", "relation": "calls", "tail": "pipeline.consumer_release", "tail_type": "CUDA_API"}, {"head": "with_staging_unified", "head_type": "__global__", "description": "A more concise version of the with_staging kernel function that overlaps memory transfers with computation using cuda::pipeline.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "with_staging_unified", "head_type": "__global__", "description": "A more concise version of the with_staging kernel function that overlaps memory transfers with computation using cuda::pipeline.", "relation": "calls", "tail": "pipeline.producer_acquire", "tail_type": "CUDA_API"}, {"head": "with_staging_unified", "head_type": "__global__", "description": "A more concise version of the with_staging kernel function that overlaps memory transfers with computation using cuda::pipeline.", "relation": "calls", "tail": "pipeline.producer_commit", "tail_type": "CUDA_API"}, {"head": "with_staging_unified", "head_type": "__global__", "description": "A more concise version of the with_staging kernel function that overlaps memory transfers with computation using cuda::pipeline.", "relation": "calls", "tail": "pipeline.consumer_wait", "tail_type": "CUDA_API"}, {"head": "with_staging_unified", "head_type": "__global__", "description": "A more concise version of the with_staging kernel function that overlaps memory transfers with computation using cuda::pipeline.", "relation": "calls", "tail": "pipeline.consumer_release", "tail_type": "CUDA_API"}]}
{"pdf_page": [246, 249], "model_response": [{"head": "cuda::pipeline", "head_type": "__device__", "description": "Represents a CUDA pipeline for asynchronous data copies", "relation": "calls", "tail": "cuda::make_pipeline", "tail_type": "CUDA_API"}, {"head": "cuda::pipeline::producer_acquire", "head_type": "__device__", "description": "Acquires the producer role in the CUDA pipeline", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::pipeline::producer_commit", "head_type": "__device__", "description": "Commits the producer's data transfer in the CUDA pipeline", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::memcpy_async", "head_type": "__device__", "description": "Performs an asynchronous memory copy", "relation": "calls", "tail": "cuda::pipeline", "tail_type": "CUDA_API"}, {"head": "cuda::pipeline::consumer_wait", "head_type": "__device__", "description": "Waits for the consumer role in the CUDA pipeline", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::pipeline::consumer_release", "head_type": "__device__", "description": "Releases the consumer role in the CUDA pipeline", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads", "head_type": "__device__", "description": "Synchronizes all threads in the block", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncwarp", "head_type": "__device__", "description": "Synchronizes all threads in the warp", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [254, 259], "model_response": [{"head": "cuda::device::experimental::cp_async_bulk_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk-asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier<cuda::thread_scope_block>& bar", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::fence_proxy_async_shared_cta", "head_type": "__device__", "description": "Waits for a thread\u2019s shared memory writes to become visible to the async proxy.", "relation": "calls", "tail": "cuda::barrier<cuda::thread_scope_block>& bar", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_shared_to_global", "head_type": "__device__", "description": "Initiates a bulk-asynchronous copy from shared to global memory.", "relation": "calls", "tail": "cuda::barrier<cuda::thread_scope_block>& bar", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_commit_group", "head_type": "__device__", "description": "Combines all preceding bulk-asynchronous operations into a bulk async-group.", "relation": "calls", "tail": "cuda::barrier<cuda::thread_scope_block>& bar", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_wait_group_read", "head_type": "__device__", "description": "Waits for operations in a bulk async-group to have completed.", "relation": "calls", "tail": "cuda::barrier<cuda::thread_scope_block>& bar", "tail_type": "CUDA_CLASS"}, {"head": "kernel", "head_type": "__global__", "description": "Loads a 2D tile from a larger 2D array, modifies it, and writes it back.", "relation": "calls", "tail": "cuda::device::experimental::cp_async_bulk_tensor_2d_global_to_shared", "tail_type": "CUDA_API"}, {"head": "kernel", "head_type": "__global__", "description": "Loads a 2D tile from a larger 2D array, modifies it, and writes it back.", "relation": "calls", "tail": "cuda::device::experimental::cp_async_bulk_tensor_2d_shared_to_global", "tail_type": "CUDA_API"}]}
{"pdf_page": [259, 261], "model_response": [{"head": "cuda::device::experimental::cp_async_bulk_tensor_1d_global_to_shared", "head_type": "__device__", "description": "Initiates a 1D bulk tensor asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_2d_global_to_shared", "head_type": "__device__", "description": "Initiates a 2D bulk tensor asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_3d_global_to_shared", "head_type": "__device__", "description": "Initiates a 3D bulk tensor asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_4d_global_to_shared", "head_type": "__device__", "description": "Initiates a 4D bulk tensor asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_5d_global_to_shared", "head_type": "__device__", "description": "Initiates a 5D bulk tensor asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_1d_shared_to_global", "head_type": "__device__", "description": "Initiates a 1D bulk tensor asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_2d_shared_to_global", "head_type": "__device__", "description": "Initiates a 2D bulk tensor asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_3d_shared_to_global", "head_type": "__device__", "description": "Initiates a 3D bulk tensor asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_4d_shared_to_global", "head_type": "__device__", "description": "Initiates a 4D bulk tensor asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_5d_shared_to_global", "head_type": "__device__", "description": "Initiates a 5D bulk tensor asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [264, 267], "model_response": [{"head": "printf", "head_type": "__device__", "description": "Outputs formatted data from a kernel to a host-side output buffer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "helloCUDA", "head_type": "__global__", "description": "A CUDA kernel that prints a message from each thread.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits for all device tasks to complete.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "malloc", "head_type": "__host__ __device__", "description": "Allocates memory dynamically from a fixed-size heap in global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__nv_aligned_device_malloc", "head_type": "__device__", "description": "Allocates aligned memory dynamically from a fixed-size heap in global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "free", "head_type": "__host__ __device__", "description": "Deallocates memory that was allocated using malloc or __nv_aligned_device_malloc.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "memcpy", "head_type": "__host__ __device__", "description": "Copies memory from source to destination.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "memset", "head_type": "__host__ __device__", "description": "Sets a block of memory with a specified value.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [267, 270], "model_response": [{"head": "mallocTest", "head_type": "__global__", "description": "A kernel that demonstrates per-thread dynamic memory allocation and deallocation on the device.", "relation": "calls", "tail": "malloc", "tail_type": "CUDA_API"}, {"head": "mallocTest", "head_type": "__global__", "description": "A kernel that demonstrates per-thread dynamic memory allocation and deallocation on the device.", "relation": "calls", "tail": "memset", "tail_type": "CUDA_API"}, {"head": "mallocTest", "head_type": "__global__", "description": "A kernel that demonstrates per-thread dynamic memory allocation and deallocation on the device.", "relation": "calls", "tail": "free", "tail_type": "CUDA_API"}, {"head": "allocmem", "head_type": "__global__", "description": "A kernel that demonstrates dynamic memory allocation that persists between kernel launches, per block.", "relation": "calls", "tail": "malloc", "tail_type": "CUDA_API"}, {"head": "freemem", "head_type": "__global__", "description": "A kernel that prints the content of the buffer and then frees the memory allocated per block.", "relation": "calls", "tail": "free", "tail_type": "CUDA_API"}]}
{"pdf_page": [270, 274], "model_response": [{"head": "__global__ void Func(float *parameter)", "head_type": "__global__", "description": "A __global__ function that must be called with an execution configuration.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchConfig_t config", "head_type": "__host__", "description": "A configuration structure used for launching a kernel with cudaLaunchKernelEx.", "relation": "calls", "tail": "cudaLaunchKernelEx", "tail_type": "CUDA_API"}, {"head": "__launch_bounds__", "head_type": "__host__", "description": "A qualifier to specify launch bounds for a __global__ function.", "relation": "calls", "tail": "__global__ void MyKernel(...)", "tail_type": "__global__"}, {"head": "__maxnreg__", "head_type": "__host__", "description": "A qualifier to specify the maximum number of registers per thread for a __global__ function.", "relation": "calls", "tail": "__global__ void MyKernel(...)", "tail_type": "__global__"}]}
{"pdf_page": [281, 284], "model_response": [{"head": "this_thread_block", "head_type": "__device__", "description": "Constructs a thread_block object representing the current thread block.", "relation": "calls", "tail": "thread_block", "tail_type": "CUDA_CLASS"}, {"head": "thread_block::sync", "head_type": "__device__", "description": "Synchronizes the threads within the thread block.", "relation": "calls", "tail": "barrier_wait", "tail_type": "CUDA_API"}, {"head": "thread_block::barrier_arrive", "head_type": "__device__", "description": "Arrives on the thread block barrier and returns an arrival token.", "relation": "calls", "tail": "barrier_wait", "tail_type": "CUDA_API"}, {"head": "cluster_group::sync", "head_type": "__device__", "description": "Synchronizes the threads within the cluster group.", "relation": "calls", "tail": "barrier_wait", "tail_type": "CUDA_API"}, {"head": "cluster_group::barrier_arrive", "head_type": "__device__", "description": "Arrives on the cluster barrier and returns an arrival token.", "relation": "calls", "tail": "barrier_wait", "tail_type": "CUDA_API"}, {"head": "grid_group::sync", "head_type": "__device__", "description": "Synchronizes the threads within the grid group if the grid_group is valid.", "relation": "calls", "tail": "barrier_wait", "tail_type": "CUDA_API"}, {"head": "grid_group::barrier_arrive", "head_type": "__device__", "description": "Arrives on the grid barrier and returns an arrival token.", "relation": "calls", "tail": "barrier_wait", "tail_type": "CUDA_API"}]}
{"pdf_page": [284, 287], "model_response": [{"head": "multi_grid_group::is_valid", "head_type": "__device__", "description": "Returns whether the multi_grid_group can be used", "relation": "single", "tail": "", "tail_type": ""}, {"head": "multi_grid_group::sync", "head_type": "__device__", "description": "Synchronize the threads named in the group", "relation": "single", "tail": "", "tail_type": ""}, {"head": "multi_grid_group::num_threads", "head_type": "__device__", "description": "Total number of threads in the group", "relation": "single", "tail": "", "tail_type": ""}, {"head": "multi_grid_group::thread_rank", "head_type": "__device__", "description": "Rank of the calling thread within [0, num_threads)", "relation": "single", "tail": "", "tail_type": ""}, {"head": "multi_grid_group::grid_rank", "head_type": "__device__", "description": "Rank of the grid within [0,num_grids]", "relation": "single", "tail": "", "tail_type": ""}, {"head": "multi_grid_group::num_grids", "head_type": "__device__", "description": "Total number of grids launched", "relation": "single", "tail": "", "tail_type": ""}, {"head": "thread_block_tile::sync", "head_type": "__device__", "description": "Synchronize the threads named in the group", "relation": "single", "tail": "", "tail_type": ""}, {"head": "thread_block_tile::num_threads", "head_type": "__device__", "description": "Total number of threads in the group", "relation": "single", "tail": "", "tail_type": ""}, {"head": "thread_block_tile::thread_rank", "head_type": "__device__", "description": "Rank of the calling thread within [0, num_threads)", "relation": "single", "tail": "", "tail_type": ""}, {"head": "thread_block_tile::meta_group_size", "head_type": "__device__", "description": "Returns the number of groups created when the parent group was partitioned", "relation": "single", "tail": "", "tail_type": ""}, {"head": "thread_block_tile::meta_group_rank", "head_type": "__device__", "description": "Linear rank of the group within the set of tiles partitioned from a parent group", "relation": "single", "tail": "", "tail_type": ""}, {"head": "tiled_partition", "head_type": "__device__", "description": "Constructs a thread_block_tile", "relation": "calls", "tail": "ParentT", "tail_type": "CUDA_CLASS"}, {"head": "this_thread_block", "head_type": "__device__", "description": "Obtain the default 'current thread block' group", "relation": "single", "tail": "", "tail_type": ""}, {"head": "block_tile_memory", "head_type": "__device__", "description": "Template struct for reserving shared memory for thread_block_tile usage", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [287, 290], "model_response": [{"head": "cooperative_groups::memcpy_async", "head_type": "__device__", "description": "Performs an asynchronous memory copy using a thread group.", "relation": "calls", "tail": "cooperative_groups::this_thread", "tail_type": "CUDA_CLASS"}, {"head": "coalesced_group::sync", "head_type": "__device__", "description": "Synchronizes the threads within the coalesced group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::num_threads", "head_type": "__device__", "description": "Returns the total number of threads in the coalesced group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::thread_rank", "head_type": "__device__", "description": "Returns the rank of the calling thread within the coalesced group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::meta_group_size", "head_type": "__device__", "description": "Returns the number of groups created when the parent group was partitioned.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::meta_group_rank", "head_type": "__device__", "description": "Returns the linear rank of the group within the set of tiles partitioned from a parent group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::shfl", "head_type": "__device__", "description": "Performs a warp shuffle operation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::shfl_up", "head_type": "__device__", "description": "Performs a warp shuffle operation up within the group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::shfl_down", "head_type": "__device__", "description": "Performs a warp shuffle operation down within the group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::any", "head_type": "__device__", "description": "Performs a warp vote operation to check if any thread in the group satisfies a condition.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::all", "head_type": "__device__", "description": "Performs a warp vote operation to check if all threads in the group satisfy a condition.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::ballot", "head_type": "__device__", "description": "Performs a warp vote operation to collect a ballot of thread satisfaction of a condition.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::match_any", "head_type": "__device__", "description": "Performs a warp match operation to find any thread with a matching value.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_group::match_all", "head_type": "__device__", "description": "Performs a warp match operation to find all threads with a matching value.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "tiled_partition", "head_type": "__device__", "description": "Partitions the parent group into a one-dimensional, row-major tiling of subgroups.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [290, 293], "model_response": [{"head": "labeled_partition", "head_type": "__device__", "description": "Partitions the parent group into one-dimensional subgroups based on a label.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "binary_partition", "head_type": "__device__", "description": "Partitions the parent group into one-dimensional subgroups based on a predicate.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "barrier_arrive", "head_type": "__device__", "description": "Arrives at a barrier and returns an arrival token.", "relation": "calls", "tail": "barrier_wait", "tail_type": "CUDA_API"}, {"head": "barrier_wait", "head_type": "__device__", "description": "Waits at a barrier using an arrival token.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "sync", "head_type": "__device__", "description": "Synchronizes the threads within a group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "memcpy_async", "head_type": "__device__", "description": "Performs an asynchronous memory copy within a group.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [293, 296], "model_response": [{"head": "memcpy_async", "head_type": "__global__", "description": "Performs a copy of shape bytes between host and device memory.", "relation": "calls", "tail": "wait", "tail_type": "CUDA_API"}, {"head": "wait_prior", "head_type": "__global__", "description": "Waits for a certain number of previous memcpy_async requests to complete.", "relation": "calls", "tail": "memcpy_async", "tail_type": "CUDA_API"}, {"head": "reduce", "head_type": "__global__", "description": "Performs a reduction operation on data provided by each thread in the group.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [296, 299], "model_response": [{"head": "reduce_update_async", "head_type": "__device__", "description": "Asynchronously updates a reduction result in an atomic variable.", "relation": "calls", "tail": "cg::reduce", "tail_type": "CUDA_API"}, {"head": "reduce_store_async", "head_type": "__device__", "description": "Asynchronously stores a reduction result in a destination pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "reduce", "head_type": "__device__", "description": "Performs a reduction operation within a thread block.", "relation": "calls", "tail": "__reduce_add_sync", "tail_type": "CUDA_API"}, {"head": "__reduce_add_sync", "head_type": "__device__", "description": "A specialized reduction function that performs an addition operation synchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "block_reduce", "head_type": "__device__", "description": "An example function that demonstrates block-wide reduction.", "relation": "calls", "tail": "reduce_update_async", "tail_type": "CUDA_API"}, {"head": "cg::reduce", "head_type": "__device__", "description": "A cooperative groups reduction function that can use hardware acceleration.", "relation": "calls", "tail": "cg::plus", "tail_type": "CUDA_CLASS"}, {"head": "cg::plus", "head_type": "__device__", "description": "A reduction operation that adds two values.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [299, 303], "model_response": [{"head": "inclusive_scan", "head_type": "__global__", "description": "Performs a scan operation on the data provided by each thread in the group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "exclusive_scan", "head_type": "__global__", "description": "Performs a scan operation, excluding the data from the calling thread.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "inclusive_scan_update", "head_type": "__global__", "description": "Performs a scan operation and updates an atomic value.", "relation": "calls", "tail": "atomic.load", "tail_type": "CUDA_API"}, {"head": "exclusive_scan_update", "head_type": "__global__", "description": "Performs a scan operation, excluding the data from the calling thread, and updates an atomic value.", "relation": "calls", "tail": "atomic.store", "tail_type": "CUDA_API"}, {"head": "stream_compaction", "head_type": "__device__", "description": "Compacts a stream of data based on a predicate.", "relation": "calls", "tail": "exclusive_scan", "tail_type": "CUDA_API"}, {"head": "calculate_buffer_space_needed", "head_type": "__device__", "description": "Calculates the buffer space needed by a thread.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "kernel", "head_type": "__global__", "description": "A CUDA kernel that demonstrates buffer space allocation using exclusive_scan_update.", "relation": "calls", "tail": "exclusive_scan_update", "tail_type": "CUDA_API"}, {"head": "invoke_one", "head_type": "__global__", "description": "Invokes a function on a single arbitrary thread of the group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "invoke_one_broadcast", "head_type": "__global__", "description": "Invokes a function on a single arbitrary thread and broadcasts the result to all threads in the group.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [303, 307], "model_response": [{"head": "atomicAddOneRelaxed", "head_type": "__device__", "description": "Performs a relaxed atomic add operation on a given atomic variable.", "relation": "calls", "tail": "atomic.fetch_add", "tail_type": "CUDA_API"}, {"head": "grid.sync", "head_type": "__device__", "description": "Synchronizes all the threads across the entire grid.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchCooperativeKernel", "head_type": "__host__", "description": "Launches a cooperative kernel on the GPU.", "relation": "calls", "tail": "cudaOccupancyMaxActiveBlocksPerMultiprocessor", "tail_type": "CUDA_API"}, {"head": "cudaDeviceGetAttribute", "head_type": "__host__", "description": "Gets the value of a device attribute.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Retrieves the properties of a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchCooperativeKernelMultiDevice", "head_type": "__host__", "description": "Launches a cooperative kernel across multiple devices.", "relation": "calls", "tail": "cudaDeviceEnablePeerAccess", "tail_type": "CUDA_API"}, {"head": "cudaSetDevice", "head_type": "__host__", "description": "Sets the device to be used for CUDA API calls.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamCreate", "head_type": "__host__", "description": "Creates a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetAttribute", "head_type": "__host__", "description": "Queries a device attribute.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [319, 322], "model_response": [{"head": "__global__ void permute", "head_type": "__global__", "description": "Kernel function to permute data using shared memory.", "relation": "calls", "tail": "permute_data", "tail_type": "CUDA_API"}, {"head": "__global__ void permute", "head_type": "__global__", "description": "Kernel function to permute data using shared memory.", "relation": "calls", "tail": "__syncthreads", "tail_type": "CUDA_API"}, {"head": "__global__ void permute", "head_type": "__global__", "description": "Kernel function to launch child kernels for further data permutation.", "relation": "calls", "tail": "permute <<<...>>>", "tail_type": "CUDA_API"}, {"head": "void host_launch", "head_type": "__host__", "description": "Host function to launch the permute kernel.", "relation": "calls", "tail": "permute <<<...>>>", "tail_type": "CUDA_API"}, {"head": "cudaGetLastError", "head_type": "__device__", "description": "Function to retrieve the last error code generated by a CUDA call.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetParameterBuffer", "head_type": "__device__", "description": "Function to get the parameter buffer for a kernel launch.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchDevice", "head_type": "__device__", "description": "Function to launch a device kernel.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [323, 327], "model_response": [{"head": "cudaLaunchDevice", "head_type": "__device__", "description": "Launches a device-side kernel with specified parameters.", "relation": "calls", "tail": "cudaGetParameterBuffer", "tail_type": "CUDA_API"}, {"head": "parentKernel", "head_type": "__global__", "description": "A kernel that launches other kernels.", "relation": "calls", "tail": "childKernel", "tail_type": "__global__"}, {"head": "parentKernel", "head_type": "__global__", "description": "A kernel that launches other kernels.", "relation": "calls", "tail": "cudaGetLastError", "tail_type": "CUDA_API"}, {"head": "parentKernel", "head_type": "__global__", "description": "A kernel that launches other kernels.", "relation": "calls", "tail": "tailKernel", "tail_type": "__global__"}, {"head": "main", "head_type": "__host__", "description": "The main function that launches the parent kernel.", "relation": "calls", "tail": "parentKernel", "tail_type": "__global__"}, {"head": "main", "head_type": "__host__", "description": "The main function that checks for errors after kernel launches.", "relation": "calls", "tail": "cudaGetLastError", "tail_type": "CUDA_API"}, {"head": "main", "head_type": "__host__", "description": "The main function that waits for the parent kernel to complete.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}]}
{"pdf_page": [334, 337], "model_response": [{"head": "__global__ void parent_launch", "head_type": "__global__", "description": "The parent kernel that launches a child grid and synchronizes with it.", "relation": "calls", "tail": "__global__ void child_launch", "tail_type": "__global__"}, {"head": "__global__ void parent_launch", "head_type": "__global__", "description": "Synchronization call within the parent kernel.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "__global__ void child_launch", "head_type": "__global__", "description": "The child kernel that is launched by the parent.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "void host_launch", "head_type": "__host__", "description": "Host function that launches the parent kernel.", "relation": "calls", "tail": "__global__ void parent_launch", "tail_type": "__global__"}, {"head": "__device__ void x", "head_type": "__device__", "description": "Device function that correctly launches a child kernel with a global memory pointer.", "relation": "calls", "tail": "child <<<1,1>>>(&value)", "tail_type": "__global__"}, {"head": "__device__ void y", "head_type": "__device__", "description": "Device function that incorrectly attempts to launch a child kernel with a local memory pointer.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [341, 344], "model_response": [{"head": "__global__ void permute", "head_type": "__global__", "description": "Kernel function that performs a permutation on data.", "relation": "calls", "tail": "permute_data", "tail_type": "CUDA_API"}, {"head": "__global__ void permute", "head_type": "__global__", "description": "Kernel function that performs a permutation on data.", "relation": "calls", "tail": "__syncthreads", "tail_type": "CUDA_API"}, {"head": "__global__ void permute", "head_type": "__global__", "description": "Kernel function that launches child kernels for further permutation.", "relation": "calls", "tail": "permute <<<...>>>", "tail_type": "CUDA_API"}, {"head": "void host_launch", "head_type": "__host__", "description": "Host function that launches the permute kernel.", "relation": "calls", "tail": "permute <<<...>>>", "tail_type": "CUDA_API"}, {"head": "cudaGetLastError", "head_type": "__device__", "description": "Function to retrieve the last CUDA error code.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetParameterBuffer", "head_type": "__device__", "description": "Function to get a parameter buffer for a kernel launch.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchDevice", "head_type": "__device__", "description": "Function to launch a CUDA kernel from the device.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [345, 348], "model_response": [{"head": "cudaLaunchDevice", "head_type": "__device__", "description": "Launches a kernel with the specified parameter buffer and launch configuration.", "relation": "calls", "tail": "cudaGetParameterBuffer", "tail_type": "CUDA_API"}, {"head": "cudaGetParameterBuffer", "head_type": "__device__", "description": "Returns a parameter buffer with the specified alignment and size.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [348, 352], "model_response": [{"head": "childKernel", "head_type": "__global__", "description": "A simple CUDA kernel that prints 'Hello '.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "parentKernel", "head_type": "__global__", "description": "A CUDA kernel that launches the childKernel and waits for it to complete by calling cudaDeviceSynchronize().", "relation": "calls", "tail": "childKernel", "tail_type": "__global__"}, {"head": "parentKernel", "head_type": "__global__", "description": "Checks the error status of the child kernel launch and device synchronization.", "relation": "calls", "tail": "cudaGetLastError", "tail_type": "CUDA_API"}, {"head": "parentKernel", "head_type": "__global__", "description": "Waits for the child kernel to complete.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "main", "head_type": "__host__", "description": "The main function that launches the parentKernel and waits for it to complete.", "relation": "calls", "tail": "parentKernel", "tail_type": "__global__"}, {"head": "main", "head_type": "__host__", "description": "Checks the error status of the parent kernel launch and device synchronization.", "relation": "calls", "tail": "cudaGetLastError", "tail_type": "CUDA_API"}, {"head": "main", "head_type": "__host__", "description": "Waits for the parent kernel to complete.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}]}
{"pdf_page": [355, 357], "model_response": [{"head": "cuDeviceGetAttribute", "head_type": "__host__", "description": "Query for Virtual Memory Management support on a device", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemCreate", "head_type": "__host__", "description": "Allocates physical memory without device or host mappings", "relation": "calls", "tail": "cuMemGetAllocationGranularity", "tail_type": "CUDA_API"}, {"head": "cuMemCreate", "head_type": "__host__", "description": "Allocates physical memory with specified properties", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemGetAllocationPropertiesFromHandle", "head_type": "__host__", "description": "Queries properties of a memory allocation", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemAddressReserve", "head_type": "__host__", "description": "Reserves a virtual address range", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemRelease", "head_type": "__host__", "description": "Frees memory allocated by cuMemCreate", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemExportToShareableHandle", "head_type": "__host__", "description": "Exports a memory allocation to a shareable handle", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemImportFromShareableHandle", "head_type": "__host__", "description": "Imports a memory allocation from a shareable handle", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGetAttribute", "head_type": "__host__", "description": "Queries for support of a handle type on a device", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [357, 360], "model_response": [{"head": "cuDeviceGetAttribute", "head_type": "__host__", "description": "Queries device attributes to check if generic compression is supported.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemCreate", "head_type": "__host__", "description": "Allocates memory with specific allocation properties, such as compression type.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemGetAllocationPropertiesFromHandle", "head_type": "__host__", "description": "Retrieves properties of an allocated memory block, including compression type.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemAddressReserve", "head_type": "__host__", "description": "Reserves a virtual address range for later use with memory mappings.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemMap", "head_type": "__host__", "description": "Maps a physical allocation to a reserved virtual address range.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemSetAccess", "head_type": "__host__", "description": "Sets access rights for a device to a mapped virtual address range.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemUnmap", "head_type": "__host__", "description": "Unmaps a virtual address range that was previously mapped with cuMemMap.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemAddressFree", "head_type": "__host__", "description": "Frees a previously reserved virtual address range.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [360, 363], "model_response": [{"head": "cudaDriverGetVersion", "head_type": "__host__", "description": "Gets the version of the installed CUDA driver.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetAttribute", "head_type": "__host__", "description": "Queries device attributes.", "relation": "calls", "tail": "cudaDevAttrMemoryPoolsSupported", "tail_type": "CUDA_API"}, {"head": "cudaDeviceGetAttribute", "head_type": "__host__", "description": "Queries device attributes for memory pool handle types.", "relation": "calls", "tail": "cudaDevAttrMemoryPoolSupportedHandleTypes", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory asynchronously.", "relation": "calls", "tail": "cudaStreamPerThread", "tail_type": "CUDA_API"}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "Frees memory asynchronously.", "relation": "calls", "tail": "cudaStreamPerThread", "tail_type": "CUDA_API"}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Makes a stream wait on an event.", "relation": "calls", "tail": "cudaEventRecord", "tail_type": "CUDA_API"}, {"head": "cudaDeviceSetMempool", "head_type": "__host__", "description": "Sets the current memory pool for a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetMempool", "head_type": "__host__", "description": "Gets the current memory pool for a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocFromPoolAsync", "head_type": "__host__", "description": "Allocates memory from a specified memory pool asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetDefaultMempool", "head_type": "__host__", "description": "Gets a handle to the default memory pool of a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolCreate", "head_type": "__host__", "description": "Creates a memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAttribute", "head_type": "__host__", "description": "Sets attributes of a memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolGetAttribute", "head_type": "__host__", "description": "Gets attributes of a memory pool.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [363, 365], "model_response": [{"head": "cudaDeviceGetDefaultMempool", "head_type": "__host__", "description": "Retrieves the default memory pool of a device", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "Modifies the accessibility of the default memory pool", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolGetAccess", "head_type": "__host__", "description": "Queries the accessibility of the default memory pool", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolCreate", "head_type": "__host__", "description": "Creates an explicit memory pool", "relation": "calls", "tail": "cudaMemPoolProps", "tail_type": "CUDA_CLASS"}, {"head": "cudaMemPoolCreate", "head_type": "__host__", "description": "Creates an IPC capable memory pool on a CPU NUMA node", "relation": "calls", "tail": "cudaMemPoolProps", "tail_type": "CUDA_CLASS"}, {"head": "cudaMemPoolSetAttribute", "head_type": "__host__", "description": "Sets an attribute for a memory pool", "relation": "calls", "tail": "cudaMemPoolAttrReleaseThreshold", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolTrimTo", "head_type": "__host__", "description": "Allows explicitly shrinking a memory pool's memory footprint", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory from a memory pool asynchronously", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "Frees memory from a memory pool asynchronously", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamSynchronize", "head_type": "__host__", "description": "Waits for a stream to complete", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [365, 369], "model_response": [{"head": "cudaMemPoolGetAttribute", "head_type": "__host__", "description": "Gets an attribute from a memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrReservedMemCurrent", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolGetAttribute", "head_type": "__host__", "description": "Gets an attribute from a memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrReservedMemHigh", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolGetAttribute", "head_type": "__host__", "description": "Gets an attribute from a memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrUsedMemCurrent", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolGetAttribute", "head_type": "__host__", "description": "Gets an attribute from a memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrUsedMemHigh", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolSetAttribute", "head_type": "__host__", "description": "Sets an attribute for a memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrReservedMemHigh", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolSetAttribute", "head_type": "__host__", "description": "Sets an attribute for a memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrUsedMemHigh", "tail_type": "CUDA_API"}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Makes a stream wait on an event.", "relation": "calls", "tail": "cudaEventRecord", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "Frees memory asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceCanAccessPeer", "head_type": "__host__", "description": "Queries if a device can access the memory of another device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "Sets the accessibility of a memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolCreate", "head_type": "__host__", "description": "Creates a memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolExportToShareableHandle", "head_type": "__host__", "description": "Exports a memory pool to a shareable handle.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [368, 372], "model_response": [{"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "Sets the access rights for a memory pool.", "relation": "calls", "tail": "cudaErrorPeerAccessUnsupported", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolExportToShareableHandle", "head_type": "__host__", "description": "Exports a memory pool to a shareable handle.", "relation": "calls", "tail": "cudaMemPoolCreate", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolImportFromShareableHandle", "head_type": "__host__", "description": "Imports a memory pool from a shareable handle.", "relation": "calls", "tail": "cudaMemPoolCreate", "tail_type": "CUDA_API"}, {"head": "cudaEventCreate", "head_type": "__host__", "description": "Creates an event.", "relation": "calls", "tail": "cudaEventDisableTiming", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory from a memory pool asynchronously.", "relation": "calls", "tail": "cudaMemPoolCreate", "tail_type": "CUDA_API"}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event.", "relation": "calls", "tail": "cudaEventCreate", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolExportPointer", "head_type": "__host__", "description": "Exports a pointer from a memory pool.", "relation": "calls", "tail": "cudaMallocAsync", "tail_type": "CUDA_API"}, {"head": "cudaIpcGetEventHandle", "head_type": "__host__", "description": "Gets an interprocess communication event handle.", "relation": "calls", "tail": "cudaEventCreate", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolImportPointer", "head_type": "__host__", "description": "Imports a pointer into a memory pool.", "relation": "calls", "tail": "cudaMemPoolCreate", "tail_type": "CUDA_API"}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Makes a stream wait on an event.", "relation": "calls", "tail": "cudaEventCreate", "tail_type": "CUDA_API"}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "Frees memory asynchronously.", "relation": "calls", "tail": "cudaMallocAsync", "tail_type": "CUDA_API"}, {"head": "cudaIpcEventRecord", "head_type": "__host__", "description": "Records an interprocess communication event.", "relation": "calls", "tail": "cudaEventCreate", "tail_type": "CUDA_API"}]}
{"pdf_page": [375, 379], "model_response": [{"head": "cudaGraphCreate", "head_type": "__host__", "description": "Create an empty CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddMemAllocNode", "head_type": "__host__", "description": "Add a memory allocation node to a CUDA graph.", "relation": "calls", "tail": "CUDA_MEM_ALLOC_NODE_PARAMS", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphAddKernelNode", "head_type": "__host__", "description": "Add a kernel node to a CUDA graph.", "relation": "calls", "tail": "cudaGraphNode_t", "tail_type": "CUDA_API"}, {"head": "cudaGraphAddMemFreeNode", "head_type": "__host__", "description": "Add a memory free node to a CUDA graph.", "relation": "calls", "tail": "cudaGraphNode_t", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Asynchronously allocate memory on the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "Asynchronously free memory on the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "End capture on a stream and create a graph.", "relation": "calls", "tail": "cudaGraph_t", "tail_type": "CUDA_API"}, {"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiate a CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launch a CUDA graph.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [379, 382], "model_response": [{"head": "cudaGraphAddMemAllocNode", "head_type": "__host__", "description": "Adds a memory allocation node to a graph.", "relation": "calls", "tail": "NULL", "tail_type": ""}, {"head": "cudaGraphAddKernelNode", "head_type": "__host__", "description": "Adds a kernel node to a graph.", "relation": "calls", "tail": "NULL", "tail_type": ""}, {"head": "cudaGraphAddMemFreeNode", "head_type": "__host__", "description": "Adds a memory free node to a graph.", "relation": "calls", "tail": "NULL", "tail_type": ""}, {"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches an instantiated graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Makes a stream wait on an event.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddEventRecordNode", "head_type": "__host__", "description": "Adds an event record node to a graph.", "relation": "calls", "tail": "NULL", "tail_type": ""}, {"head": "cudaGraphAddEventWaitNode", "head_type": "__host__", "description": "Adds an event wait node to a graph.", "relation": "calls", "tail": "NULL", "tail_type": ""}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "Frees memory asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphDestroy", "head_type": "__host__", "description": "Destroys a graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphExecDestroy", "head_type": "__host__", "description": "Destroys an instantiated graph.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [386, 388], "model_response": [{"head": "cudaDeviceGetGraphMemAttribute", "head_type": "__host__", "description": "Queries graph memory footprint attributes", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddMemAllocNode", "head_type": "__host__", "description": "Adds a memory allocation node to a graph", "relation": "calls", "tail": "cudaMemAllocNodeParams", "tail_type": "CUDA_CLASS"}, {"head": "cudaMemAllocNodeParams", "head_type": "__host__", "description": "Parameters structure for memory allocation node", "relation": "calls", "tail": "cudaMemPoolSetAccess", "tail_type": "CUDA_API"}, {"head": "cudaStreamBeginCapture", "head_type": "__host__", "description": "Begins capturing a stream", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory asynchronously", "relation": "calls", "tail": "cudaMemPoolSetAccess", "tail_type": "CUDA_API"}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends capturing a stream", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "Sets the accessibility of a memory pool", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [483, 485], "model_response": [{"head": "cuInit", "head_type": "__host__", "description": "Initializes the CUDA driver API.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGetCount", "head_type": "__host__", "description": "Gets the number of devices supporting CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGet", "head_type": "__host__", "description": "Gets a handle for a specific device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuCtxCreate", "head_type": "__host__", "description": "Creates a CUDA context.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuModuleLoad", "head_type": "__host__", "description": "Loads a module from a binary file.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemAlloc", "head_type": "__host__", "description": "Allocates device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemcpyHtoD", "head_type": "__host__", "description": "Copies memory from host to device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuModuleGetFunction", "head_type": "__host__", "description": "Gets a function handle from a module.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuLaunchKernel", "head_type": "__host__", "description": "Launches a kernel.", "relation": "calls", "tail": "vecAdd", "tail_type": "CUDA_API"}, {"head": "cuCtxPopCurrent", "head_type": "__host__", "description": "Pops the current context from the stack.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuCtxAttach", "head_type": "__host__", "description": "Increments the usage count of a context.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuCtxDetach", "head_type": "__host__", "description": "Decrements the usage count of a context.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDevicePrimaryCtxRetain", "head_type": "__host__", "description": "Retains the primary context of a device.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [485, 487], "model_response": [{"head": "cuModuleLoad", "head_type": "__host__", "description": "Loads a module from a file.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuModuleGetFunction", "head_type": "__host__", "description": "Retrieves a function handle from a module.", "relation": "calls", "tail": "cuModule", "tail_type": "CUDA_CLASS"}, {"head": "cuModuleLoadDataEx", "head_type": "__host__", "description": "Loads a module from a string of PTX code with extra options.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuLinkCreate", "head_type": "__host__", "description": "Creates a link state for linking multiple PTX modules.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuLinkAddData", "head_type": "__host__", "description": "Adds PTX code to the link state.", "relation": "calls", "tail": "cuLinkState", "tail_type": "CUDA_CLASS"}, {"head": "cuLinkComplete", "head_type": "__host__", "description": "Completes the linking process and returns the binary data.", "relation": "calls", "tail": "cuLinkState", "tail_type": "CUDA_CLASS"}, {"head": "cuModuleLoadData", "head_type": "__host__", "description": "Loads a module from a binary (CUBIN) data.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuLinkDestroy", "head_type": "__host__", "description": "Destroys the link state.", "relation": "calls", "tail": "cuLinkState", "tail_type": "CUDA_CLASS"}, {"head": "cuLaunchKernel", "head_type": "__host__", "description": "Launches a kernel with a given execution configuration.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [487, 490], "model_response": [{"head": "ALIGN_UP", "head_type": "__host__", "description": "Macro to align the offset to meet a specified alignment requirement", "relation": "single", "tail": "", "tail_type": ""}, {"head": "ADD_TO_PARAM_BUFFER", "head_type": "__host__", "description": "Macro to add a value to the parameter buffer with proper alignment", "relation": "calls", "tail": "ALIGN_UP", "tail_type": "__host__"}, {"head": "cuLaunchKernel", "head_type": "__host__", "description": "Launches a CUDA function on the device", "relation": "calls", "tail": "CU_LAUNCH_PARAM_BUFFER_POINTER", "tail_type": "CUDA_API"}, {"head": "cuMemAlloc", "head_type": "__host__", "description": "Allocates device memory using the CUDA driver API", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuCtxGetCurrent", "head_type": "__host__", "description": "Retrieves the current context from the CUDA driver API", "relation": "single", "tail": "", "tail_type": ""}, {"head": "PFN_cuMemAlloc_v3020", "head_type": "__host__", "description": "Function pointer type for cuMemAlloc with a specific version", "relation": "single", "tail": "", "tail_type": ""}, {"head": "PFN_cuMemAlloc_v2000", "head_type": "__host__", "description": "Function pointer type for cuMemAlloc with a specific version", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [490, 493], "model_response": [{"head": "cuGetProcAddress", "head_type": "__host__", "description": "Retrieves the function pointer to a CUDA driver API.", "relation": "calls", "tail": "cuStreamBeginCapture", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "Retrieves the function pointer to a CUDA driver API.", "relation": "calls", "tail": "cuStreamBeginCapture_v2", "tail_type": "CUDA_API"}, {"head": "cudaGetDriverEntryPoint", "head_type": "__host__", "description": "Retrieves the function pointer to a CUDA driver API using the CUDA runtime version.", "relation": "calls", "tail": "cuMemAllocAsync", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "Retrieves the function pointer to a CUDA driver API with a specified version.", "relation": "calls", "tail": "cuFoo", "tail_type": "CUDA_API"}, {"head": "cuDriverGetVersion", "head_type": "__host__", "description": "Gets the installed CUDA driver version.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [493, 496], "model_response": [{"head": "cuDeviceGet", "head_type": "__host__", "description": "Gets the handle for device at a specified index.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGetUuid", "head_type": "__host__", "description": "Gets the UUID of the specified device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "Gets the pointer to a CUDA driver function.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}, {"head": "cuDriverGetVersion", "head_type": "__host__", "description": "Gets the version of the installed CUDA driver.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "Gets the pointer to a CUDA driver function based on the driver version.", "relation": "calls", "tail": "cuFoo", "tail_type": "CUDA_API"}, {"head": "cuFoo", "head_type": "__device__", "description": "A theoretical CUDA API function that has been versioned.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [496, 499], "model_response": [{"head": "cudaGetDriverEntryPoint", "head_type": "__host__", "description": "Obtains a function pointer for a CUDA runtime API function.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "Gets a function pointer for a CUDA driver API function.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}, {"head": "cudaDriverGetVersion", "head_type": "__host__", "description": "Gets the version of the installed CUDA driver.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "Gets a function pointer for a CUDA driver API function based on the driver version.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}, {"head": "cuCtxCreate", "head_type": "__host__", "description": "Creates a CUDA context.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [511, 514], "model_response": [{"head": "cudaMalloc", "head_type": "__host__", "description": "Allocates memory on the device.", "relation": "calls", "tail": "cudaMallocManaged", "tail_type": "CUDA_API"}, {"head": "write_value", "head_type": "__global__", "description": "Writes a value to a pointer in global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpy", "head_type": "__host__", "description": "Copies memory from the device to the host.", "relation": "calls", "tail": "cudaMemcpyDefault", "tail_type": "CUDA_API"}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees memory allocated on the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocManaged", "head_type": "__host__", "description": "Allocates managed memory on the device which is accessible from both the host and the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits for the device to finish all outstanding work.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [514, 518], "model_response": [{"head": "printme", "head_type": "__global__", "description": "Kernel function that prints a string passed to it.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocManaged", "head_type": "__host__", "description": "Allocates managed memory that is accessible from both host and device.", "relation": "calls", "tail": "cudaFree", "tail_type": "CUDA_API"}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees memory allocated by cudaMallocManaged.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "write_value", "head_type": "__global__", "description": "Kernel function that writes an integer value to a pointer location.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits for all device tasks to complete.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaPointerGetAttributes", "head_type": "__host__", "description": "Retrieves information about a pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDevice", "head_type": "__host__", "description": "Gets the current device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetAttribute", "head_type": "__host__", "description": "Retrieves device attributes.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [518, 521], "model_response": [{"head": "cudaMemPrefetchAsync", "head_type": "__host__", "description": "Asynchronous prefetch of data to specified processor.", "relation": "calls", "tail": "cudaStream_t", "tail_type": "CUDA_API"}, {"head": "cudaMemAdvise", "head_type": "__host__", "description": "Set usage hints for a memory range.", "relation": "calls", "tail": "cudaMemoryAdvise", "tail_type": "CUDA_ENUM"}, {"head": "cudaMemRangeGetAttribute", "head_type": "__host__", "description": "Query attributes of a memory range.", "relation": "calls", "tail": "cudaMemRangeAttribute", "tail_type": "CUDA_ENUM"}, {"head": "test_prefetch_sam", "head_type": "__host__", "description": "Example function that demonstrates prefetching using System Allocator Memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "test_prefetch_managed", "head_type": "__host__", "description": "Example function that demonstrates prefetching using Managed Memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "test_advise_managed", "head_type": "__host__", "description": "Example function that demonstrates setting usage hints on Managed Memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [521, 525], "model_response": [{"head": "kernel", "head_type": "__global__", "description": "A kernel that prints the first 8 characters of an input character array to the standard output stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "test_malloc", "head_type": "__host__", "description": "A function that allocates memory on the host using malloc, and then calls the kernel with the allocated memory.", "relation": "calls", "tail": "malloc", "tail_type": "CUDA_API"}, {"head": "test_malloc", "head_type": "__host__", "description": "A function that allocates memory on the host using malloc, and then calls the kernel with the allocated memory.", "relation": "calls", "tail": "free", "tail_type": "CUDA_API"}, {"head": "test_managed", "head_type": "__host__", "description": "A function that allocates managed memory and calls the kernel with it.", "relation": "calls", "tail": "cudaMallocManaged", "tail_type": "CUDA_API"}, {"head": "test_managed", "head_type": "__host__", "description": "A function that allocates managed memory and calls the kernel with it.", "relation": "calls", "tail": "cudaFree", "tail_type": "CUDA_API"}, {"head": "test_stack", "head_type": "__host__", "description": "A function that calls the kernel with a stack-allocated array.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "test_static", "head_type": "__host__", "description": "A function that calls the kernel with a file-scope static array.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "test_global", "head_type": "__host__", "description": "A function that calls the kernel with a global-scope array.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "test_extern", "head_type": "__host__", "description": "A function that calls the kernel with an extern variable.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "setup", "head_type": "__host__", "description": "A constructor function that initializes the extern variable.", "relation": "calls", "tail": "malloc", "tail_type": "CUDA_API"}, {"head": "tear_down", "head_type": "__host__", "description": "A destructor function that cleans up the extern variable.", "relation": "calls", "tail": "free", "tail_type": "CUDA_API"}, {"head": "test_file_backed", "head_type": "__host__", "description": "A function that maps a file into memory and calls the kernel with the mapped memory.", "relation": "calls", "tail": "open", "tail_type": "CUDA_API"}, {"head": "test_file_backed", "head_type": "__host__", "description": "A function that maps a file into memory and calls the kernel with the mapped memory.", "relation": "calls", "tail": "close", "tail_type": "CUDA_API"}, {"head": "test_file_backed", "head_type": "__host__", "description": "A function that maps a file into memory and calls the kernel with the mapped memory.", "relation": "calls", "tail": "mmap", "tail_type": "CUDA_API"}, {"head": "test_file_backed", "head_type": "__host__", "description": "A function that maps a file into memory and calls the kernel with the mapped memory.", "relation": "calls", "tail": "munmap", "tail_type": "CUDA_API"}]}
{"pdf_page": [527, 529], "model_response": [{"head": "write", "head_type": "__global__", "description": "Kernel that writes to an integer array in GPU memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "append", "head_type": "__global__", "description": "Kernel that appends values to an integer array in GPU memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemAdvise", "head_type": "__host__", "description": "Sets memory usage hints for a given memory range.", "relation": "calls", "tail": "cudaMemAdviseSetAccessedBy", "tail_type": "CUDA_API"}, {"head": "cudaMallocManaged", "head_type": "__host__", "description": "Allocates memory that is managed by CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits for the device to finish executing all tasks.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees memory allocated by cudaMallocManaged.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [532, 535], "model_response": [{"head": "cudaStreamCreate", "head_type": "__host__", "description": "Creates a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocHost", "head_type": "__host__", "description": "Allocates page-locked host memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocManaged", "head_type": "__host__", "description": "Allocates managed memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "kernel", "head_type": "__global__", "description": "Launches a CUDA kernel.", "relation": "calls", "tail": "cudaStream_t", "tail_type": "CUDA_API"}, {"head": "cudaSetDevice", "head_type": "__host__", "description": "Sets the device to be used for CUDA kernel executions.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamAttachMemAsync", "head_type": "__host__", "description": "Associates memory with a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits for all device tasks to complete.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "transform", "head_type": "__global__", "description": "A kernel that presumably transforms data.", "relation": "calls", "tail": "cudaStream_t", "tail_type": "CUDA_API"}, {"head": "host_process", "head_type": "__host__", "description": "A CPU function that processes data.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "convert", "head_type": "__global__", "description": "A kernel that presumably converts data.", "relation": "calls", "tail": "cudaStream_t", "tail_type": "CUDA_API"}, {"head": "cudaStreamDestroy", "head_type": "__host__", "description": "Destroys a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [547, 550], "model_response": [{"head": "cuMemCreate", "head_type": "__host__", "description": "Creates a physical memory allocation.", "relation": "calls", "tail": "cuMemGetAllocationGranularity", "tail_type": "CUDA_API"}, {"head": "cuMemAddressReserve", "head_type": "__host__", "description": "Reserves an address space for the allocation.", "relation": "calls", "tail": "cuMemMap", "tail_type": "CUDA_API"}, {"head": "cuMemMap", "head_type": "__host__", "description": "Maps the reserved address space to a pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemSetAccess", "head_type": "__host__", "description": "Sets access rights to the mapped memory for devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolCreate", "head_type": "__host__", "description": "Creates a memory pool on a node.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "Sets access to the memory pool for peer devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetMemPool", "head_type": "__host__", "description": "Sets the memory pool for a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory from the set memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemExportToShareableHandle", "head_type": "__host__", "description": "Exports a memory allocation handle to be shared across nodes.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemImportFromShareableHandle", "head_type": "__host__", "description": "Imports a shared memory allocation handle from another node.", "relation": "single", "tail": "", "tail_type": ""}]}
