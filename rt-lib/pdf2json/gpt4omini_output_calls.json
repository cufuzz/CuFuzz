{"pdf_page": [26, 29], "model_response": [{"head": "VecAdd", "head_type": "__global__", "description": "This kernel adds two vectors A and B and stores the result in C.", "relation": "calls", "tail": "threadIdx", "tail_type": "CUDA_API"}, {"head": "MatAdd", "head_type": "__global__", "description": "This kernel adds two matrices A and B and stores the result in C.", "relation": "calls", "tail": "threadIdx", "tail_type": "CUDA_API"}, {"head": "MatAdd", "head_type": "__global__", "description": "This kernel adds two matrices A and B and stores the result in C.", "relation": "calls", "tail": "blockIdx", "tail_type": "CUDA_API"}, {"head": "MatAdd", "head_type": "__global__", "description": "This kernel adds two matrices A and B and stores the result in C.", "relation": "calls", "tail": "blockDim", "tail_type": "CUDA_API"}, {"head": "MatAdd", "head_type": "__global__", "description": "This kernel adds two matrices A and B and stores the result in C.", "relation": "calls", "tail": "__syncthreads", "tail_type": "CUDA_API"}]}
{"pdf_page": [29, 32], "model_response": [{"head": "cudaOccupancyMaxPotentialClusterSize", "head_type": "__host__", "description": "Queries the maximum potential cluster size that can be supported on the hardware.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__global__ void __cluster_dims__(2,1,1)cluster_kernel", "head_type": "__global__", "description": "Defines a kernel with compile time cluster size.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchKernelEx", "head_type": "__host__", "description": "Launches a kernel with runtime cluster size.", "relation": "calls", "tail": "cluster_kernel", "tail_type": "CUDA_API"}, {"head": "cluster.sync()", "head_type": "__device__", "description": "Synchronizes all thread blocks in the cluster.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "num_threads()", "head_type": "__device__", "description": "Queries the number of threads in the cluster group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "num_blocks()", "head_type": "__device__", "description": "Queries the number of blocks in the cluster group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "dim_threads()", "head_type": "__device__", "description": "Queries the rank of a thread in the cluster group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "dim_blocks()", "head_type": "__device__", "description": "Queries the rank of a block in the cluster group.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [32, 35], "model_response": []}
{"pdf_page": [41, 45], "model_response": [{"head": "cudaMalloc", "head_type": "__host__", "description": "Allocates device memory.", "relation": "calls", "tail": "cudaMemcpy", "tail_type": "CUDA_API"}, {"head": "cudaMemcpy", "head_type": "__host__", "description": "Copies data between host memory and device memory.", "relation": "calls", "tail": "cudaMemcpy", "tail_type": "CUDA_API"}, {"head": "VecAdd", "head_type": "__global__", "description": "Kernel function for vector addition.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocPitch", "head_type": "__host__", "description": "Allocates aligned device memory for 2D arrays.", "relation": "calls", "tail": "MyKernel", "tail_type": "CUDA_CLASS"}, {"head": "MyKernel", "head_type": "__global__", "description": "Kernel function that processes a 2D array.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMalloc3D", "head_type": "__host__", "description": "Allocates aligned device memory for 3D arrays.", "relation": "calls", "tail": "MyKernel", "tail_type": "CUDA_CLASS"}, {"head": "cudaMemcpyToSymbol", "head_type": "__host__", "description": "Copies data from the host to a symbol in global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetSymbolAddress", "head_type": "__host__", "description": "Retrieves the address of a symbol in global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetSymbolSize", "head_type": "__host__", "description": "Obtains the size of the memory allocated for a symbol in global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetLimit", "head_type": "__host__", "description": "Sets a limit for the CUDA device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Retrieves properties of the specified CUDA device.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [45, 49], "model_response": [{"head": "cudaStreamSetAttribute", "head_type": "__host__", "description": "Sets the attributes to a CUDA stream.", "relation": "calls", "tail": "cudaStreamAttrValue", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphKernelNodeSetAttribute", "head_type": "__host__", "description": "Sets the attributes to a CUDA Graph Kernel node.", "relation": "calls", "tail": "cudaKernelNodeAttrValue", "tail_type": "CUDA_CLASS"}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Queries GPU properties.", "relation": "calls", "tail": "cudaDeviceProp", "tail_type": "CUDA_CLASS"}, {"head": "cudaDeviceSetLimit", "head_type": "__host__", "description": "Sets the device limit for the persisting L2 cache size.", "relation": "calls", "tail": "cudaLimit", "tail_type": "CUDA_CLASS"}, {"head": "cudaStreamSetAttribute", "head_type": "__host__", "description": "Sets the attributes to a CUDA stream.", "relation": "calls", "tail": "cudaStreamAttrValue", "tail_type": "CUDA_CLASS"}, {"head": "cudaCtxResetPersistingL2Cache", "head_type": "__host__", "description": "Removes any persistent lines in L2.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamCreate", "head_type": "__host__", "description": "Creates CUDA stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Queries GPU properties.", "relation": "calls", "tail": "cudaDeviceProp", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [49, 54], "model_response": [{"head": "MatMul", "head_type": "__host__", "description": "Loads matrices A and B to device memory.", "relation": "calls", "tail": "cudaMalloc", "tail_type": "CUDA_API"}, {"head": "MatMul", "head_type": "__host__", "description": "Loads matrices A and B to device memory.", "relation": "calls", "tail": "cudaMemcpy", "tail_type": "CUDA_API"}, {"head": "MatMul", "head_type": "__host__", "description": "Invokes the matrix multiplication kernel.", "relation": "calls", "tail": "MatMulKernel", "tail_type": "CUDA_API"}, {"head": "MatMul", "head_type": "__host__", "description": "Reads matrix C from device memory.", "relation": "calls", "tail": "cudaMemcpy", "tail_type": "CUDA_API"}, {"head": "MatMul", "head_type": "__host__", "description": "Frees device memory allocated for matrices.", "relation": "calls", "tail": "cudaFree", "tail_type": "CUDA_API"}, {"head": "MatMulKernel", "head_type": "__global__", "description": "Computes an element of the result matrix C.", "relation": "calls", "tail": "GetSubMatrix", "tail_type": "CUDA_CLASS"}, {"head": "MatMulKernel", "head_type": "__global__", "description": "Computes an element of the result matrix C.", "relation": "calls", "tail": "GetElement", "tail_type": "CUDA_API"}, {"head": "MatMulKernel", "head_type": "__global__", "description": "Writes the element to the result matrix C.", "relation": "calls", "tail": "SetElement", "tail_type": "CUDA_API"}, {"head": "GetSubMatrix", "head_type": "__device__", "description": "Returns a sub-matrix from a given matrix.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "GetElement", "head_type": "__device__", "description": "Retrieves an element from the matrix.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "SetElement", "head_type": "__device__", "description": "Sets an element in the matrix.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [55, 58], "model_response": [{"head": "clusterHist_kernel", "head_type": "__global__", "description": "This kernel computes histograms using shared or distributed shared memory.", "relation": "calls", "tail": "cg::this_cluster", "tail_type": "CUDA_CLASS"}, {"head": "clusterHist_kernel", "head_type": "__global__", "description": "This kernel computes histograms using shared or distributed shared memory.", "relation": "calls", "tail": "atomicAdd", "tail_type": "CUDA_API"}, {"head": "clusterHist_kernel", "head_type": "__global__", "description": "This kernel computes histograms using shared or distributed shared memory.", "relation": "calls", "tail": "cluster.sync", "tail_type": "CUDA_API"}, {"head": "cudaFuncSetAttribute", "head_type": "__host__", "description": "This function sets a specified attribute for a function.", "relation": "calls", "tail": "clusterHist_kernel", "tail_type": "CUDA_API"}, {"head": "cudaLaunchKernelEx", "head_type": "__host__", "description": "This function launches a kernel with a specified configuration.", "relation": "calls", "tail": "clusterHist_kernel", "tail_type": "CUDA_API"}]}
{"pdf_page": [60, 62], "model_response": [{"head": "cudaLaunchKernelEx", "head_type": "__host__", "description": "This function launches a kernel with the specified configuration.", "relation": "calls", "tail": "myKernel", "tail_type": "CUDA_API"}, {"head": "cudaStreamSetAttribute", "head_type": "__host__", "description": "This function sets an attribute for a specified CUDA stream.", "relation": "calls", "tail": "cudaLaunchAttributeMemSyncDomainMap", "tail_type": "CUDA_API"}, {"head": "cudaStreamSetAttribute", "head_type": "__host__", "description": "This function sets an attribute for a specified CUDA stream.", "relation": "calls", "tail": "mapAttr", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [63, 67], "model_response": [{"head": "cudaStreamCreate", "head_type": "__host__", "description": "Creates a new stream for concurrent execution.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocHost", "head_type": "__host__", "description": "Allocates page-locked host memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyAsync", "head_type": "__host__", "description": "Copies data from host to device asynchronously.", "relation": "calls", "tail": "cudaMemcpyDeviceToHost", "tail_type": "CUDA_API"}, {"head": "MyKernel", "head_type": "__global__", "description": "Processes input data on the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamDestroy", "head_type": "__host__", "description": "Releases resources associated with a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits until all preceding commands have completed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamSynchronize", "head_type": "__host__", "description": "Waits until all preceding commands in a specific stream have completed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Delays commands in the stream until the specified event has completed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamQuery", "head_type": "__host__", "description": "Queries to check if all preceding commands in a stream have completed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchHostFunc", "head_type": "__host__", "description": "Inserts a host function call into the stream.", "relation": "calls", "tail": "MyCallback", "tail_type": "CUDA_CLASS"}, {"head": "cudaDeviceGetStreamPriorityRange", "head_type": "__host__", "description": "Retrieves the range of stream priorities supported by the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamCreateWithPriority", "head_type": "__host__", "description": "Creates a stream with specified priority.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [67, 70], "model_response": [{"head": "primary_kernel", "head_type": "__global__", "description": "Initial work that should finish before starting secondary kernel.", "relation": "calls", "tail": "cudaTriggerProgrammaticLaunchCompletion", "tail_type": "CUDA_API"}, {"head": "secondary_kernel", "head_type": "__global__", "description": "Independent work that blocks until primary kernels are completed.", "relation": "calls", "tail": "cudaGridDependencySynchronize", "tail_type": "CUDA_API"}, {"head": "cudaLaunchKernelEx", "head_type": "CUDA_API", "description": "Launches the secondary kernel using the specified configuration.", "relation": "calls", "tail": "secondary_kernel", "tail_type": "__global__"}, {"head": "cudaLaunchAttribute", "head_type": "CUDA_API", "description": "Defines launch attributes for the secondary kernel.", "relation": "calls", "tail": "cudaLaunchAttributeProgrammaticStreamSerialization", "tail_type": "CUDA_API"}]}
{"pdf_page": [72, 75], "model_response": [{"head": "cudaGraphCreate", "head_type": "__host__", "description": "Creates an empty graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddKernelNode", "head_type": "__host__", "description": "Adds a kernel node to the graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddDependencies", "head_type": "__host__", "description": "Adds dependencies between graph nodes.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamBeginCapture", "head_type": "__host__", "description": "Begins capturing work into a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends capturing work and returns a graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event in a given stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Waits for an event to complete in a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamIsCapturing", "head_type": "__host__", "description": "Queries if a stream is currently capturing.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [75, 77], "model_response": [{"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends the stream capture mode.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamAddCallback", "head_type": "__host__", "description": "Adds a callback function to the stream.", "relation": "calls", "tail": "launchWork", "tail_type": "CUDA_API"}, {"head": "cudaUserObjectCreate", "head_type": "__host__", "description": "Creates a CUDA user object.", "relation": "calls", "tail": "cudaUserObjectNoDestructorSync", "tail_type": "CUDA_API"}, {"head": "cudaGraphRetainUserObject", "head_type": "__host__", "description": "Retains a reference to a user object in a graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a CUDA graph execution object.", "relation": "calls", "tail": "cudaGraphDestroy", "tail_type": "CUDA_API"}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches a CUDA graph.", "relation": "calls", "tail": "cudaStreamSynchronize", "tail_type": "CUDA_API"}, {"head": "cudaGraphExecDestroy", "head_type": "__host__", "description": "Destroys a CUDA graph execution object.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamSynchronize", "head_type": "__host__", "description": "Waits for the specified stream to complete.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [77, 81], "model_response": [{"head": "cudaGraphExecUpdate", "head_type": "__host__", "description": "Updates an instantiated graph with new parameters from a topologically identical graph.", "relation": "calls", "tail": "cudaGraph_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaStreamBeginCapture", "head_type": "__host__", "description": "Begins capturing a stream to create a graph.", "relation": "calls", "tail": "do_cuda_work", "tail_type": "CUSTOM_FUNCTION"}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends the capturing of a stream and produces a graph.", "relation": "calls", "tail": "cudaGraph_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a cudaGraphExec_t from a cudaGraph_t.", "relation": "calls", "tail": "cudaGraph_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphDestroy", "head_type": "__host__", "description": "Destroys a cudaGraph_t and frees its resources.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches an instantiated graph for execution.", "relation": "calls", "tail": "cudaGraphExec_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaStreamSynchronize", "head_type": "__host__", "description": "Waits for the stream to finish executing.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphNodeSetEnabled", "head_type": "__host__", "description": "Sets the enabled state of a graph node.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphNodeGetEnabled", "head_type": "__host__", "description": "Queries the enabled state of a graph node.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [81, 87], "model_response": [{"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a device graph for launch.", "relation": "calls", "tail": "cudaGraphInstantiateFlagDeviceLaunch", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphUpload", "head_type": "__host__", "description": "Uploads a previously instantiated graph to the device.", "relation": "calls", "tail": "cudaStream_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphInstantiateWithParams", "head_type": "__host__", "description": "Instantiates a graph with the provided parameters.", "relation": "calls", "tail": "cudaGraphInstantiateFlagDeviceLaunch", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches a graph on the specified stream.", "relation": "calls", "tail": "cudaStream_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaStreamBeginCapture", "head_type": "__host__", "description": "Begins capturing a sequence of operations onto a stream.", "relation": "calls", "tail": "cudaStreamCaptureModeGlobal", "tail_type": "CUDA_CLASS"}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends capturing a sequence of operations onto a stream.", "relation": "calls", "tail": "cudaGraph_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphLaunch", "head_type": "__device__", "description": "Launches a graph from another graph on the device.", "relation": "calls", "tail": "cudaStreamGraphFireAndForget", "tail_type": "CUDA_CLASS"}, {"head": "launchFireAndForgetGraph", "head_type": "__global__", "description": "Launches a fire-and-forget graph using the provided graph handle.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "launchTailGraph", "head_type": "__global__", "description": "Launches a tail graph using the provided graph handle.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}]}
{"pdf_page": [87, 91], "model_response": [{"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a graph for execution.", "relation": "calls", "tail": "cudaGraphInstantiateFlagDeviceLaunch", "tail_type": "CUDA_API"}, {"head": "cudaGraphUpload", "head_type": "__host__", "description": "Uploads a graph to the device.", "relation": "calls", "tail": "stream", "tail_type": "CUDA_API"}, {"head": "cudaStreamBeginCapture", "head_type": "__host__", "description": "Begins capture of a stream.", "relation": "calls", "tail": "cudaStreamCaptureModeGlobal", "tail_type": "CUDA_API"}, {"head": "launchTailGraph", "head_type": "__global__", "description": "Launches a tail graph.", "relation": "calls", "tail": "gExec2", "tail_type": "CUDA_API"}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends capture of a stream.", "relation": "calls", "tail": "g1", "tail_type": "CUDA_API"}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches a graph.", "relation": "calls", "tail": "gExec1", "tail_type": "CUDA_API"}, {"head": "cudaGetCurrentGraphExec", "head_type": "__device__", "description": "Gets the current executing graph instance.", "relation": "calls", "tail": "cudaStreamGraphTailLaunch", "tail_type": "CUDA_API"}, {"head": "cudaGraphLaunch", "head_type": "__global__", "description": "Launches a sibling graph.", "relation": "calls", "tail": "cudaStreamGraphFireAndForgetAsSibling", "tail_type": "CUDA_API"}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends capture of a stream.", "relation": "calls", "tail": "g1", "tail_type": "CUDA_API"}, {"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a graph for execution.", "relation": "calls", "tail": "g1", "tail_type": "CUDA_API"}, {"head": "cudaGraphSetConditional", "head_type": "__device__", "description": "Sets the value of a conditional.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphConditionalHandleCreate", "head_type": "__host__", "description": "Creates a conditional handle.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphCondAssignDefault", "head_type": "__host__", "description": "Assigns a default to a conditional value.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [91, 95], "model_response": [{"head": "setHandle", "head_type": "__global__", "description": "This kernel sets a conditional handle for a CUDA graph.", "relation": "calls", "tail": "cudaGraphSetConditional", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional IF node.", "relation": "calls", "tail": "cudaGraphCreate", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional IF node.", "relation": "calls", "tail": "cudaGraphConditionalHandleCreate", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional IF node.", "relation": "calls", "tail": "cudaGraphAddNode", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional IF node.", "relation": "calls", "tail": "cudaGraphInstantiate", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional IF node.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "loopKernel", "head_type": "__global__", "description": "This kernel decreases a count and sets the conditional handle.", "relation": "calls", "tail": "cudaGraphSetConditional", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional WHILE node.", "relation": "calls", "tail": "cudaGraphConditionalHandleCreate", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional WHILE node.", "relation": "calls", "tail": "cudaGraphAddNode", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional WHILE node.", "relation": "calls", "tail": "cudaGraphInstantiate", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional WHILE node.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional WHILE node.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "graphSetup", "head_type": "void", "description": "Sets up a CUDA graph with a conditional WHILE node.", "relation": "calls", "tail": "cudaGraphDestroy", "tail_type": "CUDA_API"}, {"head": "cudaEventCreate", "head_type": "CUDA_API", "description": "Creates an event for timing purposes.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventDestroy", "head_type": "CUDA_API", "description": "Destroys an event that was previously created.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventRecord", "head_type": "CUDA_API", "description": "Records an event for timing.", "relation": "calls", "tail": "cudaMemcpyAsync", "tail_type": "CUDA_API"}, {"head": "cudaEventRecord", "head_type": "CUDA_API", "description": "Records an event for timing.", "relation": "calls", "tail": "MyKernel", "tail_type": "CUDA_API"}, {"head": "cudaEventRecord", "head_type": "CUDA_API", "description": "Records an event for timing.", "relation": "calls", "tail": "cudaMemcpyAsync", "tail_type": "CUDA_API"}, {"head": "cudaEventSynchronize", "head_type": "CUDA_API", "description": "Synchronizes on an event to ensure all preceding tasks have completed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventElapsedTime", "head_type": "CUDA_API", "description": "Calculates the elapsed time between two events.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [95, 98], "model_response": [{"head": "cudaGetDeviceCount", "head_type": "__host__", "description": "Gets the number of available CUDA-capable devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Retrieves properties for a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaSetDevice", "head_type": "__host__", "description": "Sets the current device for host code.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMalloc", "head_type": "__host__", "description": "Allocates memory on the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamCreate", "head_type": "__host__", "description": "Creates a stream for managing concurrency.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event in a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventElapsedTime", "head_type": "__host__", "description": "Calculates the elapsed time between two events.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventSynchronize", "head_type": "__host__", "description": "Waits for an event to complete.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventQuery", "head_type": "__host__", "description": "Queries an event's status.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Makes a stream wait for an event to complete.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceCanAccessPeer", "head_type": "__host__", "description": "Determines whether two devices can directly access each other's memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceEnablePeerAccess", "head_type": "__host__", "description": "Enables access to the memory of another device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyPeer", "head_type": "__host__", "description": "Copies memory between two devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaPointerGetAttributes", "head_type": "__host__", "description": "Gets attributes of a pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpy", "head_type": "__host__", "description": "Copies data between host and device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaHostAlloc", "head_type": "__host__", "description": "Allocates host memory that is accessible from the device.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [98, 101], "model_response": [{"head": "cudaIpcGetMemHandle", "head_type": "__host__", "description": "Retrieves the IPC handle for a given device memory pointer.", "relation": "calls", "tail": "cudaIpcOpenMemHandle", "tail_type": "CUDA_API"}, {"head": "cudaIpcOpenMemHandle", "head_type": "__host__", "description": "Retrieves a device pointer from the IPC handle that is valid within another process.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Synchronizes the device and waits for the completion of all preceding tasks.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaPeekAtLastError", "head_type": "__host__", "description": "Returns the last error for the calling thread.", "relation": "calls", "tail": "cudaGetLastError", "tail_type": "CUDA_API"}, {"head": "cudaGetLastError", "head_type": "__host__", "description": "Returns the last error for the calling thread and resets the error variable.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetLimit", "head_type": "__host__", "description": "Queries the size of the call stack on devices of compute capability 2.x and higher.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetLimit", "head_type": "__host__", "description": "Sets the size of the call stack.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [101, 105], "model_response": [{"head": "transformKernel", "head_type": "__global__", "description": "A simple transformation kernel that applies texture transformations.", "relation": "calls", "tail": "tex2D", "tail_type": "CUDA_API"}, {"head": "cudaCreateTextureObject", "head_type": "__host__", "description": "Creates a texture object using resource and texture descriptions.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpy2DToArray", "head_type": "__host__", "description": "Copies 2D data from host memory to a CUDA array in device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMalloc", "head_type": "__host__", "description": "Allocates memory on the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDestroyTextureObject", "head_type": "__host__", "description": "Dismantles a texture object and frees associated resources.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeArray", "head_type": "__host__", "description": "Frees a CUDA array in device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees memory allocated on the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpy", "head_type": "__host__", "description": "Copies data from device memory back to host memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [106, 108], "model_response": [{"head": "surf2Dread", "head_type": "__device__", "description": "Reads from a 2D surface.", "relation": "calls", "tail": "uchar4", "tail_type": "CUDA_CLASS"}, {"head": "surf2Dwrite", "head_type": "__device__", "description": "Writes to a 2D surface.", "relation": "calls", "tail": "uchar4", "tail_type": "CUDA_CLASS"}, {"head": "cudaCreateChannelDesc", "head_type": "__host__", "description": "Creates a channel descriptor for CUDA arrays.", "relation": "calls", "tail": "cudaChannelFormatDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaMallocArray", "head_type": "__host__", "description": "Allocates an array in device memory.", "relation": "calls", "tail": "cudaArray_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaMemcpy2DToArray", "head_type": "__host__", "description": "Copies data from host memory to a CUDA array.", "relation": "calls", "tail": "cudaArray_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaCreateSurfaceObject", "head_type": "__host__", "description": "Creates a surface object from a resource descriptor.", "relation": "calls", "tail": "cudaSurfaceObject_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaMemcpy2DFromArray", "head_type": "__host__", "description": "Copies data from a CUDA array back to host memory.", "relation": "calls", "tail": "cudaArray_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaDestroySurfaceObject", "head_type": "__host__", "description": "Destroys a surface object.", "relation": "calls", "tail": "cudaSurfaceObject_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaFreeArray", "head_type": "__host__", "description": "Frees a CUDA array from device memory.", "relation": "calls", "tail": "cudaArray_t", "tail_type": "CUDA_CLASS"}, {"head": "copyKernel", "head_type": "__global__", "description": "Kernel function to copy data between surfaces.", "relation": "calls", "tail": "cudaSurfaceObject_t", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [108, 111], "model_response": [{"head": "cudaGraphicsMapResources", "head_type": "__host__", "description": "Maps buffer object for writing from CUDA.", "relation": "calls", "tail": "cudaGraphicsResourceGetMappedPointer", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsResourceGetMappedPointer", "head_type": "__host__", "description": "Gets a pointer to the mapped buffer object.", "relation": "calls", "tail": "createVertices", "tail_type": "__global__"}, {"head": "createVertices", "head_type": "__global__", "description": "Kernel that modifies a 2D grid of vertices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsUnmapResources", "head_type": "__host__", "description": "Unmaps the buffer object.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsGLRegisterBuffer", "head_type": "__host__", "description": "Registers a buffer object with CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsUnregisterResource", "head_type": "__host__", "description": "Unregisters a CUDA graphics resource.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [111, 114], "model_response": [{"head": "cudaGraphicsD3D9RegisterResource", "head_type": "__host__", "description": "Registers a Direct3D 9 resource with CUDA.", "relation": "calls", "tail": "cudaGraphicsResourceSetMapFlags", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsResourceSetMapFlags", "head_type": "__host__", "description": "Sets mapping flags for a CUDA graphics resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsMapResources", "head_type": "__host__", "description": "Maps a CUDA graphics resource for access from a kernel.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsResourceGetMappedPointer", "head_type": "__host__", "description": "Gets a pointer to the mapped CUDA graphics resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "createVertices", "head_type": "__global__", "description": "Kernel function to create vertex positions.", "relation": "calls", "tail": "make_float4", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsUnmapResources", "head_type": "__host__", "description": "Unmaps a CUDA graphics resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsUnregisterResource", "head_type": "__host__", "description": "Unregisters a CUDA graphics resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaD3D10GetDevice", "head_type": "__host__", "description": "Gets the CUDA device associated with a Direct3D 10 device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaSetDevice", "head_type": "__host__", "description": "Sets the current device for CUDA operations.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [113, 116], "model_response": [{"head": "cudaSetDevice", "head_type": "__host__", "description": "Sets the device to use for CUDA operations.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsD3D10RegisterResource", "head_type": "__host__", "description": "Registers a Direct3D10 resource with CUDA for graphics interop.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsResourceSetMapFlags", "head_type": "__host__", "description": "Sets flags for mapping a CUDA graphics resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsMapResources", "head_type": "__host__", "description": "Maps a CUDA graphics resource for access by CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsResourceGetMappedPointer", "head_type": "__host__", "description": "Gets a pointer to mapped CUDA graphics resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "createVertices", "head_type": "__global__", "description": "Kernel that creates positions for vertices.", "relation": "calls", "tail": "sinf", "tail_type": "CUDA_API"}, {"head": "createVertices", "head_type": "__global__", "description": "Kernel that creates positions for vertices.", "relation": "calls", "tail": "cosf", "tail_type": "CUDA_API"}, {"head": "cudaGraphicsUnmapResources", "head_type": "__host__", "description": "Unmaps a CUDA graphics resource after access.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphicsUnregisterResource", "head_type": "__host__", "description": "Unregisters a CUDA graphics resource.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [115, 118], "model_response": [{"head": "createVertices", "head_type": "__global__", "description": "This kernel function computes vertex positions based on UV coordinates.", "relation": "calls", "tail": "make_float4", "tail_type": "CUDA_API"}, {"head": "createVertices", "head_type": "__global__", "description": "This kernel function computes vertex positions based on UV coordinates.", "relation": "calls", "tail": "sinf", "tail_type": "CUDA_API"}, {"head": "createVertices", "head_type": "__global__", "description": "This kernel function computes vertex positions based on UV coordinates.", "relation": "calls", "tail": "cosf", "tail_type": "CUDA_API"}, {"head": "Render", "head_type": "__host__", "description": "This function maps the vertex buffer for writing from CUDA.", "relation": "calls", "tail": "cudaGraphicsMapResources", "tail_type": "CUDA_API"}, {"head": "Render", "head_type": "__host__", "description": "This function maps the vertex buffer for writing from CUDA.", "relation": "calls", "tail": "cudaGraphicsResourceGetMappedPointer", "tail_type": "CUDA_API"}, {"head": "Render", "head_type": "__host__", "description": "This function maps the vertex buffer for writing from CUDA.", "relation": "calls", "tail": "createVertices", "tail_type": "__global__"}, {"head": "Render", "head_type": "__host__", "description": "This function maps the vertex buffer for writing from CUDA.", "relation": "calls", "tail": "cudaGraphicsUnmapResources", "tail_type": "CUDA_API"}, {"head": "releaseVB", "head_type": "__host__", "description": "This function releases the vertex buffer resources.", "relation": "calls", "tail": "cudaGraphicsUnregisterResource", "tail_type": "CUDA_API"}]}
{"pdf_page": [118, 122], "model_response": [{"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports external memory to CUDA.", "relation": "calls", "tail": "cudaExternalMemoryHandleDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "__host__", "description": "Retrieves a pointer to a mapped buffer from an external memory object.", "relation": "calls", "tail": "cudaExternalMemoryBufferDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaDestroyExternalMemory", "head_type": "__host__", "description": "Destroys an external memory object.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeMipmappedArray", "head_type": "__host__", "description": "Frees a CUDA mipmapped array.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaImportExternalSemaphore", "head_type": "__host__", "description": "Imports an external semaphore to CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaSignalExternalSemaphoresAsync", "head_type": "__host__", "description": "Signals an external semaphore asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaWaitExternalSemaphoresAsync", "head_type": "__host__", "description": "Waits on an external semaphore asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDestroyExternalSemaphore", "head_type": "__host__", "description": "Destroys an external semaphore object.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceCount", "head_type": "__host__", "description": "Gets the number of CUDA-capable devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Retrieves the properties of a device.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [122, 125], "model_response": [{"head": "cudaMipmappedArray_t", "head_type": "__host__", "description": "Function to map a mipmapped array onto an external memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedMipmappedArray", "tail_type": "CUDA_API"}, {"head": "cudaChannelFormatDesc", "head_type": "__host__", "description": "Structure that describes the channel format.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaExtent", "head_type": "__host__", "description": "Structure that describes the extent of a CUDA resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "getCudaChannelFormatDescForVulkanFormat", "head_type": "__host__", "description": "Function to convert Vulkan format to CUDA channel format descriptor.", "relation": "calls", "tail": "assert", "tail_type": "CUDA_API"}, {"head": "getCudaExtentForVulkanExtent", "head_type": "__host__", "description": "Function to convert Vulkan extent to CUDA extent.", "relation": "calls", "tail": "assert", "tail_type": "CUDA_API"}, {"head": "getCudaMipmappedArrayFlagsForVulkanImage", "head_type": "__host__", "description": "Function to get mipmapped array flags for a Vulkan image.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaExternalSemaphore_t", "head_type": "__host__", "description": "Type used for external semaphores in CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "importVulkanSemaphoreObjectFromFileDescriptor", "head_type": "__host__", "description": "Function to import a Vulkan semaphore object from a file descriptor.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "cudaImportExternalSemaphore", "head_type": "__host__", "description": "Function to import an external semaphore into CUDA.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [125, 127], "model_response": [{"head": "cudaImportExternalSemaphore", "head_type": "__global__", "description": "Imports an external semaphore into CUDA.", "relation": "calls", "tail": "cudaExternalSemaphoreHandleDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaImportExternalSemaphore", "head_type": "__global__", "description": "Imports an external semaphore into CUDA.", "relation": "calls", "tail": "cudaExternalSemaphore_t", "tail_type": "CUDA_API"}, {"head": "cudaSignalExternalSemaphoresAsync", "head_type": "__global__", "description": "Signals external semaphores asynchronously in CUDA.", "relation": "calls", "tail": "cudaExternalSemaphoreSignalParams", "tail_type": "CUDA_CLASS"}, {"head": "cudaSignalExternalSemaphoresAsync", "head_type": "__global__", "description": "Signals external semaphores asynchronously in CUDA.", "relation": "calls", "tail": "cudaStream_t", "tail_type": "CUDA_API"}, {"head": "cudaWaitExternalSemaphoresAsync", "head_type": "__global__", "description": "Waits on external semaphores asynchronously in CUDA.", "relation": "calls", "tail": "cudaExternalSemaphoreWaitParams", "tail_type": "CUDA_CLASS"}, {"head": "cudaWaitExternalSemaphoresAsync", "head_type": "__global__", "description": "Waits on external semaphores asynchronously in CUDA.", "relation": "calls", "tail": "cudaStream_t", "tail_type": "CUDA_API"}]}
{"pdf_page": [127, 130], "model_response": [{"head": "cudaGetDeviceCount", "head_type": "__host__", "description": "Gets the number of CUDA devices.", "relation": "calls", "tail": "cudaDeviceCount", "tail_type": "CUDA_API"}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Retrieves properties of a CUDA device.", "relation": "calls", "tail": "cudaDeviceProp", "tail_type": "CUDA_CLASS"}, {"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports external memory into CUDA.", "relation": "calls", "tail": "cudaExternalMemoryHandleDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports external memory into CUDA.", "relation": "calls", "tail": "cudaExternalMemoryHandleDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaFree", "head_type": "__host__", "description": "Deallocates memory on the device.", "relation": "calls", "tail": "", "tail_type": ""}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "__host__", "description": "Gets a pointer to a mapped buffer from an external memory object.", "relation": "calls", "tail": "cudaExternalMemoryBufferDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaFreeMipmappedArray", "head_type": "__host__", "description": "Frees a mipmapped array previously allocated on the device.", "relation": "calls", "tail": "", "tail_type": ""}]}
{"pdf_page": [129, 133], "model_response": [{"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports external memory using provided handle and description.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "__host__", "description": "Gets mapped buffer from an external memory object.", "relation": "calls", "tail": "cudaFree", "tail_type": "CUDA_API"}, {"head": "cudaExternalMemoryGetMappedMipmappedArray", "head_type": "__host__", "description": "Gets a mapped mipmapped array from an external memory object.", "relation": "calls", "tail": "cudaFreeMipmappedArray", "tail_type": "CUDA_API"}, {"head": "getCudaChannelFormatDescForDxgiFormat", "head_type": "__host__", "description": "Returns a CUDA channel format descriptor based on a DXGI format.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "getCudaExtentForD3D12Extent", "head_type": "__host__", "description": "Returns a CUDA extent structure based on D3D12 dimensions.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "getCudaMipmappedArrayFlagsForD3D12Resource", "head_type": "__host__", "description": "Gets flags for a CUDA mipmapped array based on D3D12 resource properties.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaImportExternalSemaphore", "head_type": "__host__", "description": "Imports an external semaphore with a given description.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [133, 136], "model_response": [{"head": "cudaSignalExternalSemaphoresAsync", "head_type": "__device__", "description": "Signals an external semaphore asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaWaitExternalSemaphoresAsync", "head_type": "__device__", "description": "Waits on an external semaphore asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceCount", "head_type": "__host__", "description": "Retrieves the number of available CUDA devices.", "relation": "calls", "tail": "cudaDeviceProp", "tail_type": "CUDA_CLASS"}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Retrieves properties for a given device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports external memory into CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__device__", "description": "Frees memory previously allocated by CUDA.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [135, 139], "model_response": [{"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports external memory from a given descriptor.", "relation": "calls", "tail": "cudaExternalMemoryHandleDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "__host__", "description": "Gets a mapped buffer from external memory.", "relation": "calls", "tail": "cudaExternalMemoryBufferDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaExternalMemoryGetMappedMipmappedArray", "head_type": "__host__", "description": "Gets a mapped mipmapped array from external memory.", "relation": "calls", "tail": "cudaExternalMemoryMipmappedArrayDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees allocated device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeMipmappedArray", "head_type": "__host__", "description": "Frees allocated mipmapped array memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaImportExternalSemaphore", "head_type": "__host__", "description": "Imports an external semaphore using a given descriptor.", "relation": "calls", "tail": "cudaExternalSemaphoreHandleDesc", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [138, 142], "model_response": [{"head": "getCudaMipmappedArrayFlagsForD3D12Resource", "head_type": "__host__", "description": "Returns mipmapped array flags for a given resource.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "importD3D11FenceFromNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 fence object from an NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11FenceFromNamedNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 fence object using a named handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11KeyedMutexFromNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 keyed mutex object from an NT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11KeyedMutexFromNamedNTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 keyed mutex object using a named handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "importD3D11FenceFromKMTHandle", "head_type": "__host__", "description": "Imports a Direct3D 11 fence object from a KMT handle.", "relation": "calls", "tail": "cudaImportExternalSemaphore", "tail_type": "CUDA_API"}, {"head": "signalExternalSemaphore", "head_type": "__host__", "description": "Signals an external semaphore with a specified value.", "relation": "calls", "tail": "cudaSignalExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "waitExternalSemaphore", "head_type": "__host__", "description": "Waits on an external semaphore until its value meets or exceeds the specified value.", "relation": "calls", "tail": "cudaWaitExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "signalExternalSemaphore", "head_type": "__host__", "description": "Signals an external keyed mutex with a specified key.", "relation": "calls", "tail": "cudaSignalExternalSemaphoresAsync", "tail_type": "CUDA_API"}, {"head": "waitExternalSemaphore", "head_type": "__host__", "description": "Waits on an external keyed mutex until the specified key value is reached or timeout.", "relation": "calls", "tail": "cudaWaitExternalSemaphoresAsync", "tail_type": "CUDA_API"}]}
{"pdf_page": [142, 145], "model_response": [{"head": "cuDeviceGetUuid", "head_type": "__host__", "description": "Retrieves the UUID of a CUDA device.", "relation": "calls", "tail": "memcpy", "tail_type": "CUDA_API"}, {"head": "NvSciBufAttrListSetAttrs", "head_type": "__host__", "description": "Sets the attributes of NvSciBuf attribute list.", "relation": "calls", "tail": "sizeof", "tail_type": "CUDA_API"}, {"head": "NvSciBufAttrListCreate", "head_type": "__host__", "description": "Creates an NvSciBuf attribute list.", "relation": "calls", "tail": "sizeof", "tail_type": "CUDA_API"}, {"head": "NvSciBufAttrListReconcile", "head_type": "__host__", "description": "Reconciles and allocates the attribute list.", "relation": "calls", "tail": "sizeof", "tail_type": "CUDA_API"}, {"head": "NvSciBufObjAlloc", "head_type": "__host__", "description": "Allocates a NvSciBuf object.", "relation": "calls", "tail": "bufferObjRaw", "tail_type": "CUDA_CLASS"}, {"head": "NvSciBufObjDupWithReducePerm", "head_type": "__host__", "description": "Creates a duplicate of a NvSciBuf object with reduced permissions.", "relation": "calls", "tail": "bufferObjRo", "tail_type": "CUDA_CLASS"}, {"head": "NvSciBufAttrListGetAttrs", "head_type": "__host__", "description": "Gets attributes from a NvSciBuf attribute list.", "relation": "calls", "tail": "sizeof", "tail_type": "CUDA_API"}, {"head": "cudaImportExternalMemory", "head_type": "__host__", "description": "Imports external memory into CUDA.", "relation": "calls", "tail": "cudaExternalMemoryHandleDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaExternalMemoryGetMappedBuffer", "head_type": "__host__", "description": "Maps a device pointer onto an imported memory object.", "relation": "calls", "tail": "cudaFree", "tail_type": "CUDA_API"}]}
{"pdf_page": [145, 148], "model_response": [{"head": "cudaMipmappedArray_t", "head_type": "__host__", "description": "This function maps a mipmapped array to an external memory object.", "relation": "calls", "tail": "cudaExternalMemoryGetMappedMipmappedArray", "tail_type": "CUDA_API"}, {"head": "cudaDeviceGetNvSciSyncAttributes", "head_type": "__host__", "description": "This function retrieves the NvSciSync attributes for the specified CUDA device.", "relation": "calls", "tail": "NvSciSyncAttrListCreate", "tail_type": "CUDA_API"}, {"head": "cudaDeviceGetNvSciSyncAttributes", "head_type": "__host__", "description": "This function retrieves the NvSciSync attributes for the specified CUDA device.", "relation": "calls", "tail": "NvSciSyncAttrListCreate", "tail_type": "CUDA_API"}, {"head": "cudaImportExternalSemaphore", "head_type": "__host__", "description": "This function imports an external semaphore using the given handle.", "relation": "calls", "tail": "cudaExternalSemaphoreHandleDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaSignalExternalSemaphoresAsync", "head_type": "__host__", "description": "Signals an external semaphore asynchronously.", "relation": "calls", "tail": "cudaExternalSemaphoreSignalParams", "tail_type": "CUDA_CLASS"}, {"head": "cudaWaitExternalSemaphoresAsync", "head_type": "__host__", "description": "Waits on external semaphores asynchronously.", "relation": "calls", "tail": "cudaExternalSemaphoreWaitParams", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [157, 160], "model_response": [{"head": "cudaOccupancyMaxActiveBlocksPerMultiprocessor", "head_type": "__host__", "description": "Calculates the maximum number of active blocks per multiprocessor for a given kernel.", "relation": "calls", "tail": "MyKernel", "tail_type": "CUDA_API"}, {"head": "cudaGetDevice", "head_type": "__host__", "description": "Retrieves the device number of the current device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDeviceProperties", "head_type": "__host__", "description": "Retrieves properties for the specified device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaOccupancyMaxPotentialBlockSize", "head_type": "__host__", "description": "Calculates the maximum potential block size for a kernel.", "relation": "calls", "tail": "MyKernel", "tail_type": "CUDA_API"}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits for the device to finish all preceding requested tasks.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaOccupancyMaxPotentialClusterSize", "head_type": "__host__", "description": "Calculates the maximum cluster size for a given kernel.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}, {"head": "cudaOccupancyMaxActiveClusters", "head_type": "__host__", "description": "Calculates the max active clusters of a given size.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}]}
{"pdf_page": [185, 188], "model_response": [{"head": "__threadfence", "head_type": "__device__", "description": "__threadfence ensures that no writes to all memory made by the calling thread after the call are observed by any thread as occurring before any write to all memory made by the calling thread before the call.", "relation": "calls", "tail": "atomicInc", "tail_type": "CUDA_API"}, {"head": "__threadfence_block", "head_type": "__device__", "description": "__threadfence_block ensures all writes and reads to memory made by the calling thread are ordered before and after the call.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "atomicInc", "head_type": "__device__", "description": "atomicInc atomically increments the counter.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads", "head_type": "__device__", "description": "__syncthreads synchronizes all threads within a block.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [188, 189], "model_response": [{"head": "__syncthreads", "head_type": "__global__", "description": "__syncthreads() coordinates communication between the threads of the same block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads_count", "head_type": "__global__", "description": "__syncthreads_count() is identical to __syncthreads() and counts threads where predicate is non-zero.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads_and", "head_type": "__global__", "description": "__syncthreads_and() is identical to __syncthreads() and returns non-zero if predicate is non-zero for all threads.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncthreads_or", "head_type": "__global__", "description": "__syncthreads_or() is identical to __syncthreads() and returns non-zero if predicate is non-zero for any thread.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__syncwarp", "head_type": "__global__", "description": "__syncwarp() causes executing threads to wait until all indicated warp lanes have executed a __syncwarp() with the same mask.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [201, 206], "model_response": [{"head": "mykernel", "head_type": "__global__", "description": "Kernel function to update an integer value atomically.", "relation": "calls", "tail": "atomicAdd_system", "tail_type": "CUDA_API"}, {"head": "mykernel", "head_type": "__global__", "description": "Kernel function to update an integer value atomically.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "atomicAdd", "head_type": "__device__", "description": "Reads and adds a value atomically.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "atomicCAS", "head_type": "__device__", "description": "Compares and swaps a value atomically.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "atomicExch", "head_type": "__device__", "description": "Exchanges a value atomically.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "atomicMin", "head_type": "__device__", "description": "Calculates the minimum of a value atomically.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "atomicMax", "head_type": "__device__", "description": "Calculates the maximum of a value atomically.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "atomicAnd", "head_type": "__device__", "description": "Performs a bitwise AND operation atomically.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [207, 210], "model_response": [{"head": "__isShared", "head_type": "__device__", "description": "__isShared returns 1 if pointer points to an object in shared memory space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__isConstant", "head_type": "__device__", "description": "__isConstant returns 1 if pointer points to an object in constant memory space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__isGridConstant", "head_type": "__device__", "description": "__isGridConstant returns 1 if pointer points to a kernel parameter annotated with __grid_constant__.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__isLocal", "head_type": "__device__", "description": "__isLocal returns 1 if pointer points to an object in local memory space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_global", "head_type": "__device__", "description": "__cvta_generic_to_global returns the result of executing the PTX cvta.to.global instruction on the provided pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_shared", "head_type": "__device__", "description": "__cvta_generic_to_shared returns the result of executing the PTX cvta.to.shared instruction on the provided pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_constant", "head_type": "__device__", "description": "__cvta_generic_to_constant returns the result of executing the PTX cvta.to.const instruction on the provided pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_generic_to_local", "head_type": "__device__", "description": "__cvta_generic_to_local returns the result of executing the PTX cvta.to.local instruction on the provided pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_global_to_generic", "head_type": "__device__", "description": "__cvta_global_to_generic returns the generic pointer obtained by executing the PTX cvta.global instruction on the provided raw bits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_shared_to_generic", "head_type": "__device__", "description": "__cvta_shared_to_generic returns the generic pointer obtained by executing the PTX cvta.shared instruction on the provided raw bits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_constant_to_generic", "head_type": "__device__", "description": "__cvta_constant_to_generic returns the generic pointer obtained by executing the PTX cvta.const instruction on the provided raw bits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_local_to_generic", "head_type": "__device__", "description": "__cvta_local_to_generic returns the generic pointer obtained by executing the PTX cvta.local instruction on the provided raw bits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "alloca", "head_type": "__host__ __device__", "description": "alloca allocates size bytes of memory in the stack frame of the caller.", "relation": "calls", "tail": "ptr", "tail_type": "CUDA_API"}]}
{"pdf_page": [209, 213], "model_response": [{"head": "__cvta_constant_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTXcvta.const instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__cvta_local_to_generic", "head_type": "__device__", "description": "Returns the generic pointer obtained by executing the PTXcvta.local instruction on the value provided by rawbits.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "alloca", "head_type": "__host__ __device__", "description": "Allocates size bytes of memory in the stack frame of the caller.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_assume_aligned", "head_type": "CUDA_API", "description": "Allows the compiler to assume that the argument pointer is aligned to at least align bytes, and returns the argument pointer.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_assume", "head_type": "CUDA_API", "description": "Allows the compiler to assume that the Boolean argument is true.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__assume", "head_type": "CUDA_API", "description": "Allows the compiler to assume that the Boolean argument is true.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_expect", "head_type": "CUDA_API", "description": "Indicates to the compiler that it is expected that exp == c.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__builtin_unreachable", "head_type": "CUDA_API", "description": "Indicates to the compiler that control flow never reaches the point where this function is being called from.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__all_sync", "head_type": "CUDA_API", "description": "Evaluate predicate for all non-exited threads in mask and return non-zero if and only if predicate evaluates to non-zero for all of them.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__any_sync", "head_type": "CUDA_API", "description": "Evaluate predicate for all non-exited threads in mask and return non-zero if and only if predicate evaluates to non-zero for any of them.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__ballot_sync", "head_type": "CUDA_API", "description": "Evaluate predicate for all non-exited threads in mask and return an integer whose Nth bit is set if and only if predicate evaluates to non-zero for the Nth thread of the warp.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__activemask", "head_type": "CUDA_API", "description": "Returns a 32-bit integer mask of all currently active threads in the calling warp.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [212, 215], "model_response": [{"head": "__all_sync", "head_type": "__device__", "description": "__all_sync evaluates a predicate for all non-exited threads in a specified mask.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__any_sync", "head_type": "__device__", "description": "__any_sync evaluates a predicate for all non-exited threads and checks if any are non-zero.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__ballot_sync", "head_type": "__device__", "description": "__ballot_sync returns an integer showing which threads in a warp have a non-zero predicate.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__activemask", "head_type": "__device__", "description": "__activemask returns a mask of all currently active threads in the calling warp.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__match_any_sync", "head_type": "__device__", "description": "__match_any_sync performs a broadcast-and-compare operation to check values across threads.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__match_all_sync", "head_type": "__device__", "description": "__match_all_sync checks if all threads have the same value and sets a predicate accordingly.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_add_sync", "head_type": "__device__", "description": "__reduce_add_sync applies an arithmetic add reduction operation on values provided by threads.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_min_sync", "head_type": "__device__", "description": "__reduce_min_sync applies a minimum reduction operation on values provided by threads.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_max_sync", "head_type": "__device__", "description": "__reduce_max_sync applies a maximum reduction operation on values provided by threads.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_and_sync", "head_type": "__device__", "description": "__reduce_and_sync applies a logical AND reduction operation on values provided by threads.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_or_sync", "head_type": "__device__", "description": "__reduce_or_sync applies a logical OR reduction operation on values provided by threads.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__reduce_xor_sync", "head_type": "__device__", "description": "__reduce_xor_sync applies a logical XOR reduction operation on values provided by threads.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [216, 218], "model_response": [{"head": "__shfl_sync", "head_type": "__device__", "description": "__shfl_sync synchronizes threads in a warp and retrieves a value from a specified lane.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "__shfl_up_sync", "head_type": "__device__", "description": "__shfl_up_sync shifts a value up the warp by a specified delta.", "relation": "calls", "tail": "__shfl_sync", "tail_type": "CUDA_API"}, {"head": "__shfl_xor_sync", "head_type": "__device__", "description": "__shfl_xor_sync performs a bitwise XOR operation with the specified lane ID.", "relation": "calls", "tail": "__shfl_sync", "tail_type": "CUDA_API"}, {"head": "__global__", "head_type": "__global__", "description": "The main kernel function to perform reduction across a warp.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__global__", "head_type": "__global__", "description": "The main kernel function to broadcast a single value across a warp.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__global__", "head_type": "__global__", "description": "The main kernel function to perform an inclusive plus-scan across sub-partitions.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [227, 230], "model_response": [{"head": "__global__", "head_type": "__global__", "description": "__global__ defines a kernel that can be called from host code.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "block.sync()", "head_type": "__device__", "description": "block.sync() synchronizes all threads in a block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::barrier", "head_type": "__device__", "description": "cuda::barrier is used to synchronize threads using a barrier.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "init()", "head_type": "__device__", "description": "init initializes the barrier with the expected arrival count.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "bar.arrive()", "head_type": "__device__", "description": "bar.arrive() marks this thread's arrival at the barrier.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "bar.wait()", "head_type": "__device__", "description": "bar.wait() blocks the calling thread until all participating threads have called arrive.", "relation": "calls", "tail": "cuda::barrier", "tail_type": "CUDA_CLASS"}, {"head": "__shared__", "head_type": "__device__", "description": "__shared__ declares a shared variable that can be accessed by threads in a block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cooperative_groups::this_thread_block()", "head_type": "__device__", "description": "This function returns a reference to the current thread block for cooperative groups.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [230, 232], "model_response": [{"head": "arrive_and_wait", "head_type": "__device__", "description": "This function waits for the barrier's counter to be non-zero and then signals arrival.", "relation": "calls", "tail": "arrive", "tail_type": "CUDA_API"}, {"head": "arrive", "head_type": "__device__", "description": "This function signals that a thread has arrived at the barrier.", "relation": "calls", "tail": "wait", "tail_type": "CUDA_API"}, {"head": "arrive", "head_type": "__device__", "description": "This function signals that a thread has arrived at the barrier.", "relation": "calls", "tail": "arrive", "tail_type": "CUDA_API"}, {"head": "producer", "head_type": "__device__", "description": "The function that produces data and fills the buffer.", "relation": "calls", "tail": "arrive_and_wait", "tail_type": "CUDA_API"}, {"head": "consumer", "head_type": "__device__", "description": "The function that consumes data from the filled buffer.", "relation": "calls", "tail": "arrive_and_wait", "tail_type": "CUDA_API"}, {"head": "producer_consumer_pattern", "head_type": "__global__", "description": "The main function coordinating producer and consumer threads.", "relation": "calls", "tail": "producer", "tail_type": "CUDA_API"}, {"head": "producer_consumer_pattern", "head_type": "__global__", "description": "The main function coordinating producer and consumer threads.", "relation": "calls", "tail": "consumer", "tail_type": "CUDA_API"}]}
{"pdf_page": [233, 235], "model_response": [{"head": "psum", "head_type": "__global__", "description": "This function computes a prefix sum using shared memory and synchronization.", "relation": "calls", "tail": "block.sync", "tail_type": "CUDA_API"}, {"head": "psum", "head_type": "__global__", "description": "This function computes a prefix sum using shared memory and synchronization.", "relation": "calls", "tail": "bar->arrive", "tail_type": "CUDA_API"}, {"head": "psum", "head_type": "__global__", "description": "This function computes a prefix sum using shared memory and synchronization.", "relation": "calls", "tail": "bar->wait", "tail_type": "CUDA_API"}, {"head": "psum", "head_type": "__global__", "description": "This function computes a prefix sum using shared memory and synchronization.", "relation": "calls", "tail": "new (bar) barrier_t", "tail_type": "CUDA_CLASS"}, {"head": "__mbarrier_init", "head_type": "__host__", "description": "This function initializes a memory barrier with an expected arrival count.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__mbarrier_arrive", "head_type": "__host__", "description": "This function decrements the pending count of a barrier.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__mbarrier_test_wait", "head_type": "__host__", "description": "This function tests whether a token is associated with the preceding phase of a barrier.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__mbarrier_pending_count", "head_type": "__host__", "description": "This function returns the pending count associated with a barrier token.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [236, 239], "model_response": [{"head": "compute", "head_type": "__device__", "description": "Computes using all values of current batch from shared memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "without_memcpy_async", "head_type": "__global__", "description": "Copies data from global memory to shared memory without using memcpy_async.", "relation": "calls", "tail": "compute", "tail_type": "CUDA_API"}, {"head": "with_memcpy_async", "head_type": "__global__", "description": "Copies data from global memory to shared memory using memcpy_async.", "relation": "calls", "tail": "compute", "tail_type": "CUDA_API"}, {"head": "with_barrier", "head_type": "__global__", "description": "Copies data using memcpy_async and synchronizes with barrier.", "relation": "calls", "tail": "compute", "tail_type": "CUDA_API"}]}
{"pdf_page": [241, 243], "model_response": [{"head": "__global__ void with_single_stage(int* global_out, int const* global_in, size_t size, size_t batch_sz)", "head_type": "__global__", "description": "__global__ function that performs asynchronous data copies using cuda::pipeline.", "relation": "calls", "tail": "pipeline.producer_acquire()", "tail_type": "CUDA_API"}, {"head": "pipeline.producer_acquire()", "head_type": "CUDA_API", "description": "Acquires the pipeline head stage from all producer threads.", "relation": "calls", "tail": "cuda::memcpy_async(block, shared, global_in + global_idx, sizeof(int)*block.size(), pipeline)", "tail_type": "CUDA_API"}, {"head": "cuda::memcpy_async(block, shared, global_in + global_idx, sizeof(int)*block.size(), pipeline)", "head_type": "CUDA_API", "description": "Submits async copies to the pipeline's head stage.", "relation": "calls", "tail": "pipeline.producer_commit()", "tail_type": "CUDA_API"}, {"head": "pipeline.producer_commit()", "head_type": "CUDA_API", "description": "Advances the pipeline's head stage.", "relation": "calls", "tail": "pipeline.consumer_wait()", "tail_type": "CUDA_API"}, {"head": "pipeline.consumer_wait()", "head_type": "CUDA_API", "description": "Waits for the operations committed to the previous compute stage to complete.", "relation": "calls", "tail": "compute(global_out + global_idx, shared)", "tail_type": "CUDA_API"}, {"head": "compute(global_out + global_idx, shared)", "head_type": "CUDA_API", "description": "Performs computation overlapped with the memcpy_async of the 'copy' stage.", "relation": "calls", "tail": "pipeline.consumer_release()", "tail_type": "CUDA_API"}, {"head": "pipeline.consumer_release()", "head_type": "CUDA_API", "description": "Releases the stage resources.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [243, 246], "model_response": [{"head": "with_staging", "head_type": "__global__", "description": "This kernel function manages asynchronous memory transfers and computation using a pipeline.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "with_staging", "head_type": "__global__", "description": "This kernel function manages asynchronous memory transfers and computation using a pipeline.", "relation": "calls", "tail": "compute", "tail_type": "__device__"}, {"head": "compute", "head_type": "__device__", "description": "This device function processes the data fetched into shared memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "with_staging_unified", "head_type": "__global__", "description": "This kernel function uses a unified approach to manage memory transfers and computation.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "with_staging_unified", "head_type": "__global__", "description": "This kernel function uses a unified approach to manage memory transfers and computation.", "relation": "calls", "tail": "compute", "tail_type": "__device__"}, {"head": "compute", "head_type": "__device__", "description": "This device function processes the data fetched into shared memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [246, 249], "model_response": [{"head": "__global__ void with_specialized_staging_unified", "head_type": "__global__", "description": "This kernel uses specialized staging in a unified way to manage producer and consumer roles.", "relation": "calls", "tail": "cuda::make_pipeline", "tail_type": "CUDA_API"}, {"head": "__global__ void with_specialized_staging_unified", "head_type": "__global__", "description": "This kernel uses specialized staging in a unified way to manage producer and consumer roles.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "__global__ void with_staging_scope_thread", "head_type": "__global__", "description": "This kernel demonstrates the use of staging with scoped threads.", "relation": "calls", "tail": "cuda::make_pipeline", "tail_type": "CUDA_API"}, {"head": "__global__ void with_staging_scope_thread", "head_type": "__global__", "description": "This kernel demonstrates the use of staging with scoped threads.", "relation": "calls", "tail": "cuda::memcpy_async", "tail_type": "CUDA_API"}, {"head": "__device__ void compute", "head_type": "__device__", "description": "The compute function processes data from shared memory into global memory.", "relation": "calls", "tail": "cuda::pipeline_role", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [254, 259], "model_response": [{"head": "cp_async_bulk_global_to_shared", "head_type": "__device__", "description": "Initiates a bulk-asynchronous copy from global to shared memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "fence_proxy_async_shared_cta", "head_type": "__device__", "description": "Waits for thread\u2019s shared memory writes to become visible to the async proxy.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cp_async_bulk_shared_to_global", "head_type": "__device__", "description": "Initiates a bulk-asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cp_async_bulk_commit_group", "head_type": "__device__", "description": "Combines all preceding bulk-asynchronous operations into a bulk async-group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cp_async_bulk_wait_group_read", "head_type": "__device__", "description": "Waits for operations in a bulk async-group to have completed with read modifier.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDriverEntryPoint", "head_type": "__host__", "description": "Gets the entry point for the CUDA driver API.", "relation": "calls", "tail": "cuGetProcAddress", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "CUDA_API", "description": "Gets the address of a CUDA function in the driver.", "relation": "calls", "tail": "cuTensorMapEncodeTiled", "tail_type": "CUDA_API"}, {"head": "cuTensorMapEncodeTiled", "head_type": "CUDA_API", "description": "Creates a tensor map using specified parameters.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemcpyToSymbol", "head_type": "__host__", "description": "Copies data to a symbol in constant memory.", "relation": "calls", "tail": "global_tensor_map", "tail_type": "CUDA_CLASS"}, {"head": "cudaMemcpy", "head_type": "__host__", "description": "Copies data from host to device or device to host.", "relation": "calls", "tail": "global_tensor_map", "tail_type": "CUDA_CLASS"}, {"head": "cp_async_bulk_tensor_2d_global_to_shared", "head_type": "__device__", "description": "Initiates bulk tensor copy from global to shared.", "relation": "calls", "tail": "fence_proxy_async_shared_cta", "tail_type": "CUDA_API"}, {"head": "cp_async_bulk_tensor_2d_shared_to_global", "head_type": "__device__", "description": "Initiates TMA transfer to copy shared memory to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "barrier_arrive_tx", "head_type": "CUDA_API", "description": "Handles arrival at a barrier.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [259, 261], "model_response": [{"head": "cuda::device::experimental::cp_async_bulk_tensor_1d_global_to_shared", "head_type": "__device__", "description": "This function initiates a 1D bulk tensor asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier<cuda::thread_scope_block>", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_2d_global_to_shared", "head_type": "__device__", "description": "This function initiates a 2D bulk tensor asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier<cuda::thread_scope_block>", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_3d_global_to_shared", "head_type": "__device__", "description": "This function initiates a 3D bulk tensor asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier<cuda::thread_scope_block>", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_4d_global_to_shared", "head_type": "__device__", "description": "This function initiates a 4D bulk tensor asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier<cuda::thread_scope_block>", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_5d_global_to_shared", "head_type": "__device__", "description": "This function initiates a 5D bulk tensor asynchronous copy from global to shared memory.", "relation": "calls", "tail": "cuda::barrier<cuda::thread_scope_block>", "tail_type": "CUDA_CLASS"}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_1d_shared_to_global", "head_type": "__device__", "description": "This function initiates a 1D bulk tensor asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_2d_shared_to_global", "head_type": "__device__", "description": "This function initiates a 2D bulk tensor asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_3d_shared_to_global", "head_type": "__device__", "description": "This function initiates a 3D bulk tensor asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_4d_shared_to_global", "head_type": "__device__", "description": "This function initiates a 4D bulk tensor asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuda::device::experimental::cp_async_bulk_tensor_5d_shared_to_global", "head_type": "__device__", "description": "This function initiates a 5D bulk tensor asynchronous copy from shared to global memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [264, 267], "model_response": [{"head": "helloCUDA", "head_type": "__global__", "description": "A kernel function that prints a message to each thread.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "helloCUDA", "head_type": "__global__", "description": "A kernel function that prints a message to each thread.", "relation": "calls", "tail": "threadIdx.x", "tail_type": "CUDA_CLASS"}, {"head": "helloCUDA", "head_type": "__global__", "description": "A kernel function that prints a message to each thread.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "malloc", "head_type": "__host__ __device__", "description": "Allocates memory dynamically from a fixed-size heap in global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__nv_aligned_device_malloc", "head_type": "__device__", "description": "Allocates memory dynamically from a fixed-size heap in global memory with alignment.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "free", "head_type": "__host__ __device__", "description": "Deallocates the memory pointed to by ptr.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "memcpy", "head_type": "__host__ __device__", "description": "Copies size bytes from the memory location pointed by src to the memory location pointed by dest.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "memset", "head_type": "__host__ __device__", "description": "Sets size bytes of memory block pointed by ptr to value.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetLimit", "head_type": "__host__", "description": "Gets the size of the buffer used to transfer the printf arguments and internal metadata to the host.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetLimit", "head_type": "__host__", "description": "Sets the size of the buffer used to transfer the printf arguments and internal metadata to the host.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [267, 270], "model_response": [{"head": "mallocTest", "head_type": "__global__", "description": "This kernel tests memory allocation using malloc.", "relation": "calls", "tail": "malloc", "tail_type": "CUDA_API"}, {"head": "mallocTest", "head_type": "__global__", "description": "This kernel tests memory allocation using malloc.", "relation": "calls", "tail": "memset", "tail_type": "CUDA_API"}, {"head": "mallocTest", "head_type": "__global__", "description": "This kernel tests memory allocation using malloc.", "relation": "calls", "tail": "free", "tail_type": "CUDA_API"}, {"head": "mallocTest", "head_type": "__global__", "description": "This kernel tests memory allocation using malloc.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "mallocTest", "head_type": "__global__", "description": "This kernel tests memory allocation using malloc.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "allocmem", "head_type": "__global__", "description": "This kernel allocates memory that persists between kernel launches.", "relation": "calls", "tail": "malloc", "tail_type": "CUDA_API"}, {"head": "allocmem", "head_type": "__global__", "description": "This kernel allocates memory that persists between kernel launches.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "usemem", "head_type": "__global__", "description": "This kernel uses the allocated memory to add thread IDs.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "freemem", "head_type": "__global__", "description": "This kernel frees the previously allocated memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [270, 274], "model_response": [{"head": "Func", "head_type": "__global__", "description": "This function is a global CUDA kernel that takes a float pointer as a parameter.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchKernelEx", "head_type": "__global__", "description": "This API function launches a kernel with a runtime configuration.", "relation": "calls", "tail": "Func", "tail_type": "CUDA_API"}, {"head": "cudaLaunchConfig_t", "head_type": "__global__", "description": "This structure defines the configuration for launching a CUDA kernel.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__launch_bounds__", "head_type": "__global__", "description": "This qualifier specifies launch bounds for a global CUDA function.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "MyKernel", "head_type": "__global__", "description": "This function is a global CUDA kernel that may have launch bounds applied.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "__maxnreg__", "head_type": "__global__", "description": "This qualifier specifies the maximum number of registers allocated to a single thread.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [281, 284], "model_response": [{"head": "__global__ void kernel( int *globalInput)", "head_type": "__global__", "description": "__global__ kernel function which loads an integer from global memory into shared memory.", "relation": "calls", "tail": "thread_block g=this_thread_block()", "tail_type": "CUDA_CLASS"}, {"head": "g.sync()", "head_type": "__device__", "description": "Synchronizes the threads in the thread block.", "relation": "calls", "tail": "g.barrier_arrive()", "tail_type": "CUDA_API"}, {"head": "g.barrier_arrive()", "head_type": "__device__", "description": "Arrives on the thread block barrier.", "relation": "calls", "tail": "g.barrier_wait(t)", "tail_type": "CUDA_API"}, {"head": "g.barrier_wait(t)", "head_type": "__device__", "description": "Waits on the thread block barrier.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "thread_block::arrival_token barrier_arrive()", "head_type": "__device__", "description": "Returns a token that is needed to be passed into barrier_wait().", "relation": "single", "tail": "", "tail_type": ""}, {"head": "static unsigned int thread_rank()", "head_type": "__device__", "description": "Returns the rank of the calling thread.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "static dim3 group_index()", "head_type": "__device__", "description": "Returns the 3-Dimensional index of the block within the launched grid.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "static dim3 thread_index()", "head_type": "__device__", "description": "Returns the 3-Dimensional index of the thread within the launched block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "static dim3 dim_threads()", "head_type": "__device__", "description": "Returns dimensions of the launched block in units of threads.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "static unsigned int num_threads()", "head_type": "__device__", "description": "Returns total number of threads in the group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "static unsigned int size()", "head_type": "__device__", "description": "Returns total number of threads in the group (alias of num_threads()).", "relation": "single", "tail": "", "tail_type": ""}, {"head": "static dim3 group_dim()", "head_type": "__device__", "description": "Returns dimensions of the launched block (alias of dim_threads()).", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [284, 287], "model_response": [{"head": "multi_grid_group", "head_type": "CUDA_CLASS", "description": "Represents all the threads launched across all devices of a multi-device cooperative launch.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "thread_block_tile", "head_type": "CUDA_CLASS", "description": "A templated version of a tiled group for optimal execution.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "tiled_partition", "head_type": "CUDA_API", "description": "Creates a tiled partition of a group.", "relation": "calls", "tail": "thread_block_tile", "tail_type": "CUDA_CLASS"}, {"head": "this_thread_block", "head_type": "CUDA_API", "description": "Obtains the current thread block group.", "relation": "calls", "tail": "thread_block", "tail_type": "CUDA_CLASS"}, {"head": "sync", "head_type": "CUDA_API", "description": "Synchronizes the threads named in the group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "tiled_partition", "head_type": "CUDA_API", "description": "Creates a tiled partition of a group.", "relation": "calls", "tail": "thread_block_tile", "tail_type": "CUDA_CLASS"}, {"head": "block_tile_memory", "head_type": "CUDA_CLASS", "description": "Struct template that has to reside in either shared or global memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [287, 290], "model_response": [{"head": "memcpy_async", "head_type": "__global__", "description": "Asynchronously copy memory from source to destination.", "relation": "calls", "tail": "this_thread", "tail_type": "CUDA_API"}, {"head": "this_thread", "head_type": "CUDA_CLASS", "description": "Represents the current thread in a cooperative group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "coalesced_threads", "head_type": "CUDA_API", "description": "Creates a group containing all active threads at that point in time.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "sync", "head_type": "CUDA_CLASS", "description": "Synchronizes the threads in the coalesced group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "num_threads", "head_type": "CUDA_CLASS", "description": "Returns total number of threads in the coalesced group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "thread_rank", "head_type": "CUDA_CLASS", "description": "Returns the rank of the calling thread within the group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "tiled_partition", "head_type": "CUDA_API", "description": "Partitions a parent group into subgroups.", "relation": "calls", "tail": "this_thread_block", "tail_type": "CUDA_API"}, {"head": "tiled_partition", "head_type": "CUDA_API", "description": "Partitions a parent group into subgroups.", "relation": "calls", "tail": "tiled_partition", "tail_type": "CUDA_API"}]}
{"pdf_page": [290, 293], "model_response": [{"head": "labeled_partition", "head_type": "__device__", "description": "Divides the parent group into one-dimensional subgroups based on label.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "binary_partition", "head_type": "__device__", "description": "Divides the parent group into one-dimensional subgroups based on a predicate.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "oddEven", "head_type": "__global__", "description": "Kernel function that partitions a tile into odd and even groups.", "relation": "calls", "tail": "binary_partition", "tail_type": "CUDA_API"}, {"head": "init_shared_data", "head_type": "__device__", "description": "Initializes shared data within a thread block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "local_processing", "head_type": "__device__", "description": "Performs local processing in a thread block.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "process_shared_data", "head_type": "__device__", "description": "Processes data from shared memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cluster_kernel", "head_type": "__global__", "description": "Kernel function that initializes shared state and synchronizes the cluster.", "relation": "calls", "tail": "init_shared_data", "tail_type": "CUDA_API"}, {"head": "cluster_kernel", "head_type": "__global__", "description": "Kernel function that initializes shared state and synchronizes the cluster.", "relation": "calls", "tail": "barrier_arrive", "tail_type": "CUDA_API"}, {"head": "cluster_kernel", "head_type": "__global__", "description": "Kernel function that initializes shared state and synchronizes the cluster.", "relation": "calls", "tail": "local_processing", "tail_type": "CUDA_API"}, {"head": "cluster_kernel", "head_type": "__global__", "description": "Kernel function that initializes shared state and synchronizes the cluster.", "relation": "calls", "tail": "barrier_wait", "tail_type": "CUDA_API"}, {"head": "cluster_kernel", "head_type": "__global__", "description": "Kernel function that initializes shared state and synchronizes the cluster.", "relation": "calls", "tail": "process_shared_data", "tail_type": "CUDA_API"}]}
{"pdf_page": [293, 296], "model_response": [{"head": "memcpy_async", "head_type": "__global__", "description": "Performs a copy of shape bytes.", "relation": "calls", "tail": "wait", "tail_type": "CUDA_API"}, {"head": "memcpy_async", "head_type": "__global__", "description": "Performs a copy of min(dstLayout, srcLayout) elements.", "relation": "calls", "tail": "wait", "tail_type": "CUDA_API"}, {"head": "wait", "head_type": "__device__", "description": "Blocks calling threads until all previous copies are done.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "wait_prior", "head_type": "__device__", "description": "Waits for previous requests while the latest NumStages might still be in progress.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [296, 299], "model_response": [{"head": "std_dev", "head_type": "__device__", "description": "Calculates the approximate standard deviation of integers in a vector.", "relation": "calls", "tail": "cg::reduce", "tail_type": "CUDA_API"}, {"head": "std_dev", "head_type": "__device__", "description": "Calculates the approximate standard deviation of integers in a vector.", "relation": "calls", "tail": "cg::plus", "tail_type": "CUDA_CLASS"}, {"head": "std_dev", "head_type": "__device__", "description": "Calculates the approximate standard deviation of integers in a vector.", "relation": "calls", "tail": "sqrtf", "tail_type": "CUDA_API"}, {"head": "block_reduce", "head_type": "__device__", "description": "Performs block-wide reduction of an array and stores the result in an atomic variable.", "relation": "calls", "tail": "cg::reduce_update_async", "tail_type": "CUDA_API"}, {"head": "block_reduce", "head_type": "__device__", "description": "Performs block-wide reduction of an array and stores the result in an atomic variable.", "relation": "calls", "tail": "cg::plus", "tail_type": "CUDA_CLASS"}, {"head": "cg::plus<int>", "head_type": "CUDA_CLASS", "description": "A function object that adds two integers.", "relation": "calls", "tail": "__reduce_add_sync", "tail_type": "CUDA_API"}, {"head": "std_dev", "head_type": "__device__", "description": "Calculates the approximate standard deviation of integers in a vector.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [299, 303], "model_response": [{"head": "inclusive_scan", "head_type": "__device__", "description": "inclusive_scan performs a scan operation over the data being passed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "exclusive_scan", "head_type": "__device__", "description": "exclusive_scan performs a scan operation excluding the calling thread's data.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "inclusive_scan_update", "head_type": "__device__", "description": "inclusive_scan_update is an update variant that modifies an atomic variable.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "exclusive_scan_update", "head_type": "__device__", "description": "exclusive_scan_update is an update variant that modifies an atomic variable, excluding the calling thread's data.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "kernel", "head_type": "__global__", "description": "kernel executes the inclusive_scan operation on each thread of the tile.", "relation": "calls", "tail": "cg::inclusive_scan", "tail_type": "CUDA_API"}, {"head": "stream_compaction", "head_type": "__device__", "description": "stream_compaction filters input data based on a predicate and uses exclusive_scan for indexing.", "relation": "calls", "tail": "cg::exclusive_scan", "tail_type": "CUDA_API"}, {"head": "kernel", "head_type": "__global__", "description": "kernel performs dynamic buffer space allocation using exclusive_scan_update.", "relation": "calls", "tail": "cg::exclusive_scan_update", "tail_type": "CUDA_API"}, {"head": "calculate_buffer_space_needed", "head_type": "__device__", "description": "calculates buffer space needed for each thread.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "my_thread_data", "head_type": "__device__", "description": "returns the thread-specific data for the given index.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "invoke_one", "head_type": "__device__", "description": "invoke_one calls the provided function on a selected thread from the group.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "invoke_one_broadcast", "head_type": "__device__", "description": "invoke_one_broadcast calls the provided function on a selected thread and broadcasts the result.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [303, 307], "model_response": [{"head": "atomicAddOneRelaxed", "head_type": "__device__", "description": "Implements a relaxed atomic addition operation with a specific thread scope.", "relation": "calls", "tail": "invoke_one_broadcast", "tail_type": "CUDA_API"}, {"head": "atomicAddOneRelaxed", "head_type": "__device__", "description": "Implements a relaxed atomic addition operation with a specific thread scope.", "relation": "calls", "tail": "fetch_add", "tail_type": "CUDA_API"}, {"head": "atomicAddOneRelaxed", "head_type": "__device__", "description": "Implements a relaxed atomic addition operation with a specific thread scope.", "relation": "calls", "tail": "coalesced_threads", "tail_type": "CUDA_API"}, {"head": "cudaLaunchCooperativeKernel", "head_type": "CUDA_API", "description": "Launches a cooperative kernel across multiple devices.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaOccupancyMaxActiveBlocksPerMultiprocessor", "head_type": "CUDA_API", "description": "Calculates the maximum number of active blocks per multiprocessor for the specified kernel and configuration.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetAttribute", "head_type": "CUDA_API", "description": "Queries the specified attribute for a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchCooperativeKernelMultiDevice", "head_type": "CUDA_API", "description": "Launches a kernel across multiple devices in a cooperative manner.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamCreate", "head_type": "CUDA_API", "description": "Creates a new stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaSetDevice", "head_type": "CUDA_API", "description": "Sets the device that is to be used for subsequent CUDA API calls.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "this_grid", "head_type": "__device__", "description": "Obtains the current grid group.", "relation": "calls", "tail": "grid_group", "tail_type": "CUDA_CLASS"}, {"head": "this_multi_grid", "head_type": "__device__", "description": "Obtains the current multi-grid group.", "relation": "calls", "tail": "multi_grid_group", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [319, 322], "model_response": [{"head": "permute", "head_type": "__global__", "description": "This function implements a kernel that performs permutations on the input data.", "relation": "calls", "tail": "permute_data", "tail_type": "CUDA_API"}, {"head": "permute", "head_type": "__global__", "description": "This function implements a kernel that performs permutations on the input data.", "relation": "calls", "tail": "permute", "tail_type": "CUDA_API"}, {"head": "host_launch", "head_type": "__host__", "description": "This function launches the permute kernel from the host side.", "relation": "calls", "tail": "permute", "tail_type": "CUDA_API"}, {"head": "cudaGetLastError", "head_type": "CUDA_API", "description": "Retrieves the last error code from the CUDA runtime.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetParameterBuffer", "head_type": "CUDA_API", "description": "Generates parameter buffer for kernel launch.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchDevice", "head_type": "CUDA_API", "description": "Launches a device-side kernel with parameters.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [323, 327], "model_response": [{"head": "cudaLaunchDevice", "head_type": "__device__", "description": "Launches the specified kernel with the parameter buffer.", "relation": "calls", "tail": "cudaGetParameterBuffer", "tail_type": "CUDA_API"}, {"head": "cudaGetParameterBuffer", "head_type": "__device__", "description": "Gets a buffer for kernel parameters.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "childKernel", "head_type": "__global__", "description": "A child kernel that prints 'Hello'.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "tailKernel", "head_type": "__global__", "description": "A tail kernel that prints 'World!'.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "parentKernel", "head_type": "__global__", "description": "A parent kernel that launches childKernel and tailKernel.", "relation": "calls", "tail": "childKernel", "tail_type": "CUDA_API"}, {"head": "parentKernel", "head_type": "__global__", "description": "A parent kernel that launches childKernel and tailKernel.", "relation": "calls", "tail": "tailKernel", "tail_type": "CUDA_API"}]}
{"pdf_page": [334, 337], "model_response": [{"head": "child_launch", "head_type": "__global__", "description": "This function increments a value in global memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "parent_launch", "head_type": "__global__", "description": "This function initializes global memory and launches the child grid.", "relation": "calls", "tail": "child_launch", "tail_type": "CUDA_API"}, {"head": "host_launch", "head_type": "__host__", "description": "This function launches the parent kernel from the host.", "relation": "calls", "tail": "parent_launch", "tail_type": "CUDA_API"}]}
{"pdf_page": [341, 344], "model_response": [{"head": "permute", "head_type": "__global__", "description": "This function performs permutation on data using shared memory.", "relation": "calls", "tail": "permute_data", "tail_type": "CUDA_API"}, {"head": "permute", "head_type": "__global__", "description": "This function performs permutation on data using shared memory.", "relation": "calls", "tail": "permute", "tail_type": "CUDA_API"}, {"head": "host_launch", "head_type": "__host__", "description": "This function launches the permute kernel.", "relation": "calls", "tail": "permute", "tail_type": "CUDA_API"}, {"head": "cudaGetLastError", "head_type": "__host__", "description": "This function retrieves the last error code returned by any CUDA function.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetParameterBuffer", "head_type": "CUDA_API", "description": "This function is generated automatically from kernel launch syntax.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaLaunchDevice", "head_type": "CUDA_API", "description": "This function launches a device kernel.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [345, 348], "model_response": [{"head": "cudaLaunchDevice", "head_type": "__device__", "description": "Launches a specified kernel with a given parameter buffer and configuration.", "relation": "calls", "tail": "cudaGetParameterBuffer", "tail_type": "CUDA_API"}, {"head": "cudaGetParameterBuffer", "head_type": "__device__", "description": "Obtains a parameter buffer based on alignment and size requirements.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [348, 352], "model_response": [{"head": "childKernel", "head_type": "__global__", "description": "This kernel prints 'Hello'.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "parentKernel", "head_type": "__global__", "description": "This kernel launches childKernel and prints 'World!'.", "relation": "calls", "tail": "childKernel", "tail_type": "CUDA_API"}, {"head": "parentKernel", "head_type": "__global__", "description": "This kernel launches childKernel and prints 'World!'.", "relation": "calls", "tail": "cudaGetLastError", "tail_type": "CUDA_API"}, {"head": "parentKernel", "head_type": "__global__", "description": "This kernel launches childKernel and prints 'World!'.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "main", "head_type": "__host__", "description": "This function launches the parentKernel.", "relation": "calls", "tail": "parentKernel", "tail_type": "CUDA_API"}, {"head": "main", "head_type": "__host__", "description": "This function launches the parentKernel.", "relation": "calls", "tail": "cudaGetLastError", "tail_type": "CUDA_API"}, {"head": "main", "head_type": "__host__", "description": "This function launches the parentKernel.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}]}
{"pdf_page": [355, 357], "model_response": [{"head": "cuDeviceGetAttribute", "head_type": "__host__", "description": "Queries the attribute of a CUDA device.", "relation": "calls", "tail": "CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED", "tail_type": "CUDA_API"}, {"head": "cuMemCreate", "head_type": "__host__", "description": "Allocates physical memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemGetAllocationGranularity", "head_type": "__host__", "description": "Queries the granularity requirements for memory allocation.", "relation": "calls", "tail": "CU_MEM_ALLOC_GRANULARITY_MINIMUM", "tail_type": "CUDA_API"}, {"head": "cuMemRelease", "head_type": "__host__", "description": "Frees the allocated memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemExportToShareableHandle", "head_type": "__host__", "description": "Exports a shareable handle for an allocation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemImportFromShareableHandle", "head_type": "__host__", "description": "Imports a memory allocation using a shareable handle.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGetAttribute", "head_type": "__host__", "description": "Queries the attribute of a CUDA device.", "relation": "calls", "tail": "CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED", "tail_type": "CUDA_API"}, {"head": "cuDeviceGetAttribute", "head_type": "__host__", "description": "Queries the attribute of a CUDA device.", "relation": "calls", "tail": "CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_HANDLE_SUPPORTED", "tail_type": "CUDA_API"}]}
{"pdf_page": [357, 360], "model_response": [{"head": "cuMemCreate", "head_type": "__host__", "description": "Allocates memory with specific properties.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "CUmemAllocationProp::allocFlags", "head_type": "__host__", "description": "Specifies memory allocation properties.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGetAttribute", "head_type": "__host__", "description": "Queries device attributes.", "relation": "calls", "tail": "CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED", "tail_type": "CUDA_API"}, {"head": "cuMemGetAllocationPropertiesFromHandle", "head_type": "__host__", "description": "Queries properties of a memory allocation.", "relation": "calls", "tail": "CUmemAllocationPropPrivate", "tail_type": "CUDA_CLASS"}, {"head": "cuMemAddressReserve", "head_type": "__host__", "description": "Reserves a range of virtual addresses.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemAddressFree", "head_type": "__host__", "description": "Frees a previously reserved virtual address range.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemMap", "head_type": "__host__", "description": "Maps allocated memory to a reserved address range.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemUnmap", "head_type": "__host__", "description": "Unmaps previously mapped memory from an address range.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemSetAccess", "head_type": "__host__", "description": "Sets access rights for a mapped address range.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEnablePeerAccess", "head_type": "__host__", "description": "Enables peer access for CUDA device allocations.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [360, 363], "model_response": [{"head": "cudaDeviceGetAttribute", "head_type": "__host__", "description": "This function queries a specific attribute of the device.", "relation": "calls", "tail": "cudaDevAttrMemoryPoolsSupported", "tail_type": "CUDA_API"}, {"head": "cudaDriverGetVersion", "head_type": "__host__", "description": "This function retrieves the version of the CUDA driver.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetAttribute", "head_type": "__host__", "description": "This function queries if the device supports the memory pool handle types.", "relation": "calls", "tail": "cudaDevAttrMemoryPoolSupportedHandleTypes", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "This function allocates memory asynchronously.", "relation": "calls", "tail": "cudaStreamPerThread", "tail_type": "CUDA_API"}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "This function frees memory asynchronously.", "relation": "calls", "tail": "cudaStreamPerThread", "tail_type": "CUDA_API"}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "This function causes a stream to wait until an event has occurred.", "relation": "calls", "tail": "cudaEventRecord", "tail_type": "CUDA_API"}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "This function records an event in a stream.", "relation": "calls", "tail": "cudaStream2", "tail_type": "CUDA_API"}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "This function frees memory asynchronously.", "relation": "calls", "tail": "stream3", "tail_type": "CUDA_API"}, {"head": "cudaMalloc", "head_type": "__host__", "description": "This function allocates memory on the device.", "relation": "calls", "tail": "ptr", "tail_type": "CUDA_API"}, {"head": "cudaStreamSynchronize", "head_type": "__host__", "description": "This function blocks until the specified stream has completed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__host__", "description": "This function frees memory on the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetMempool", "head_type": "__host__", "description": "This function sets the current memory pool for a device.", "relation": "calls", "tail": "cudaDeviceGetMempool", "tail_type": "CUDA_API"}, {"head": "cudaDeviceGetDefaultMempool", "head_type": "__host__", "description": "This function retrieves the default memory pool of the device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolCreate", "head_type": "__host__", "description": "This function creates a memory pool.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [363, 365], "model_response": [{"head": "cudaDeviceGetDefaultMempool", "head_type": "__host__", "description": "This API retrieves the default memory pool of a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "This API modifies the accessibility of the default memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolGetAccess", "head_type": "__host__", "description": "This API queries the accessibility of the default memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolCreate", "head_type": "__host__", "description": "This API creates an explicit memory pool.", "relation": "calls", "tail": "cudaMemPoolProps", "tail_type": "CUDA_CLASS"}, {"head": "cudaMemPoolCreate", "head_type": "__host__", "description": "This API creates an IPC capable memory pool on a valid CPU NUMA node.", "relation": "calls", "tail": "cudaMemPoolProps", "tail_type": "CUDA_CLASS"}, {"head": "cudaMemPoolSetAttribute", "head_type": "__host__", "description": "This API sets attributes for a memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrReleaseThreshold", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolTrimTo", "head_type": "__host__", "description": "This API explicitly shrinks a memory pool's memory footprint.", "relation": "calls", "tail": "cudaMemPoolAttrsReleaseThreshold", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "This API allocates memory asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "This API frees memory asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamSynchronize", "head_type": "__host__", "description": "This API synchronizes the stream.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [365, 369], "model_response": [{"head": "cudaMemPoolGetAttribute", "head_type": "__host__", "description": "Queries the attributes of a CUDA memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrReservedMemCurrent", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolGetAttribute", "head_type": "__host__", "description": "Queries the attributes of a CUDA memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrReservedMemHigh", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolGetAttribute", "head_type": "__host__", "description": "Queries the attributes of a CUDA memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrUsedMemCurrent", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolGetAttribute", "head_type": "__host__", "description": "Queries the attributes of a CUDA memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrUsedMemHigh", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolSetAttribute", "head_type": "__host__", "description": "Sets the specified attribute of a CUDA memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrReservedMemHigh", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolSetAttribute", "head_type": "__host__", "description": "Sets the specified attribute of a CUDA memory pool.", "relation": "calls", "tail": "cudaMemPoolAttrUsedMemHigh", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory asynchronously.", "relation": "calls", "tail": "cudaFreeAsync", "tail_type": "CUDA_API"}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records a CUDA event.", "relation": "calls", "tail": "cudaStreamWaitEvent", "tail_type": "CUDA_API"}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Makes the specified stream wait for an event to complete.", "relation": "calls", "tail": "cudaMallocAsync", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory asynchronously.", "relation": "calls", "tail": "cudaFreeAsync", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory asynchronously.", "relation": "calls", "tail": "cudaStreamWaitEvent", "tail_type": "CUDA_API"}, {"head": "cudaDeviceCanAccessPeer", "head_type": "__host__", "description": "Checks if one device can access the memory of another device.", "relation": "calls", "tail": "cudaMemPoolSetAccess", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "Sets access permissions to a memory pool.", "relation": "calls", "tail": "cudaMemAccessDesc", "tail_type": "CUDA_CLASS"}, {"head": "cudaMemPoolExportToShareableHandle", "head_type": "__host__", "description": "Exports a shareable handle to a memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolImportFromShareableHandle", "head_type": "__host__", "description": "Imports a shared handle into a memory pool.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [368, 372], "model_response": [{"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "Enables access from any GPU it plans to access the memory from.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolExportToShareableHandle", "head_type": "__host__", "description": "Retrieves an OS native handle to the pool.", "relation": "calls", "tail": "cudaMemPoolCreate", "tail_type": "CUDA_API"}, {"head": "cudaMemPoolImportFromShareableHandle", "head_type": "__host__", "description": "Creates an imported memory pool from the shareable handle.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocAsync", "head_type": "__device__", "description": "Allocates memory from the exporting memory pool asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolExportPointer", "head_type": "__host__", "description": "Exports a pointer from the memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolImportPointer", "head_type": "__host__", "description": "Imports a pointer into the memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaEventCreate", "head_type": "__host__", "description": "Creates a new event for IPC.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Waits for an event to complete before processing further.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeAsync", "head_type": "__device__", "description": "Frees memory asynchronously in the importing process.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [375, 379], "model_response": [{"head": "cudaGraphCreate", "head_type": "__host__", "description": "Creates an empty graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddMemAllocNode", "head_type": "__host__", "description": "Adds a memory allocation node to the graph.", "relation": "calls", "tail": "CUDA_MEM_ALLOC_NODE_PARAMS", "tail_type": "CUDA_CLASS"}, {"head": "cudaGraphAddKernelNode", "head_type": "__host__", "description": "Adds a kernel node to the graph.", "relation": "calls", "tail": "cudaGraphAddMemAllocNode", "tail_type": "CUDA_API"}, {"head": "cudaGraphAddMemFreeNode", "head_type": "__host__", "description": "Creates a free node in the graph.", "relation": "calls", "tail": "params.dptr", "tail_type": "CUDA_API"}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event in a specified stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Causes a stream to wait until an event has completed.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "Frees the memory allocated asynchronously.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends stream capture and produces the captured graph.", "relation": "calls", "tail": "params.dptr", "tail_type": "CUDA_API"}, {"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Creates a graph execution object from a graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches the graph execution object on a stream.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [379, 382], "model_response": [{"head": "cudaGraphAddMemAllocNode", "head_type": "__host__", "description": "Adds a memory allocation node to the graph.", "relation": "calls", "tail": "cudaGraphAddMemFreeNode", "tail_type": "CUDA_API"}, {"head": "cudaGraphAddKernelNode", "head_type": "__host__", "description": "Adds a kernel execution node to the graph.", "relation": "calls", "tail": "cudaGraphInstantiate", "tail_type": "CUDA_API"}, {"head": "cudaGraphAddMemFreeNode", "head_type": "__host__", "description": "Adds a memory free node to the graph.", "relation": "calls", "tail": "cudaGraphInstantiate", "tail_type": "CUDA_API"}, {"head": "cudaGraphInstantiate", "head_type": "__host__", "description": "Instantiates a graph for execution.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "cudaGraphLaunch", "head_type": "__host__", "description": "Launches a graph execution.", "relation": "calls", "tail": "cudaStreamWaitEvent", "tail_type": "CUDA_API"}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event in a stream.", "relation": "calls", "tail": "cudaStreamWaitEvent", "tail_type": "CUDA_API"}, {"head": "cudaStreamWaitEvent", "head_type": "__host__", "description": "Waits for an event to be recorded in a stream.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}, {"head": "cudaEventRecord", "head_type": "__host__", "description": "Records an event in a stream.", "relation": "calls", "tail": "cudaStreamRecordEvent", "tail_type": "CUDA_API"}, {"head": "cudaStreamRecordEvent", "head_type": "__host__", "description": "Records an event in a stream.", "relation": "calls", "tail": "cudaGraphLaunch", "tail_type": "CUDA_API"}, {"head": "cudaGraphInstantiateWithFlags", "head_type": "__host__", "description": "Instantiates a graph with specific flags.", "relation": "calls", "tail": "cudaStreamBeginCapture", "tail_type": "CUDA_API"}, {"head": "cudaStreamBeginCapture", "head_type": "__host__", "description": "Begins capturing a stream for a graph.", "relation": "calls", "tail": "cudaStreamEndCapture", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory asynchronously in a stream.", "relation": "calls", "tail": "cudaStreamBeginCapture", "tail_type": "CUDA_API"}, {"head": "cudaFreeAsync", "head_type": "__host__", "description": "Frees memory asynchronously in a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphExecDestroy", "head_type": "__host__", "description": "Destroys the graph execution object.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [386, 388], "model_response": [{"head": "cudaDeviceGetGraphMemAttribute", "head_type": "__host__", "description": "Allows applications to query their graph memory footprint.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGraphAddMemAllocNode", "head_type": "__host__", "description": "Adds a memory allocation node to a graph.", "relation": "calls", "tail": "cudaMemAllocNodeParams", "tail_type": "CUDA_CLASS"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Asynchronously allocates memory from a memory pool.", "relation": "calls", "tail": "cudaMemPoolSetAccess", "tail_type": "CUDA_API"}, {"head": "cudaStreamBeginCapture", "head_type": "__host__", "description": "Begins stream capture.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamEndCapture", "head_type": "__host__", "description": "Ends stream capture and generates a graph.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "Sets the access permissions for a memory pool.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [483, 485], "model_response": [{"head": "cuInit", "head_type": "__host__", "description": "Initializes the CUDA driver API.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGetCount", "head_type": "__host__", "description": "Gets the number of devices supporting CUDA.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGet", "head_type": "__host__", "description": "Gets the handle for a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuCtxCreate", "head_type": "__host__", "description": "Creates a context for the specified device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuModuleLoad", "head_type": "__host__", "description": "Loads a module from a binary file.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemAlloc", "head_type": "__host__", "description": "Allocates device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemcpyHtoD", "head_type": "__host__", "description": "Copies memory from host to device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuModuleGetFunction", "head_type": "__host__", "description": "Gets a function handle from a module.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuLaunchKernel", "head_type": "__host__", "description": "Launches a kernel on the device.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [485, 487], "model_response": [{"head": "cuModuleLoad", "head_type": "__host__", "description": "Loads a module from a specified PTX file.", "relation": "calls", "tail": "CUmodule", "tail_type": "CUDA_CLASS"}, {"head": "cuModuleGetFunction", "head_type": "__host__", "description": "Retrieves a handle to a kernel function from a loaded module.", "relation": "calls", "tail": "CUfunction", "tail_type": "CUDA_CLASS"}, {"head": "cuModuleLoadDataEx", "head_type": "__host__", "description": "Compiles and loads a new module from PTX code with specified options.", "relation": "calls", "tail": "CUjit_option", "tail_type": "CUDA_CLASS"}, {"head": "cuLinkCreate", "head_type": "__host__", "description": "Creates a link state for linking multiple PTX codes.", "relation": "calls", "tail": "CUlinkState", "tail_type": "CUDA_CLASS"}, {"head": "cuLinkAddData", "head_type": "__host__", "description": "Adds PTX code to a link state.", "relation": "calls", "tail": "CUjit_option", "tail_type": "CUDA_CLASS"}, {"head": "cuLinkComplete", "head_type": "__host__", "description": "Completes the linking process and returns the resulting cubin.", "relation": "calls", "tail": "void", "tail_type": "CUDA_API"}, {"head": "cuModuleLoadData", "head_type": "__host__", "description": "Loads the cubin into a module.", "relation": "calls", "tail": "CUmodule", "tail_type": "CUDA_CLASS"}, {"head": "cuLinkDestroy", "head_type": "__host__", "description": "Destroys a link state.", "relation": "calls", "tail": "CUlinkState", "tail_type": "CUDA_CLASS"}, {"head": "cuLaunchKernel", "head_type": "__host__", "description": "Launches a kernel with a given execution configuration.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [487, 490], "model_response": [{"head": "ADD_TO_PARAM_BUFFER", "head_type": "__host__", "description": "This macro adds a value to the parameter buffer and adjusts the buffer size.", "relation": "calls", "tail": "ALIGN_UP", "tail_type": "CUDA_API"}, {"head": "ADD_TO_PARAM_BUFFER", "head_type": "__host__", "description": "This macro adds a value to the parameter buffer and adjusts the buffer size.", "relation": "calls", "tail": "memcpy", "tail_type": "CUDA_API"}, {"head": "ADD_TO_PARAM_BUFFER", "head_type": "__host__", "description": "This macro adds a value to the parameter buffer and adjusts the buffer size.", "relation": "calls", "tail": "paramBuffer", "tail_type": "CUDA_CLASS"}, {"head": "cuLaunchKernel", "head_type": "__host__", "description": "This function launches a kernel on the device.", "relation": "calls", "tail": "cuFunction", "tail_type": "CUDA_CLASS"}, {"head": "cuMemAlloc", "head_type": "__host__", "description": "Allocates device memory.", "relation": "calls", "tail": "devPtr", "tail_type": "CUDA_CLASS"}, {"head": "cudaMalloc", "head_type": "__host__", "description": "Allocates device memory.", "relation": "calls", "tail": "d_data", "tail_type": "CUDA_CLASS"}, {"head": "cuCtxGetCurrent", "head_type": "__host__", "description": "Retrieves the current context.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [490, 493], "model_response": [{"head": "cuGetProcAddress", "head_type": "__host__", "description": "The function retrieves the address of a CUDA driver API function.", "relation": "calls", "tail": "pfn_cuStreamBeginCapture_v1", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "The function retrieves the address of a CUDA driver API function.", "relation": "calls", "tail": "pfn_cuStreamBeginCapture_v2", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "The function retrieves the address of a CUDA driver API function.", "relation": "calls", "tail": "pfn_cuStreamBeginCapture_latest", "tail_type": "CUDA_API"}, {"head": "cudaGetDriverEntryPoint", "head_type": "__host__", "description": "The function retrieves the entry point of a CUDA driver API function using the runtime API.", "relation": "calls", "tail": "pfn_cuMemAllocAsync", "tail_type": "CUDA_API"}, {"head": "cuDriverGetVersion", "head_type": "__host__", "description": "The function retrieves the version of the installed CUDA driver.", "relation": "calls", "tail": "cuGetProcAddress", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "The function retrieves the address of a CUDA driver API function.", "relation": "calls", "tail": "pfn_cuFoo", "tail_type": "CUDA_API"}]}
{"pdf_page": [493, 496], "model_response": [{"head": "cuDeviceGet", "head_type": "__host__", "description": "Gets a handle to a device.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}, {"head": "cuDeviceGetUuid", "head_type": "__host__", "description": "Gets the UUID of a device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "Obtains the function pointer to a CUDA API specified by name.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}, {"head": "cuDriverGetVersion", "head_type": "__host__", "description": "Retrieves the version of the CUDA driver.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuDeviceGet", "head_type": "__host__", "description": "Gets a handle to a device.", "relation": "calls", "tail": "cuDriverGetVersion", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "Obtains the function pointer to a CUDA API specified by name.", "relation": "calls", "tail": "cuDeviceGetUuid", "tail_type": "CUDA_API"}]}
{"pdf_page": [496, 499], "model_response": [{"head": "cudaGetDriverEntryPoint", "head_type": "__host__", "description": "This API retrieves the entry point of a specified CUDA Runtime API function.", "relation": "calls", "tail": "cudaEnableDefault", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "This API retrieves a function pointer to a specified driver API function.", "relation": "calls", "tail": "CUDA_VERSION", "tail_type": "CUDA_API"}, {"head": "cudaGetDriverEntryPoint", "head_type": "__host__", "description": "This API retrieves the entry point of a specified CUDA Runtime API function.", "relation": "calls", "tail": "cudaEnableDefault", "tail_type": "CUDA_API"}, {"head": "cudaDriverGetVersion", "head_type": "__host__", "description": "This API retrieves the version of the installed CUDA driver.", "relation": "calls", "tail": "", "tail_type": ""}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "This API retrieves a function pointer to a specified driver API function.", "relation": "calls", "tail": "CUDA_VERSION", "tail_type": "CUDA_API"}, {"head": "cudaGetDriverEntryPoint", "head_type": "__host__", "description": "This API retrieves the entry point of a specified CUDA Runtime API function.", "relation": "calls", "tail": "cudaEnableDefault", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "This API retrieves a function pointer to a specified driver API function.", "relation": "calls", "tail": "cudaVersion", "tail_type": "CUDA_API"}, {"head": "cuGetProcAddress", "head_type": "__host__", "description": "This API retrieves a function pointer to a specified driver API function.", "relation": "calls", "tail": "driverStatus", "tail_type": "CUDA_CLASS"}]}
{"pdf_page": [511, 514], "model_response": [{"head": "cudaMalloc", "head_type": "__host__", "description": "Allocates device memory on the GPU.", "relation": "calls", "tail": "cudaMemcpy", "tail_type": "CUDA_API"}, {"head": "cudaMemcpy", "head_type": "__host__", "description": "Copies data between host and device memory.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Blocks the CPU until the GPU has completed its work.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocManaged", "head_type": "__host__", "description": "Allocates Unified Memory which can be accessed from both host and device.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "write_value", "head_type": "__global__", "description": "A kernel function that writes a value to a given pointer.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [514, 518], "model_response": [{"head": "printme", "head_type": "__global__", "description": "This kernel prints a string passed to it.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "cudaMallocManaged", "head_type": "__host__", "description": "Allocates size bytes of managed memory and sets devPtr to refer to the allocation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "printme", "head_type": "__global__", "description": "This kernel prints a string passed to it.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "write_value", "head_type": "__global__", "description": "This kernel writes an integer value to a pointer location.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "write_value", "head_type": "__global__", "description": "This kernel writes an integer value to a pointer location.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaPointerGetAttributes", "head_type": "__host__", "description": "Checks the attributes of a pointer to determine its memory allocation type.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaGetDevice", "head_type": "__host__", "description": "Gets the current active device and assigns it to the variable.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceGetAttribute", "head_type": "__host__", "description": "Retrieves a device attribute.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [518, 521], "model_response": [{"head": "cudaMemPrefetchAsync", "head_type": "__host__", "description": "The cudaMemPrefetchAsync API is an asynchronous stream-ordered API that may migrate data to reside closer to the specified processor.", "relation": "calls", "tail": "cudaStreamSynchronize", "tail_type": "CUDA_API"}, {"head": "cudaMallocManaged", "head_type": "__host__", "description": "Allocates managed memory that can be accessed by both the CPU and GPU.", "relation": "calls", "tail": "cudaFree", "tail_type": "CUDA_API"}, {"head": "cudaMemAdvise", "head_type": "__host__", "description": "The cudaMemAdvise API is used to provide hints regarding how a memory area will be used.", "relation": "calls", "tail": "cudaMemPrefetchAsync", "tail_type": "CUDA_API"}, {"head": "cudaMemRangeGetAttribute", "head_type": "__host__", "description": "Queries an attribute of the memory range starting at devPtr with a size of count bytes.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [521, 525], "model_response": [{"head": "kernel", "head_type": "__global__", "description": "The kernel that prints the first 8 characters of the input character array.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "kernel", "head_type": "__global__", "description": "The kernel that prints the first 8 characters of the input character array.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "test_malloc", "head_type": "__host__", "description": "Tests the malloc method to allocate memory for the kernel.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}, {"head": "test_malloc", "head_type": "__host__", "description": "Tests the malloc method to allocate memory for the kernel.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "test_managed", "head_type": "__host__", "description": "Tests the managed method to allocate memory for the kernel.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}, {"head": "test_managed", "head_type": "__host__", "description": "Tests the managed method to allocate memory for the kernel.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "test_stack", "head_type": "__host__", "description": "Tests the stack method to access the kernel.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}, {"head": "test_stack", "head_type": "__host__", "description": "Tests the stack method to access the kernel.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "test_static", "head_type": "__host__", "description": "Tests the static variable method to access the kernel.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}, {"head": "test_static", "head_type": "__host__", "description": "Tests the static variable method to access the kernel.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "test_global", "head_type": "__host__", "description": "Tests a global scope variable to access the kernel.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}, {"head": "test_global", "head_type": "__host__", "description": "Tests a global scope variable to access the kernel.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "test_extern", "head_type": "__host__", "description": "Tests an extern variable to access the kernel.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}, {"head": "test_extern", "head_type": "__host__", "description": "Tests an extern variable to access the kernel.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "kernel", "head_type": "__global__", "description": "The kernel that accesses a global variable address.", "relation": "calls", "tail": "printf", "tail_type": "CUDA_API"}, {"head": "test_file_backed", "head_type": "__host__", "description": "Tests file-backed memory to access the kernel.", "relation": "calls", "tail": "kernel", "tail_type": "CUDA_API"}, {"head": "test_file_backed", "head_type": "__host__", "description": "Tests file-backed memory to access the kernel.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}]}
{"pdf_page": [527, 529], "model_response": [{"head": "write", "head_type": "__global__", "description": "The write kernel initializes ret in GPU memory.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "append", "head_type": "__global__", "description": "The append kernel modifies ret in GPU memory.", "relation": "calls", "tail": "cudaDeviceSynchronize", "tail_type": "CUDA_API"}, {"head": "cudaMemAdvise", "head_type": "CUDA_API", "description": "Sets access hints for the managed memory.", "relation": "calls", "tail": "cudaCpuDeviceId", "tail_type": "CUDA_CLASS"}, {"head": "test_malloc", "head_type": "CUDA_API", "description": "Function testing memory allocation and GPU memory access.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMallocManaged", "head_type": "CUDA_API", "description": "Allocates managed memory accessible from both CPU and GPU.", "relation": "calls", "tail": "cudaMemAdvise", "tail_type": "CUDA_API"}, {"head": "cudaFree", "head_type": "CUDA_API", "description": "Frees the allocated managed memory.", "relation": "single", "tail": "", "tail_type": ""}]}
{"pdf_page": [532, 535], "model_response": [{"head": "cudaStreamCreate", "head_type": "__host__", "description": "Creates a new stream.", "relation": "calls", "tail": "cudaStream_t", "tail_type": "CUDA_CLASS"}, {"head": "cudaMallocHost", "head_type": "__host__", "description": "Allocates non-managed, CPU-accessible memory.", "relation": "calls", "tail": "int*", "tail_type": "CUDA_API"}, {"head": "cudaMallocManaged", "head_type": "__host__", "description": "Allocates managed memory that can be accessed from both CPU and GPU.", "relation": "calls", "tail": "int*", "tail_type": "CUDA_API"}, {"head": "cudaSetDevice", "head_type": "__host__", "description": "Sets the current device.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaStreamAttachMemAsync", "head_type": "__host__", "description": "Associates a memory region with a specific stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSynchronize", "head_type": "__host__", "description": "Waits for the device to complete all preceding requested tasks.", "relation": "calls", "tail": "cudaStreamAttachMemAsync", "tail_type": "CUDA_API"}, {"head": "cudaStreamDestroy", "head_type": "__host__", "description": "Destroy a stream.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaFree", "head_type": "__host__", "description": "Frees the memory allocated by cudaMallocManaged.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "transform", "head_type": "__global__", "description": "Kernel function that performs a transformation on data.", "relation": "calls", "tail": "cudaStreamSynchronize", "tail_type": "CUDA_API"}, {"head": "convert", "head_type": "__global__", "description": "Kernel function that converts data.", "relation": "calls", "tail": "cudaStreamSynchronize", "tail_type": "CUDA_API"}]}
{"pdf_page": [547, 550], "model_response": [{"head": "cuMemCreate", "head_type": "__host__", "description": "Creates a memory allocation handle.", "relation": "calls", "tail": "cuMemGetAllocationGranularity", "tail_type": "CUDA_API"}, {"head": "cuMemGetAllocationGranularity", "head_type": "__host__", "description": "Gets the granularity for memory allocation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemCreate", "head_type": "__host__", "description": "Creates a memory allocation handle.", "relation": "calls", "tail": "ROUND_UP", "tail_type": "CUDA_API"}, {"head": "cuMemAddressReserve", "head_type": "__host__", "description": "Reserves an address space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemMap", "head_type": "__host__", "description": "Maps a memory allocation to an address space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemSetAccess", "head_type": "__host__", "description": "Sets access permissions for a mapped memory region.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolCreate", "head_type": "__host__", "description": "Creates a memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaMemPoolSetAccess", "head_type": "__host__", "description": "Sets access permissions for memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cudaDeviceSetMemPool", "head_type": "__host__", "description": "Sets the memory pool for a CUDA device.", "relation": "calls", "tail": "cudaMallocAsync", "tail_type": "CUDA_API"}, {"head": "cudaMallocAsync", "head_type": "__host__", "description": "Allocates memory asynchronously from a memory pool.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemExportToShareableHandle", "head_type": "__host__", "description": "Exports a memory allocation handle to be shared.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemImportFromShareableHandle", "head_type": "__host__", "description": "Imports a shared memory allocation handle.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemGetAllocationGranularity", "head_type": "__host__", "description": "Gets the granularity for memory allocation.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemAddressReserve", "head_type": "__host__", "description": "Reserves an address space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemMap", "head_type": "__host__", "description": "Maps a memory allocation to an address space.", "relation": "single", "tail": "", "tail_type": ""}, {"head": "cuMemSetAccess", "head_type": "__host__", "description": "Sets access permissions for a mapped memory region.", "relation": "single", "tail": "", "tail_type": ""}]}
